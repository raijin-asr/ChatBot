{"cells":[{"cell_type":"markdown","id":"8cb8aa81-2b8a-4937-be5c-20d756fd8efa","metadata":{"id":"8cb8aa81-2b8a-4937-be5c-20d756fd8efa"},"source":["### Types of Chatbots:\n","\n","There are broadly two variants of chatbots: Rule-Based and Self-learning.\n","\n","    1. In a Rule-based approach, a bot answers questions based on some rules, which it is trained on. The rules defined can be very simple to very complex. The bots can handle simple queries but fail to manage complex ones.\n","    \n","    2. Self-learning bots are the ones that use some Machine Learning-based approaches and are more efficient than rule-based bots. These bots can be of further two types: Retrieval Based or Generative.\n","\n","\n","    2.a) In retrieval-based models, a chatbot uses some heuristic to select a response from a library of predefined responses. The chatbot uses the message and context of the conversation for choosing the best response from a predefined list of bot messages. The context can include a current position in the dialogue tree, all previous messages in the conversation, previously saved variables (e.g., username). Heuristics for selecting a response can be engineered in many different ways, from rule-based if-else conditional logic to machine learning classifiers.\n","\n","    2. b) Generative bots can generate the answers and not always replies with one of the answers from a set of answers. This makes them more intelligent as they take word by word from the query and generates the answers"]},{"cell_type":"markdown","id":"d89c6752-cda5-46c1-9b04-60a8db4a1745","metadata":{"id":"d89c6752-cda5-46c1-9b04-60a8db4a1745"},"source":["### Examples\n","\n","Retrieval-Based Chatbot Example:\n","\n","\n","Predefined Responses:\n","\n","    \"The library is open from 9 AM to 5 PM on weekdays.\"\n","    \"You can borrow up to 5 books at a time.\"\n","    \"To renew a book, please visit the library's website or contact the help desk.\"\n","\n","Conversation:\n","User: \"What are the library's opening hours?\"\n","Bot: \"The library is open from 9 AM to 5 PM on weekdays.\"\n","\n","How it works:\n","\n","    Message: \"What are the library's opening hours?\"\n","    Heuristic: The bot matches the user's message to the closest predefined response using keywords or patterns (e.g., \"opening hours\").\n","    Selected Response: \"The library is open from 9 AM to 5 PM on weekdays.\"\n","\n","Generative Chatbot Example:\n","\n","Imagine a more advanced chatbot that can generate responses on the fly.\n","\n","Conversation:\n","User: \"What are the library's opening hours?\"\n","Bot: \"The library is open from 9 AM to 5 PM on weekdays, but it is closed on weekends.\"\n","\n","How it works:\n","\n","    Message: \"What are the library's opening hours?\"\n","    Generative Model: The bot processes the input using a neural network that has been trained on a large dataset of conversational text. It generates a response word by word.\n","    Generated Response: \"The library is open from 9 AM to 5 PM on weekdays, but it is closed on weekends.\"\n","\n","Key Differences:\n","\n","    Retrieval-Based Bot:\n","        Response Source: Predefined responses.\n","        Selection Method: Heuristics like keyword matching or pattern recognition.\n","        Flexibility: Limited to the responses it has been given.\n","\n","    Generative Bot:\n","        Response Source: Generates responses dynamically.\n","        Selection Method: Uses machine learning models to create a response based on the input.\n","        Flexibility: More adaptable and can handle a wider range of queries with nuanced answers"]},{"cell_type":"code","execution_count":null,"id":"d4c94549-ffc9-4b3f-8eb2-21c7b9494e8e","metadata":{"id":"d4c94549-ffc9-4b3f-8eb2-21c7b9494e8e"},"outputs":[],"source":["# pip install nltk"]},{"cell_type":"markdown","id":"f87d3d84-a13c-485c-a46b-0361ce48ab72","metadata":{"id":"f87d3d84-a13c-485c-a46b-0361ce48ab72"},"source":["Text Pre- Processing with NLTK\n","\n","The main issue with text data is that it is all in text format (strings). However, Machine learning algorithms need some sort of numerical feature vector to perform the task. So before we start with any NLP project, we need to pre-process it to make it ideal for work. Basic text pre-processing includes:\n","\n","    Converting the entire text into uppercase or lowercase so that the algorithm does not treat the same words in different cases as different\n","### Tokenization\n","    Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens, i.e., words that we want. A sentence tokenizer can be used to find the list of sentences, and a Word tokenizer can be used to find the list of words in strings."]},{"cell_type":"code","execution_count":null,"id":"669eb128-9cd6-4838-88fe-e49a7400bbea","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4835,"status":"ok","timestamp":1722663454191,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"},"user_tz":-345},"id":"669eb128-9cd6-4838-88fe-e49a7400bbea","outputId":"b224d374-0318-4d10-e9cd-1ef60f3f0a02"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"name":"stdout","output_type":"stream","text":["natural language processing with nltk is fun!\n"]}],"source":["#lowercase\n","import nltk\n","nltk.download('punkt')  # Downloading the punkt tokenizer models\n","\n","text = \"Natural Language Processing with NLTK is Fun!\"\n","text_lowercase = text.lower()\n","print(text_lowercase)\n"]},{"cell_type":"markdown","id":"6ff0108a-5503-4c1d-bb76-e342a296759d","metadata":{"id":"6ff0108a-5503-4c1d-bb76-e342a296759d"},"source":["### Tokenization:\n","\n","Purpose: Breaking down the text into smaller pieces like sentences or words."]},{"cell_type":"code","execution_count":null,"id":"e5c37951-1b24-4514-8b46-1d78625c246c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":683,"status":"ok","timestamp":1722663457237,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"},"user_tz":-345},"id":"e5c37951-1b24-4514-8b46-1d78625c246c","outputId":"6a6493dd-f2c2-4ad9-b669-f62637e7a3bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Hello World.', 'Natural Language Processing with NLTK is Fun!']\n"]}],"source":["#sentence tokenization\n","from nltk.tokenize import sent_tokenize\n","\n","text = \"Hello World. Natural Language Processing with NLTK is Fun!\"\n","sentences = sent_tokenize(text)\n","print(sentences)\n"]},{"cell_type":"code","execution_count":null,"id":"93a480f7-764e-4627-961a-40d1baac2b80","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1722663458939,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"},"user_tz":-345},"id":"93a480f7-764e-4627-961a-40d1baac2b80","outputId":"16e39133-06b5-4b08-9b4f-a85cdd1e1d8c"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Natural', 'Language', 'Processing', 'with', 'NLTK', 'is', 'Fun', '!']\n"]}],"source":["#word tokenization\n","from nltk.tokenize import word_tokenize\n","\n","text = \"Natural Language Processing with NLTK is Fun!\"\n","words = word_tokenize(text)\n","print(words)\n"]},{"cell_type":"markdown","id":"e14df75c-2cf0-4266-b5ea-0cc8bb8184f3","metadata":{"id":"e14df75c-2cf0-4266-b5ea-0cc8bb8184f3"},"source":["## Term Frequency(TF) and Inverse Document Frequency (IDF):\n","\n","Term Frequency (TF) and Inverse Document Frequency (IDF) are fundamental concepts in Natural Language Processing (NLP) used to measure the importance of a word in a document relative to a collection of documents (corpus).\n","\n","a) Term Frequency (TF):\n","TF measures how frequently a term appears in a document. It is the ratio of the number of times a term appears in a document to the total number of terms in the document.\n","\n","Formula:\n","TF(t,d)=Number of times term t appears in document d / Total number of terms in document d\n","\n","Example: TF(t,d)= 3/100 =0.03\n","\n","b) Inverse Document Frequency (IDF):\n","IDF measures how important a term is. While computing TF, all terms are considered equally important. However, certain terms like \"is\", \"of\", and \"that\" may appear frequently but have little importance. IDF weighs down the frequent terms while scaling up the rare ones.\n","\n","Formula:\n","IDF(t,D)=log⁡(Total number of documents (N) / Number of documents with term t)\n","\n","Example: IDF(t,D)=log(1000/10)=log(100)≈2\n","\n","c) TF-IDF:\n","TF-IDF is a numerical statistic that is intended to reflect how important a word is to a document in a corpus.\n","\n","Formula:\n","TF-IDF(t,d,D)= TF(t,d) × IDF(t,D)\n","\n","Example: TF-IDF(t,d,D)=0.03×2=0.06"]},{"cell_type":"code","execution_count":null,"id":"04ad2ace-3838-4775-98bc-3d6e0beb3178","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1722663469314,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"},"user_tz":-345},"id":"04ad2ace-3838-4775-98bc-3d6e0beb3178","outputId":"1d4338d4-4211-48e4-eaec-5f4c3c5165f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Document 1 TF-IDF scores:\n","natural: 0.0405\n",",: 0.2197\n","language: 0.0405\n","processing: 0.0000\n","with: 0.0405\n","nltk: 0.0405\n","is: 0.0405\n","fun: 0.1099\n",".: 0.0000\n","\n","Document 2 TF-IDF scores:\n","natural: 0.0405\n","language: 0.0405\n","processing: 0.0000\n","and: 0.0405\n","machine: 0.1099\n","learning: 0.1099\n","are: 0.1099\n","closely: 0.1099\n","related: 0.1099\n",".: 0.0000\n","\n","Document 3 TF-IDF scores:\n","text: 0.1221\n","processing: 0.0000\n","with: 0.0451\n","nltk: 0.0451\n","and: 0.0451\n","python: 0.1221\n","is: 0.0451\n","powerful: 0.1221\n",".: 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import nltk\n","import math\n","from collections import Counter\n","\n","# Download necessary resources\n","nltk.download('punkt')\n","\n","# Example documents\n","documents = [\n","    \"Natural, Language, Processing with NLTK is fun.\",\n","    \"Natural Language Processing and machine learning are closely related.\",\n","    \"Text processing with NLTK and Python is powerful.\"\n","]\n","\n","# Step 1: Convert to lowercase and tokenize the text\n","tokenized_documents = [nltk.word_tokenize(doc.lower()) for doc in documents]\n","\n","# Step 2: Calculate Term Frequency (TF)\n","def compute_tf(word_dict, doc):\n","    tf_dict = {}\n","    doc_count = len(doc)\n","    for word, count in word_dict.items():\n","        tf_dict[word] = count / float(doc_count)\n","    return tf_dict\n","\n","# Compute TF for each document\n","tf_documents = []\n","for doc in tokenized_documents:\n","    word_counts = Counter(doc)\n","    tf_documents.append(compute_tf(word_counts, doc))\n","\n","# Step 3: Calculate Inverse Document Frequency (IDF)\n","def compute_idf(documents):\n","    N = len(documents)\n","    unique_words = set(word for doc in documents for word in doc)\n","    idf_dict = dict.fromkeys(unique_words, 0)\n","    # idf_dict = dict.fromkeys(documents[0], 0)\n","    for doc in documents:\n","        for word in set(doc):\n","            idf_dict[word] += 1\n","    for word, val in idf_dict.items():\n","        idf_dict[word] = math.log(N / float(val))\n","    return idf_dict\n","\n","# Compute IDF\n","idf_dict = compute_idf(tokenized_documents)\n","\n","# Step 4: Calculate TF-IDF\n","def compute_tfidf(tf_doc, idf_dict):\n","    tfidf_dict = {}\n","    for word, tf_val in tf_doc.items():\n","        tfidf_dict[word] = tf_val * idf_dict[word]\n","    return tfidf_dict\n","\n","# Compute TF-IDF for each document\n","tfidf_documents = [compute_tfidf(tf_doc, idf_dict) for tf_doc in tf_documents]\n","\n","# Print results\n","for i, doc in enumerate(tfidf_documents):\n","    print(f\"\\nDocument {i+1} TF-IDF scores:\")\n","    for word, score in doc.items():\n","        print(f\"{word}: {score:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"id":"7dedb960-ef39-42ac-b9a0-1e9f3e4c1ee2","metadata":{"id":"7dedb960-ef39-42ac-b9a0-1e9f3e4c1ee2"},"outputs":[],"source":["# pip install torch"]},{"cell_type":"code","execution_count":null,"id":"1a7ab7d9-a73b-4f65-9897-994beebd7043","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6828,"status":"ok","timestamp":1722663723386,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"},"user_tz":-345},"id":"1a7ab7d9-a73b-4f65-9897-994beebd7043","outputId":"52ce7246-8e77-4cc0-d6c8-11252fd03057"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using Device: cuda\n","Current device number is: 0\n","GPU name is: Tesla T4\n"]}],"source":["import torch\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(f\"Using Device: {device}\")\n","\n","    device_no = torch.cuda.current_device()\n","    print(f\"Current device number is: {device_no}\")\n","\n","    device_name = torch.cuda.get_device_name(device_no)\n","    print(f\"GPU name is: {device_name}\")\n","else:\n","    print(\"CUDA is not available\")"]},{"cell_type":"code","execution_count":null,"id":"8aef3830-fd37-47a9-b452-e42a03d45fab","metadata":{"id":"8aef3830-fd37-47a9-b452-e42a03d45fab"},"outputs":[],"source":["import torch #an open source ML library used for creating deep neural networks\n","import torch.nn as nn # A module in PyTorch that provides classes and functions to build neural networks\n","import torch.optim as optim # A module in PyTorch that provides various optimization algorithms for training neural networks\n","import random # A module that implements pseudo-random number generators for various distributions\n","import re # A module for working with regular expressions to match and manipulate strings\n","import numpy as np\n","\n","# Sample dataset\n","data = [\n","    (\"hello\", \"hi\"),\n","    (\"how are you\", \"I'm good, how about you?\"),\n","    (\"what is your name\", \"I'm a chatbot\"),\n","    (\"bye\", \"goodbye\"),\n","]\n","\n","# Preprocessing\n","def preprocess(text):\n","    text = text.lower()\n","    text = re.sub(r'[^\\w\\s]', '', text) # removes all characters from the input text that are not word characters or whitespace\n","    return text\n","\n","# Vocabulary\n","all_words = []\n","for (pattern, response) in data:\n","    pattern = preprocess(pattern)\n","    response = preprocess(response)\n","    words = pattern.split() + response.split() #splits it into a list of words, using whitespace as the delimiter: \"hello Ameer\" to [\"hello\",\"Aeer\"]\n","    all_words.extend(words) #appends the elements of the list words to the end of the list all_words\n","all_words = sorted(set(all_words))\n","\n","\n","# Word to index mapping\n","word_to_idx = {word: idx for idx, word in enumerate(all_words)} #enumerate iterates on pairs of (index, value) tuples.\n","\n","# Encode patterns and responses\n","def encode(text):\n","    text = preprocess(text)\n","    return [word_to_idx[word] for word in text.split() if word in word_to_idx]\n","\n","encoded_data = [(encode(pattern), encode(response)) for (pattern, response) in data]\n","\n","# Pad sequences: ad_sequence function is used to ensure that all sequences in a dataset have the same length\n","def pad_sequence(seq, max_len, padding_value=0): #seq is input sequence that needs to be padded, max_len is desired length for all sequences, padding_val is alue used to fill the sequence to reach the maximum length\n","    return seq + [padding_value] * (max_len - len(seq)) #If the input sequence is shorter than the max_len, it appends padding_value to the end of the sequence until it reaches the desired length.\n","\n","# Determine the maximum length of patterns and responses\n","max_pattern_len = max(len(pattern) for pattern, response in encoded_data)\n","max_response_len = max(len(response) for pattern, response in encoded_data)\n","max_len = max(max_pattern_len, max_response_len)\n","\n","# Pad patterns and responses\n","padded_patterns = [pad_sequence(pattern, max_pattern_len) for pattern, response in encoded_data]\n","padded_responses = [pad_sequence(response, max_response_len) for pattern, response in encoded_data]\n","\n","# Convert to tensors\n","\n","# this two lines is giving errors:\n","# patterns = torch.tensor([pattern for pattern, response in encoded_data], dtype=torch.long)\n","# responses = torch.tensor([response for pattern, response in encoded_data], dtype=torch.long)\n","\n","#this is corrected two lines:\n","# patterns = torch.tensor(padded_patterns, dtype=torch.long)\n","# responses = torch.tensor(padded_responses, dtype=torch.long)"]},{"cell_type":"markdown","id":"9b0e4923-83e7-44cb-a9cd-90007852b1a7","metadata":{"id":"9b0e4923-83e7-44cb-a9cd-90007852b1a7"},"source":["# Tensors\n","\n","### A tensor is a multi-dimensional array, a generalization of vectors (1D tensors) and matrices (2D tensors) to potentially higher dimensions.\n","\n","--Tensors are a fundamental data structure in PyTorch and other deep learning frameworks because they allow for efficient computation and automatic differentiation.\n","\n","--Scalar: A single number (0-dimensional tensor.\n","Vector: A list of numbers (1-dimensional tensor).\n","Matrix: A table of numbers (2-dimensional tensor).\n","Tensor: A multi-dimensional array of numbers (3 or more dimensions).\n","\n","--Data Representation: Tensors are used to represent various types of data:\n","Images: 3D tensors (height, width, color channels)\n","Text: 2D tensors (sequence length, embedding dimension)\n","Audio: 3D tensors (time, frequency, channel)\n"]},{"cell_type":"code","execution_count":null,"id":"18c7042a-c4c5-46f2-a3bb-08590ad2af11","metadata":{"id":"18c7042a-c4c5-46f2-a3bb-08590ad2af11"},"outputs":[],"source":["# Convert to tensors: preparing it for training for efficient computations, leverages GPU acceleration, and integrates seamlessly with the PyTorch ecosystem.\n","patterns = torch.tensor(padded_patterns, dtype=torch.long)\n","responses = torch.tensor(padded_responses, dtype=torch.long)\n","\n","# patterns = torch.tensor([pattern for pattern in padded_patterns], dtype=torch.long)\n","# responses = torch.tensor([response for response in padded_responses], dtype=torch.long)\n"]},{"cell_type":"code","execution_count":null,"id":"ec69b53d-12fe-433d-82c1-0212eb73fc0c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1722664125196,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"},"user_tz":-345},"id":"ec69b53d-12fe-433d-82c1-0212eb73fc0c","outputId":"f4917aa6-3d77-4e00-9eeb-d8e95a8445af"},"outputs":[{"data":{"text/plain":["tensor([[ 7,  0,  0,  0],\n","        [ 9,  2, 14,  0],\n","        [13, 11, 15, 12],\n","        [ 3,  0,  0,  0]])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["patterns # gives 2D tensor intergers with 4 rows and 4 columns, making it a 4x4 matrix."]},{"cell_type":"code","execution_count":null,"id":"d5f6c4b3-c4da-41bd-90dd-5ffa322960d7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":696,"status":"ok","timestamp":1722664240326,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"},"user_tz":-345},"id":"d5f6c4b3-c4da-41bd-90dd-5ffa322960d7","outputId":"0879b999-9822-4dce-b106-a0f8dc83229e"},"outputs":[{"data":{"text/plain":["ChatbotModel(\n","  (embedding): Embedding(15, 8)\n","  (fc1): Linear(in_features=40, out_features=8, bias=True)\n","  (fc2): Linear(in_features=8, out_features=5, bias=True)\n",")"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["class ChatbotModel(nn.Module): #defines a basic chatbot model architecture using PyTorch\n","    def __init__(self, vocab_size, embed_size, hidden_size, output_size): #Initializes the model with hyperparameters\n","        super(ChatbotModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size) #An embedding layer to convert word indices to dense vectors. word indices are numerical representations of words in a vocabulary. vocabulary:[\"AMeer\", \"Rai\"], and word indices: \"Ameer\":0, \"rai\":1\n","        self.fc1 = nn.Linear(embed_size * max_len, hidden_size) #The first fully connected layer.\n","        self.fc2 = nn.Linear(hidden_size, output_size) #he second fully connected layer, output layer.\n","\n","    def forward(self, x): #efines the forward pass of the mode, x: Input tensor of word indices.,\n","        x = self.embedding(x) #Embeds/convers the input words indices \"x\" into dense embedding vector. Embeddings are dense vector representations of words. They capture semantic and syntactic information about word for mathematical operations. word \"Ameer\" might have an embedding like [0.23, -0.15, 0.42, ...]\n","        x = x.view(x.size(0), -1) #Reshapes the tensor into a 2D tensor.\n","        x = torch.relu(self.fc1(x)) #Applies a ReLU activation function to the output of the first fully connected layer.\n","        x = self.fc2(x) #Passes the output through the second fully connected laye\n","        return x\n","\n","vocab_size = 15 #Total number of unique words in the vocabulary. Each unique word is assigned a unique integer index\n","embed_size = 8 #Dimensionality of word embeddings.\n","hidden_size = 8 #number of neurons in the hidden laye\n","output_size = max_response_len #Size of the output layer (likely the maximum length of a response\n","\n","model = ChatbotModel(vocab_size, embed_size, hidden_size, output_size)  #simple feed-forward neural network for generating chatbot responses\n","model\n"]},{"cell_type":"markdown","id":"Teu0FFpcGGOu","metadata":{"id":"Teu0FFpcGGOu"},"source":["ChatbotModel(\n","  (embedding): Embedding(15, 8)\n","  (fc1): Linear(in_features=40, out_features=8, bias=True)\n","  (fc2): Linear(in_features=8, out_features=5, bias=True)\n",")\n","\n","### Embedding(15, 8):\n","This indicates the embedding layer has a vocabulary size of 15 words (hence 15 rows) and each word is represented by a 8-dimensional vector (hence 8 columns).\n","\n","### Linear(in_features=40, out_features=8, bias=True):\n"," This is the first fully connected (FC) layer. It takes a 40-dimensional input (likely the flattened embedding output) and produces an 8-dimensional output. The bias=True indicates that the layer uses a bias term.\n","\n","##Linear(in_features=8, out_features=5, bias=True):\n","This is the second FC layer. It takes an 8-dimensional input (output from the first FC layer) and produces a 5-dimensional output. Again, it uses a bias term.\n","\n","#Implications(suggesstion or analysis):\n","--Small Vocabulary: With a vocabulary size of 15, the chatbot might have limited conversational capabilities.\n","\n","--Shallow Architecture: The model consists of only two linear layers, which might limit its complexity and performance.\n","\n","--Fixed Output Size: The output size of 5 suggests a fixed response length of 5 words."]},{"cell_type":"code","execution_count":null,"id":"46b4eb9b-a620-4a99-a8ca-af561cd47b50","metadata":{"id":"46b4eb9b-a620-4a99-a8ca-af561cd47b50"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":5}
