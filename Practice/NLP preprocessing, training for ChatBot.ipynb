{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fecf0b80",
   "metadata": {},
   "source": [
    "### Types of Chatbots:\n",
    "\n",
    "There are broadly two variants of chatbots: Rule-Based and Self-learning.\n",
    "\n",
    "    1. In a Rule-based approach, a bot answers questions based on some rules, which it is trained on. The rules defined can be very simple to very complex. The bots can handle simple queries but fail to manage complex ones.\n",
    "    \n",
    "    2. Self-learning bots are the ones that use some Machine Learning-based approaches and are more efficient than rule-based bots. These bots can be of further two types: Retrieval Based or Generative.\n",
    "\n",
    "\n",
    "    2.a) In retrieval-based models, a chatbot uses some heuristic to select a response from a library of predefined responses. The chatbot uses the message and context of the conversation for choosing the best response from a predefined list of bot messages. The context can include a current position in the dialogue tree, all previous messages in the conversation, previously saved variables (e.g., username). Heuristics for selecting a response can be engineered in many different ways, from rule-based if-else conditional logic to machine learning classifiers.\n",
    "\n",
    "    2. b) Generative bots can generate the answers and not always replies with one of the answers from a set of answers. This makes them more intelligent as they take word by word from the query and generates the answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d288f97",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "Retrieval-Based Chatbot Example:\n",
    "\n",
    "\n",
    "Predefined Responses:\n",
    "\n",
    "    \"The library is open from 9 AM to 5 PM on weekdays.\"\n",
    "    \"You can borrow up to 5 books at a time.\"\n",
    "    \"To renew a book, please visit the library's website or contact the help desk.\"\n",
    "\n",
    "Conversation:\n",
    "User: \"What are the library's opening hours?\"\n",
    "Bot: \"The library is open from 9 AM to 5 PM on weekdays.\"\n",
    "\n",
    "How it works:\n",
    "\n",
    "    Message: \"What are the library's opening hours?\"\n",
    "    Heuristic: The bot matches the user's message to the closest predefined response using keywords or patterns (e.g., \"opening hours\").\n",
    "    Selected Response: \"The library is open from 9 AM to 5 PM on weekdays.\"\n",
    "\n",
    "Generative Chatbot Example:\n",
    "\n",
    "Imagine a more advanced chatbot that can generate responses on the fly.\n",
    "\n",
    "Conversation:\n",
    "User: \"What are the library's opening hours?\"\n",
    "Bot: \"The library is open from 9 AM to 5 PM on weekdays, but it is closed on weekends.\"\n",
    "\n",
    "How it works:\n",
    "\n",
    "    Message: \"What are the library's opening hours?\"\n",
    "    Generative Model: The bot processes the input using a neural network that has been trained on a large dataset of conversational text. It generates a response word by word.\n",
    "    Generated Response: \"The library is open from 9 AM to 5 PM on weekdays, but it is closed on weekends.\"\n",
    "\n",
    "Key Differences:\n",
    "\n",
    "    Retrieval-Based Bot:\n",
    "        Response Source: Predefined responses.\n",
    "        Selection Method: Heuristics like keyword matching or pattern recognition.\n",
    "        Flexibility: Limited to the responses it has been given.\n",
    "\n",
    "    Generative Bot:\n",
    "        Response Source: Generates responses dynamically.\n",
    "        Selection Method: Uses machine learning models to create a response based on the input.\n",
    "        Flexibility: More adaptable and can handle a wider range of queries with nuanced answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695ef9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be5f8d2",
   "metadata": {},
   "source": [
    "Text Pre- Processing with NLTK\n",
    "\n",
    "The main issue with text data is that it is all in text format (strings). However, Machine learning algorithms need some sort of numerical feature vector to perform the task. So before we start with any NLP project, we need to pre-process it to make it ideal for work. Basic text pre-processing includes:\n",
    "\n",
    "    Converting the entire text into uppercase or lowercase so that the algorithm does not treat the same words in different cases as different\n",
    "### Tokenization\n",
    "    Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens, i.e., words that we want. A sentence tokenizer can be used to find the list of sentences, and a Word tokenizer can be used to find the list of words in strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a088039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natural language processing with nltk is fun!\n"
     ]
    }
   ],
   "source": [
    "#lowercase\n",
    "import nltk\n",
    "nltk.download('punkt')  # Downloading the punkt tokenizer models\n",
    "\n",
    "text = \"Natural Language Processing with NLTK is Fun!\"\n",
    "text_lowercase = text.lower()\n",
    "print(text_lowercase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38932bfe",
   "metadata": {},
   "source": [
    "### Tokenization:\n",
    "\n",
    "Purpose: Breaking down the text into smaller pieces like sentences or words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94861bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello World.', 'Natural Language Processing with NLTK is Fun!']\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hello World. Natural Language Processing with NLTK is Fun!\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a3322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'with', 'NLTK', 'is', 'Fun', '!']\n"
     ]
    }
   ],
   "source": [
    "#word tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Natural Language Processing with NLTK is Fun!\"\n",
    "words = word_tokenize(text)\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3928b98b",
   "metadata": {},
   "source": [
    "## Term Frequency(TF) and Inverse Document Frequency (IDF):\n",
    "\n",
    "Term Frequency (TF) and Inverse Document Frequency (IDF) are fundamental concepts in Natural Language Processing (NLP) used to measure the importance of a word in a document relative to a collection of documents (corpus).\n",
    "\n",
    "a) Term Frequency (TF):\n",
    "TF measures how frequently a term appears in a document. It is the ratio of the number of times a term appears in a document to the total number of terms in the document.\n",
    "\n",
    "Formula:\n",
    "TF(t,d)=Number of times term t appears in document d / Total number of terms in document d\n",
    "\n",
    "Example: TF(t,d)= 3/100 =0.03\n",
    "\n",
    "b) Inverse Document Frequency (IDF):\n",
    "IDF measures how important a term is. While computing TF, all terms are considered equally important. However, certain terms like \"is\", \"of\", and \"that\" may appear frequently but have little importance. IDF weighs down the frequent terms while scaling up the rare ones.\n",
    "\n",
    "Formula:\n",
    "IDF(t,D)=log⁡(Total number of documents (N) / Number of documents with term t)\n",
    "\n",
    "Example: IDF(t,D)=log(1000/10)=log(100)≈2\n",
    "\n",
    "c) TF-IDF:\n",
    "TF-IDF is a numerical statistic that is intended to reflect how important a word is to a document in a corpus.\n",
    "\n",
    "Formula:\n",
    "TF-IDF(t,d,D)= TF(t,d) × IDF(t,D)\n",
    "\n",
    "Example: TF-IDF(t,d,D)=0.03×2=0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f509904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1 TF-IDF scores:\n",
      "natural: 0.0405\n",
      ",: 0.2197\n",
      "language: 0.0405\n",
      "processing: 0.0000\n",
      "with: 0.0405\n",
      "nltk: 0.0405\n",
      "is: 0.0405\n",
      "fun: 0.1099\n",
      ".: 0.0000\n",
      "\n",
      "Document 2 TF-IDF scores:\n",
      "natural: 0.0405\n",
      "language: 0.0405\n",
      "processing: 0.0000\n",
      "and: 0.0405\n",
      "machine: 0.1099\n",
      "learning: 0.1099\n",
      "are: 0.1099\n",
      "closely: 0.1099\n",
      "related: 0.1099\n",
      ".: 0.0000\n",
      "\n",
      "Document 3 TF-IDF scores:\n",
      "text: 0.1221\n",
      "processing: 0.0000\n",
      "with: 0.0451\n",
      "nltk: 0.0451\n",
      "and: 0.0451\n",
      "python: 0.1221\n",
      "is: 0.0451\n",
      "powerful: 0.1221\n",
      ".: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"Natural, Language, Processing with NLTK is fun.\",\n",
    "    \"Natural Language Processing and machine learning are closely related.\",\n",
    "    \"Text processing with NLTK and Python is powerful.\"\n",
    "]\n",
    "\n",
    "# Step 1: Convert to lowercase and tokenize the text\n",
    "tokenized_documents = [nltk.word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "# Step 2: Calculate Term Frequency (TF)\n",
    "def compute_tf(word_dict, doc):\n",
    "    tf_dict = {}\n",
    "    doc_count = len(doc)\n",
    "    for word, count in word_dict.items():\n",
    "        tf_dict[word] = count / float(doc_count)\n",
    "    return tf_dict\n",
    "\n",
    "# Compute TF for each document\n",
    "tf_documents = []\n",
    "for doc in tokenized_documents:\n",
    "    word_counts = Counter(doc)\n",
    "    tf_documents.append(compute_tf(word_counts, doc))\n",
    "\n",
    "# Step 3: Calculate Inverse Document Frequency (IDF)\n",
    "def compute_idf(documents):\n",
    "    N = len(documents)\n",
    "    unique_words = set(word for doc in documents for word in doc)\n",
    "    idf_dict = dict.fromkeys(unique_words, 0)\n",
    "    # idf_dict = dict.fromkeys(documents[0], 0)\n",
    "    for doc in documents:\n",
    "        for word in set(doc):\n",
    "            idf_dict[word] += 1\n",
    "    for word, val in idf_dict.items():\n",
    "        idf_dict[word] = math.log(N / float(val))\n",
    "    return idf_dict\n",
    "\n",
    "# Compute IDF\n",
    "idf_dict = compute_idf(tokenized_documents)\n",
    "\n",
    "# Step 4: Calculate TF-IDF\n",
    "def compute_tfidf(tf_doc, idf_dict):\n",
    "    tfidf_dict = {}\n",
    "    for word, tf_val in tf_doc.items():\n",
    "        tfidf_dict[word] = tf_val * idf_dict[word]\n",
    "    return tfidf_dict\n",
    "\n",
    "# Compute TF-IDF for each document\n",
    "tfidf_documents = [compute_tfidf(tf_doc, idf_dict) for tf_doc in tf_documents]\n",
    "\n",
    "# Print results\n",
    "for i, doc in enumerate(tfidf_documents):\n",
    "    print(f\"\\nDocument {i+1} TF-IDF scores:\")\n",
    "    for word, score in doc.items():\n",
    "        print(f\"{word}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c66d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d1d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "Current device number is: 0\n",
      "GPU name is: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using Device: {device}\")\n",
    "\n",
    "    device_no = torch.cuda.current_device()\n",
    "    print(f\"Current device number is: {device_no}\")\n",
    "\n",
    "    device_name = torch.cuda.get_device_name(device_no)\n",
    "    print(f\"GPU name is: {device_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf7153e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Pattern Length: 4\n",
      "Max Response Length: 5\n",
      "Padded Pattern: [[7, 0, 0, 0, 0], [9, 2, 14, 0, 0], [13, 11, 15, 12, 0], [3, 0, 0, 0, 0]]\n",
      "Padded Response: [[8, 0, 0, 0, 0], [10, 5, 9, 1, 14], [10, 0, 4, 0, 0], [6, 0, 0, 0, 0]]\n",
      "Example padded pattern: [7, 0, 0, 0, 0]\n",
      "Example padded response: [8, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import torch #an open source ML library used for creating deep neural networks\n",
    "import torch.nn as nn # A module in PyTorch that provides classes and functions to build neural networks\n",
    "import torch.optim as optim # A module in PyTorch that provides various optimization algorithms for training neural networks\n",
    "import random # A module that implements pseudo-random number generators for various distributions\n",
    "import re # A module for working with regular expressions to match and manipulate strings\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = [\n",
    "    (\"hello\", \"hi\"),\n",
    "    (\"how are you\", \"I'm good, how about you?\"),\n",
    "    (\"what is your name\", \"I'm a chatbot\"),\n",
    "    (\"bye\", \"goodbye\"),\n",
    "]\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # removes all characters from the input text that are not word characters or whitespace\n",
    "    return text\n",
    "\n",
    "# Vocabulary\n",
    "all_words = []\n",
    "for (pattern, response) in data:\n",
    "    pattern = preprocess(pattern)\n",
    "    response = preprocess(response)\n",
    "    words = pattern.split() + response.split() #splits it into a list of words, using whitespace as the delimiter: \"hello Ameer\" to [\"hello\",\"Aeer\"]\n",
    "    all_words.extend(words) #appends the elements of the list words to the end of the list all_words\n",
    "all_words = sorted(set(all_words))\n",
    "\n",
    "\n",
    "# Word to index mapping\n",
    "word_to_idx = {word: idx for idx, word in enumerate(all_words)} #enumerate iterates on pairs of (index, value) tuples.\n",
    "\n",
    "# Encode patterns and responses\n",
    "def encode(text):\n",
    "    text = preprocess(text)\n",
    "    return [word_to_idx[word] for word in text.split() if word in word_to_idx]\n",
    "\n",
    "encoded_data = [(encode(pattern), encode(response)) for (pattern, response) in data]\n",
    "\n",
    "# Pad sequences: ad_sequence function is used to ensure that all sequences in a dataset have the same length\n",
    "def pad_sequence(seq, max_len, padding_value=0): #seq is input sequence that needs to be padded, max_len is desired length for all sequences, padding_val is alue used to fill the sequence to reach the maximum length\n",
    "    return seq + [padding_value] * (max_len - len(seq)) #If the input sequence is shorter than the max_len, it appends padding_value to the end of the sequence until it reaches the desired length.\n",
    "\n",
    "# Determine the maximum length of patterns and responses\n",
    "max_pattern_len = max(len(pattern) for pattern, response in encoded_data)\n",
    "max_response_len = max(len(response) for pattern, response in encoded_data)\n",
    "max_len = max(max_pattern_len, max_response_len)\n",
    "\n",
    "# Pad patterns and responses\n",
    "# padded_patterns = [pad_sequence(pattern, max_pattern_len) for pattern, response in encoded_data]\n",
    "# padded_responses = [pad_sequence(response, max_response_len) for pattern, response in encoded_data]\n",
    "\n",
    "padded_patterns = [pad_sequence(pattern, max_len) for pattern, response in encoded_data]\n",
    "padded_responses = [pad_sequence(response, max_len) for pattern, response in encoded_data]\n",
    "\n",
    "print(f\"Max Pattern Length: {max_pattern_len}\")\n",
    "print(f\"Max Response Length: {max_response_len}\")\n",
    "# print(f\"Max Length: {max_len}\")\n",
    "print(f\"Padded Pattern: {padded_patterns}\")\n",
    "print(f\"Padded Response: {padded_responses}\")\n",
    "\n",
    "# Additional debugging:\n",
    "print(\"Example padded pattern:\", padded_patterns[0])\n",
    "print(\"Example padded response:\", padded_responses[0])\n",
    "\n",
    "# Convert to tensors\n",
    "\n",
    "# this two lines is giving errors: but solved later\n",
    "# patterns = torch.tensor([pattern for pattern, response in encoded_data], dtype=torch.long)\n",
    "# responses = torch.tensor([response for pattern, response in encoded_data], dtype=torch.long)\n",
    "\n",
    "#this is corrected two lines:\n",
    "# patterns = torch.tensor(padded_patterns, dtype=torch.long)\n",
    "# responses = torch.tensor(padded_responses, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5fc8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Pattern Length: 4\n",
      "Max Response Length: 5\n",
      "Max Length: 5\n",
      "Padded Pattern: [[7, 0, 0, 0, 0], [9, 2, 14, 0, 0], [13, 11, 15, 12, 0], [3, 0, 0, 0, 0]]\n",
      "Padded Response: [[8, 0, 0, 0, 0], [10, 5, 9, 1, 14], [10, 0, 4, 0, 0], [6, 0, 0, 0, 0]]\n",
      "Example padded pattern: [7, 0, 0, 0, 0]\n",
      "Example padded response: [8, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aca7e99f",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "### A tensor is a multi-dimensional array, a generalization of vectors (1D tensors) and matrices (2D tensors) to potentially higher dimensions.\n",
    "\n",
    "--Tensors are a fundamental data structure in PyTorch and other deep learning frameworks because they allow for efficient computation and automatic differentiation.\n",
    "\n",
    "--Scalar: A single number (0-dimensional tensor.\n",
    "Vector: A list of numbers (1-dimensional tensor).\n",
    "Matrix: A table of numbers (2-dimensional tensor).\n",
    "Tensor: A multi-dimensional array of numbers (3 or more dimensions).\n",
    "\n",
    "--Data Representation: Tensors are used to represent various types of data:\n",
    "Images: 3D tensors (height, width, color channels)\n",
    "Text: 2D tensors (sequence length, embedding dimension)\n",
    "Audio: 3D tensors (time, frequency, channel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff15e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors: preparing it for training for efficient computations, leverages GPU acceleration, and integrates seamlessly with the PyTorch ecosystem.\n",
    "patterns = torch.tensor(padded_patterns, dtype=torch.long)\n",
    "responses = torch.tensor(padded_responses, dtype=torch.long)\n",
    "\n",
    "# patterns = torch.tensor([pattern for pattern in padded_patterns], dtype=torch.long)\n",
    "# responses = torch.tensor([response for response in padded_responses], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97530688",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d6c14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  0,  0,  0,  0],\n",
       "        [ 9,  2, 14,  0,  0],\n",
       "        [13, 11, 15, 12,  0],\n",
       "        [ 3,  0,  0,  0,  0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "patterns # gives 2D tensor intergers with 4 rows and 4 columns, making it a 4x4 matrix.\n",
    "# responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc5d43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatbotModel(\n",
      "  (embedding): Embedding(16, 10)\n",
      "  (lstm): LSTM(10, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=100, out_features=80, bias=True)\n",
      ")\n",
      "Vocabulary Size: 16\n",
      "Output Size: 16\n",
      "Word to Index Mapping: {'a': 0, 'about': 1, 'are': 2, 'bye': 3, 'chatbot': 4, 'good': 5, 'goodbye': 6, 'hello': 7, 'hi': 8, 'how': 9, 'im': 10, 'is': 11, 'name': 12, 'what': 13, 'you': 14, 'your': 15}\n",
      "Patterns Tensor: tensor([[ 7,  0,  0,  0,  0],\n",
      "        [ 9,  2, 14,  0,  0],\n",
      "        [13, 11, 15, 12,  0],\n",
      "        [ 3,  0,  0,  0,  0]])\n",
      "Max Length: 5\n"
     ]
    }
   ],
   "source": [
    "class ChatbotModel(nn.Module): #defines a basic chatbot model architecture using PyTorch\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size,max_len): #Initializes the model with hyperparameters\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size) #An embedding layer to convert word indices to dense vectors. word indices are numerical representations of words in a vocabulary. vocabulary:[\"AMeer\", \"Rai\"], and word indices: \"Ameer\":0, \"rai\":1\n",
    "        # self.fc1 = nn.Linear(embed_size * max_len, hidden_size) #The first fully connected layer.\n",
    "        # self.fc2 = nn.Linear(hidden_size, output_size) #he second fully connected layer, output layer.\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * max_len, output_size * max_len)\n",
    "\n",
    "    # def forward(self, x): #efines the forward pass of the mode, x: Input tensor of word indices.,\n",
    "    #     x = self.embedding(x) #Embeds/convers the input words indices \"x\" into dense embedding vector. Embeddings are dense vector representations of words. They capture semantic and syntactic information about word for mathematical operations. word \"Ameer\" might have an embedding like [0.23, -0.15, 0.42, ...]\n",
    "    #     x = x.view(x.size(0), -1) #Reshapes the tensor into a 2D tensor.\n",
    "    #     x = torch.relu(self.fc1(x)) #Applies a ReLU activation function to the output of the first fully connected layer.\n",
    "    #     x = self.fc2(x) #Passes the output through the second fully connected laye\n",
    "    #     return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.contiguous().view(x.size(0), -1)  # Flatten the output for the linear layer\n",
    "        x = self.fc(x)\n",
    "        return x.view(x.size(0), max_len, -1)  # Reshape to (batch_size, max_len, output_size)\n",
    "\n",
    "# Initialize model\n",
    "vocab_size = len(all_words)\n",
    "# vocab_size = len(word_to_idx)  #Total number of unique words in the vocabulary. Each unique word is assigned a unique integer index\n",
    "embed_size = 10 #Dimensionality of word embeddings.\n",
    "hidden_size = 20 #number of neurons in the hidden laye\n",
    "# output_size = max_response_len #Size of the output layer (likely the maximum length of a response\n",
    "output_size = vocab_size  # Output size should match the vocabulary size\n",
    "\n",
    "\n",
    "max_len = max(max_pattern_len, max_response_len)\n",
    "# max_len = max(max(len(p) for p in padded_patterns), max(len(r) for r in padded_responses))\n",
    "model = ChatbotModel(vocab_size, embed_size, hidden_size, output_size,max_len)  #simple feed-forward neural network for generating chatbot responses\n",
    "print(model)\n",
    "\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "print(f\"Output Size: {output_size}\")\n",
    "print(f\"Word to Index Mapping: {word_to_idx}\")\n",
    "print(f\"Patterns Tensor: {patterns}\")\n",
    "print(f\"Max Length: {max_len}\")\n",
    "\n",
    "for pattern in patterns:\n",
    "    for idx in pattern:\n",
    "        if idx >= vocab_size:\n",
    "            print(f\"Invalid Index: {idx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b590c",
   "metadata": {},
   "source": [
    "ChatbotModel(\n",
    "  (embedding): Embedding(15, 8)\n",
    "  (fc1): Linear(in_features=40, out_features=8, bias=True)\n",
    "  (fc2): Linear(in_features=8, out_features=5, bias=True)\n",
    ")\n",
    "\n",
    "## Embedding(15, 8):\n",
    "This indicates the embedding layer has a vocabulary size of 15 words (hence 15 rows) and each word is represented by a 8-dimensional vector (hence 8 columns).\n",
    "\n",
    "## Linear(in_features=40, out_features=8, bias=True):\n",
    " This is the first fully connected (FC) layer. It takes a 40-dimensional input (likely the flattened embedding output) and produces an 8-dimensional output. The bias=True indicates that the layer uses a bias term.\n",
    "\n",
    "##Linear(in_features=8, out_features=5, bias=True):\n",
    "This is the second FC layer. It takes an 8-dimensional input (output from the first FC layer) and produces a 5-dimensional output. Again, it uses a bias term.\n",
    "\n",
    "#Implications(suggesstion or analysis):\n",
    "--Small Vocabulary: With a vocabulary size of 15, the chatbot might have limited conversational capabilities.\n",
    "\n",
    "--Shallow Architecture: The model consists of only two linear layers, which might limit its complexity and performance.\n",
    "\n",
    "--Fixed Output Size: The output size of 5 suggests a fixed response length of 5 words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d4aaa6",
   "metadata": {},
   "source": [
    "#### Neural network model\n",
    "\n",
    "#### Neural network model designed to map input sequences (patterns) to output sequences (responses). It consists of an embedding layer followed by two fully connected (linear) layers\n",
    "--Imagine you want to teach a computer to have a conversation. This model is like a basic language learner\n",
    "\n",
    "--Components of the Model\n",
    "\n",
    "    Embedding Layer (nn.Embedding):\n",
    "        This layer converts input word indices into dense vectors of fixed size (embed_size).\n",
    "        Input: (batch_size, sequence_length) #nput shape represents the raw text data, organized into batches and sequences.\n",
    "        Output: (batch_size, sequence_length, embed_size) #utput shape represents the transformed text data, where each word is converted into a fixed-size vector (embedding), maintaining the original batch and sequence structure.\n",
    "\n",
    "        --batch_size: number of samples or sequences processed together in a single pass. For example, if you have 32 sentences, and you process them in batches of 4, the batch_size would be 4.\n",
    "        --sequence_length: This is the length of each individual sequence (or sentence) in the batch. For instance, if the longest sentence in your batch has 10 words, the sequence_length would be 10.\n",
    "\n",
    "        --input(4,10): If you have a batch of 4 sentences, each with a maximum of 10 words, the input shape would be (4, 10).\n",
    "\n",
    "        --embed_size: This is the dimensionality of the embedding vector produced for each word in the sequence. It's the size of the dense vector representing a word.\n",
    "\n",
    "        --output(4,10,64): if each word is represented by a 64-dimensional embedding, the output shape would be (4, 10, 64).\n",
    "\n",
    "       --This is like teaching the computer what words mean. It turns words into numbers that the computer can understand. For example, the word \"hello\" becomes a list of numbers.\n",
    "\n",
    "    First Fully Connected Layer (nn.Linear):\n",
    "        This layer transforms the concatenated embeddings into a hidden representation.\n",
    "        Input: (batch_size, embed_size * sequence_length)\n",
    "        Output: (batch_size, hidden_size)\n",
    "\n",
    "        --This part helps the computer understand the meaning of a whole sentence by combining the meanings of individual words. It's like understanding the difference between \"i know you\" and \"you know me\".\n",
    "\n",
    "    Second Fully Connected Layer (nn.Linear):\n",
    "        This layer maps the hidden representation to the output size.\n",
    "        Input: (batch_size, hidden_size)\n",
    "        Output: (batch_size, output_size)\n",
    "\n",
    "        --This part decides what to say back. It takes the understanding of the sentence and turns it into a new sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5069c45",
   "metadata": {},
   "source": [
    "## Forward Pass\n",
    "\n",
    "\n",
    "### Embedding\n",
    "x = self.embedding(x) #\n",
    "Converts input indices to dense vectors.\n",
    "Shape: (batch_size, sequence_length, embed_size)\n",
    "\n",
    "### Flattening:\n",
    "x = x.view(x.size(0), -1) #\n",
    "Reshapes the tensor by flattening the last two dimensions.\n",
    "Shape: (batch_size, sequence_length * embed_size)\n",
    "\n",
    "### First Linear Layer with ReLU Activation:  \n",
    "x = torch.relu(self.fc1(x)) #\n",
    "Applies a linear transformation followed by a ReLU activation.\n",
    "Shape: (batch_size, hidden_size)\n",
    " f(x) = max(0, x)\n",
    "\n",
    "### Second Linear Layer:\n",
    "x = self.fc2(x) #\n",
    "Applies another linear transformation.\n",
    "Shape: (batch_size, output_size)\n",
    "\n",
    "#### ReLU Activation Function: ReLU stands for Rectified Linear Unit. It's a mathematical function commonly used as an activation function in artificial neural networks.\n",
    "++How it works:\n",
    "--If the input is positive, the output is the input itself.\n",
    "--If the input is negative, the output is zero.\n",
    "--ReLU(x) = max(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe4d6c0",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "A loss function quantifies the error between the model's predicted output and the true output. It's essentially a measure of how well the model is performing.\n",
    "\n",
    "### Optimizer\n",
    "An optimizer is an algorithm or method used to adjust the model's parameters (weights and biases) in order to minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f8022f",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer\n",
    "\n",
    "    - Loss Function:\n",
    "        criterion = nn.CrossEntropyLoss() defines the loss function. Cross-entropy loss is commonly used for classification tasks. It measures the difference between the predicted probabilities and the true labels.\n",
    "     - Optimizer:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001) initializes the Adam optimizer with the model's parameters and a learning rate of 0.001. Adam is an adaptive learning rate optimization algorithm that's popular for training deep learning models.\n",
    "\n",
    "## Training Loop\n",
    "\n",
    "    - Number of Epochs:\n",
    "        num_epochs = 1000 sets the number of times the entire dataset is passed through the network.\n",
    "\n",
    "    - Loop Over Epochs:\n",
    "        for epoch in range(num_epochs): iterates over the number of epochs.\n",
    "\n",
    "    - Zero the Parameter Gradients:\n",
    "        optimizer.zero_grad() clears old gradients, otherwise they would accumulate with each iteration (which is not desired).\n",
    "\n",
    "    - Forward Pass:\n",
    "        outputs = model(patterns) computes the model's output (predictions) for the given input patterns. Here, patterns is the input tensor containing encoded and padded sequences.\n",
    "\n",
    "    - Compute Loss:\n",
    "        loss = criterion(outputs.view(-1, output_size), responses.view(-1)) calculates the loss between the predicted outputs and the true responses. The view(-1, output_size) reshapes the output tensor to match the expected shape for the loss function, and view(-1) flattens the responses tensor.\n",
    "\n",
    "    - Backward Pass:\n",
    "        loss.backward() computes the gradient of the loss with respect to the model's parameters. These gradients are used to update the model parameters during optimization.\n",
    "\n",
    "     - Optimizer Step:\n",
    "        optimizer.step() updates the model parameters using the computed gradients. This step applies the optimization algorithm (Adam) to adjust the parameters and minimize the loss.\n",
    "\n",
    "    - Print Loss:\n",
    "        if (epoch + 1) % 100 == 0: checks if the current epoch is a multiple of 100.\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}') prints the epoch number and the current loss value. This helps monitor the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f20c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example padded pattern: [7, 0, 0, 0, 0]\n",
      "Example padded response: [8, 0, 0, 0, 0]\n",
      "Epoch 1: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Outuput shape: 20\n",
      "Response shape: 20\n",
      "Epoch [1/10], Loss: 2.7810\n",
      "Epoch 2: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Outuput shape: 20\n",
      "Response shape: 20\n",
      "Epoch [2/10], Loss: 2.7601\n",
      "Epoch 3: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Outuput shape: 20\n",
      "Response shape: 20\n",
      "Epoch [3/10], Loss: 2.7396\n",
      "Epoch 4: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Outuput shape: 20\n",
      "Response shape: 20\n",
      "Epoch [4/10], Loss: 2.7194\n",
      "Epoch 5: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Outuput shape: 20\n",
      "Response shape: 20\n",
      "Epoch [5/10], Loss: 2.6994\n",
      "Epoch 6: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Outuput shape: 20\n",
      "Response shape: 20\n",
      "Epoch [6/10], Loss: 2.6795\n",
      "Epoch 7: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Outuput shape: 20\n",
      "Response shape: 20\n",
      "Epoch [7/10], Loss: 2.6596\n",
      "Epoch 8: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Outuput shape: 20\n",
      "Response shape: 20\n",
      "Epoch [8/10], Loss: 2.6396\n",
      "Epoch 9: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Outuput shape: 20\n",
      "Response shape: 20\n",
      "Epoch [9/10], Loss: 2.6193\n",
      "Epoch 10: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Outuput shape: 20\n",
      "Response shape: 20\n",
      "Epoch [10/10], Loss: 2.5986\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer :\n",
    "criterion = nn.CrossEntropyLoss() #commonly used loss function for classification problems. It measures the difference between the predicted probability distribution and the actual distribution\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #popular optimization algorithm that combines the advantages of Adagrad and RMSprop. lr parameter (learning rate) determines the step size the optimizer will take when updating parameters.\n",
    "\n",
    "# Additional debugging:\n",
    "print(\"Example padded pattern:\", padded_patterns[0])\n",
    "print(\"Example padded response:\", padded_responses[0])\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Clear the gradients(rate of change of the loss function with respect to the model's parameters.) from the previous step, resets the accumulated gradients for each epoch.\n",
    "    outputs = model(patterns)  # Forward pass: Compute predicted outputs by passing inputs to the model\n",
    "\n",
    "    # Reshape outputs and responses for loss computation to match the expected input format\n",
    "    outputs = outputs.view(-1, output_size)  # Shape: (batch_size, sequence_length, output_size) to (batch_size * sequence_length, output_size)\n",
    "    responses = responses.view(-1)  # Shape: (batch_size, sequence_length) to (batch_size * sequence_length)  # Convert to long for CrossEntropyLoss\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Outputs shape: {outputs.shape}, Responses shape: {responses.shape}\")  # Debug print\n",
    "\n",
    "    # Additional debugging:\n",
    "    print(\"Outuput shape:\", outputs.shape[0])\n",
    "    print(\"Response shape:\", responses.shape[0])\n",
    "\n",
    "    #  Check for size mismatch (optional, but helpful for debugging)\n",
    "    if outputs.shape[0] != responses.shape[0]:\n",
    "        print(f\"WARNING: Mismatch in output and response sizes. Outputs: {outputs.shape}, Responses: {responses.shape}\")\n",
    "\n",
    "    # Proceed with loss calculation only if sizes match\n",
    "    else:\n",
    "      loss = criterion(outputs, responses)  # Compute the loss and difference between predicted and actual outputs.\n",
    "      loss.backward()  # Backward pass: Compute gradient of the loss with respect to model parameters\n",
    "      optimizer.step()  # Update model parameters based on computed gradients\n",
    "      # Print the loss every 100 epochs if epochs =1000\n",
    "      # if (epoch + 1) % 100 == 0:\n",
    "      print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e712d7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Outputs shape: torch.Size([4, 5, 16])\n",
      "Epoch 9: Reshaped Outputs shape: torch.Size([20, 16])\n",
      "Epoch 9: Responses shape: torch.Size([20])\n",
      "Epoch 9: Output size: torch.Size([4, 5, 16])\n",
      "Epoch 9: Target size: torch.Size([20])\n",
      "Example padded pattern: [7, 0, 0, 0, 0]\n",
      "Example padded response: 8\n",
      "Vocabulary Size: 16\n",
      "Before reshaping Outputs shape: torch.Size([4, 5, 16])\n",
      "After reshaping Outputs shape: torch.Size([20, 16])\n"
     ]
    }
   ],
   "source": [
    "#Debugging\n",
    "\n",
    "# After forward pass\n",
    "outputs = model(patterns)  # Assuming `outputs` is the model's output\n",
    "print(\"Epoch {}: Outputs shape: {}\".format(epoch, outputs.shape))\n",
    "\n",
    "# If needed, reshaping outputs for loss calculation\n",
    "outputs_reshaped = outputs.view(-1, outputs.size(-1))\n",
    "print(\"Epoch {}: Reshaped Outputs shape: {}\".format(epoch, outputs_reshaped.shape))\n",
    "\n",
    "# Print the target shape\n",
    "print(\"Epoch {}: Responses shape: {}\".format(epoch, responses.shape))\n",
    "\n",
    "# Ensure outputs and targets have matching dimensions\n",
    "print(\"Epoch {}: Output size: {}\".format(epoch, outputs.size()))\n",
    "print(\"Epoch {}: Target size: {}\".format(epoch, responses.size()))\n",
    "\n",
    "# Example padded pattern and response\n",
    "print(\"Example padded pattern: {}\".format(patterns[0].tolist()))\n",
    "print(\"Example padded response: {}\".format(responses[0].tolist()))\n",
    "\n",
    "# Assuming vocab_size is defined somewhere in your code\n",
    "print(\"Vocabulary Size: {}\".format(vocab_size))\n",
    "\n",
    "# Example with CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Debug reshaping logic\n",
    "print(\"Before reshaping Outputs shape: {}\".format(outputs.shape))\n",
    "print(\"After reshaping Outputs shape: {}\".format(outputs_reshaped.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d3f22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the bot (type 'quit' to stop)!\n",
      "You: hello\n",
      "Bot: hi good a a a\n",
      "You: how are yo\n",
      "Bot: hi good a a a\n",
      "You: how are you\n",
      "Bot: hi good how a a\n",
      "You: what is your name\n",
      "Bot: hi good chatbot a a\n",
      "You: bye\n",
      "Bot: hi a a a a\n",
      "You: bye\n",
      "Bot: hi a a a a\n",
      "You: good bye\n",
      "Bot: hi good a hello a\n",
      "You: what \n",
      "Bot: hi a a a a\n",
      "You: hello\n",
      "Bot: hi good a a a\n",
      "You: how are you?\n",
      "Bot: hi good how a a\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "#prediction\n",
    "def predict_response(input_text): #processing the input text, feeding it to a model, and generating a response.\n",
    "    input_text = preprocess(input_text) #preprocessing: perform some initial cleaning or transformation on the input text like lowercasing, removing punctuation, tokenization, or stemming.\n",
    "    input_pattern = encode(input_text) #encoding: preprocessed text is converted into a numerical representation or pattern\n",
    "    input_pattern = pad_sequence(input_pattern, max_len) #padding: ensures that all input patterns have the same length (max_len) by adding padding\n",
    "    input_pattern = torch.tensor(input_pattern, dtype=torch.long).unsqueeze(0) #tensor conversion: padded input pattern is converted into a PyTorch tensor with data type torch.long\n",
    "\n",
    "\n",
    "    output = model(input_pattern) #model predictin: repared input tensor is fed into the neural network model to generate an output\n",
    "    _, predicted = torch.max(output, dim=2) # finds the index of the most probable response for each time step in the output\n",
    "    predicted = predicted.squeeze(0).numpy() #dimension reduction: The predicted indices are squeezed to remove unnecessary dimensions and converted to a NumPy array for easier manipulation.\n",
    "\n",
    "    response_words = [all_words[idx] for idx in predicted if idx in word_to_idx.values()] #decoding: predicted indices are mapped back to words using the all_words vocabulary.\n",
    "    response_text = ' '.join(response_words) #Response Formation: redicted words are joined together to form the final response text.\n",
    "    return response_text\n",
    "\n",
    "# Chat with the bot\n",
    "print(\"Start chatting with the bot (type 'quit' to stop)!\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    response = predict_response(user_input)\n",
    "    print(f\"Bot: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929e8dc",
   "metadata": {},
   "source": [
    "#### predict_response Function:\n",
    "--predict_response function is designed to take a user input, process it, feed it to a trained model, and generate a corresponding output. It's a crucial component of a chatbot system.\n",
    "\n",
    "#### Preprocesses and encodes the input text:\n",
    "--Encoding: Each token is assigned a numerical representation (index) based on a predefined vocabulary.\n",
    "\n",
    "#### Pads the sequence to a fixed length:\n",
    "---The encoded sequence is padded or truncated to match a fixed length required by the model. This ensures consistency in input shape.\n",
    "\n",
    "#### Converts the sequence to a tensor and passes it through the model:\n",
    "--padded sequence is converted into a PyTorch tensor, a data structure optimized for numerical computations.\n",
    "\n",
    "#### Prediction:\n",
    "-- Index to Word Mapping: Extracts the predicted indices and converts them back to words using the vocabulary.\n",
    "\n",
    "-- Text Generation: Joins the words to form the final response text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98521b6f",
   "metadata": {},
   "source": [
    "#Solved Working Chatbot\n",
    "##but not as expected!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc877c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Pattern Length: 4\n",
      "Max Response Length: 5\n",
      "Max Length: 5\n",
      "Padded Pattern: [[7, 0, 0, 0, 0], [9, 2, 14, 0, 0], [13, 11, 15, 12, 0], [3, 0, 0, 0, 0]]\n",
      "Padded Response: [[8, 0, 0, 0, 0], [10, 5, 9, 1, 14], [10, 0, 4, 0, 0], [6, 0, 0, 0, 0]]\n",
      "Example padded pattern: [7, 0, 0, 0, 0]\n",
      "Example padded response: [8, 0, 0, 0, 0]\n",
      "Epoch 1: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Epoch [1/10], Loss: 2.7511\n",
      "Epoch 2: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Epoch [2/10], Loss: 2.7322\n",
      "Epoch 3: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Epoch [3/10], Loss: 2.7132\n",
      "Epoch 4: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Epoch [4/10], Loss: 2.6941\n",
      "Epoch 5: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Epoch [5/10], Loss: 2.6748\n",
      "Epoch 6: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Epoch [6/10], Loss: 2.6551\n",
      "Epoch 7: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Epoch [7/10], Loss: 2.6349\n",
      "Epoch 8: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Epoch [8/10], Loss: 2.6141\n",
      "Epoch 9: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Epoch [9/10], Loss: 2.5926\n",
      "Epoch 10: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n",
      "Epoch [10/10], Loss: 2.5704\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = [\n",
    "    (\"hello\", \"hi\"),\n",
    "    (\"how are you\", \"I'm good, how about you?\"),\n",
    "    (\"what is your name\", \"I'm a chatbot\"),\n",
    "    (\"bye\", \"goodbye\"),\n",
    "]\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # removes all characters from the input text that are not word characters or whitespace\n",
    "    return text\n",
    "\n",
    "# Vocabulary\n",
    "all_words = []\n",
    "for (pattern, response) in data:\n",
    "    pattern = preprocess(pattern)\n",
    "    response = preprocess(response)\n",
    "    words = pattern.split() + response.split() #splits it into a list of words, using whitespace as the delimiter: \"hello Ameer\" to [\"hello\",\"Ameer\"]\n",
    "    all_words.extend(words) #appends the elements of the list words to the end of the list all_words\n",
    "all_words = sorted(set(all_words))\n",
    "\n",
    "# Word to index mapping\n",
    "word_to_idx = {word: idx for idx, word in enumerate(all_words)} #enumerate iterates on pairs of (index, value) tuples.\n",
    "\n",
    "# Encode patterns and responses\n",
    "def encode(text):\n",
    "    text = preprocess(text)\n",
    "    return [word_to_idx[word] for word in text.split() if word in word_to_idx]\n",
    "\n",
    "encoded_data = [(encode(pattern), encode(response)) for (pattern, response) in data]\n",
    "\n",
    "# Pad sequences: pad_sequence function is used to ensure that all sequences in a dataset have the same length\n",
    "def pad_sequence(seq, max_len, padding_value=0): #seq is input sequence that needs to be padded, max_len is desired length for all sequences, padding_val is value used to fill the sequence to reach the maximum length\n",
    "    return seq + [padding_value] * (max_len - len(seq)) #If the input sequence is shorter than the max_len, it appends padding_value to the end of the sequence until it reaches the desired length.\n",
    "\n",
    "# Determine the maximum length of patterns and responses\n",
    "max_pattern_len = max(len(pattern) for pattern, response in encoded_data)\n",
    "max_response_len = max(len(response) for pattern, response in encoded_data)\n",
    "max_len = max(max_pattern_len, max_response_len)\n",
    "\n",
    "# Pad patterns and responses\n",
    "padded_patterns = [pad_sequence(pattern, max_len) for pattern, response in encoded_data]\n",
    "padded_responses = [pad_sequence(response, max_len) for pattern, response in encoded_data]\n",
    "\n",
    "print(f\"Max Pattern Length: {max_pattern_len}\")\n",
    "print(f\"Max Response Length: {max_response_len}\")\n",
    "print(f\"Max Length: {max_len}\")\n",
    "print(f\"Padded Pattern: {padded_patterns}\")\n",
    "print(f\"Padded Response: {padded_responses}\")\n",
    "\n",
    "# Additional debugging:\n",
    "print(\"Example padded pattern:\", padded_patterns[0])\n",
    "print(\"Example padded response:\", padded_responses[0])\n",
    "\n",
    "# Convert to tensors: preparing it for training for efficient computations, leverages GPU acceleration, and integrates seamlessly with the PyTorch ecosystem.\n",
    "patterns = torch.tensor(padded_patterns, dtype=torch.long)\n",
    "responses = torch.tensor(padded_responses, dtype=torch.long)\n",
    "\n",
    "class ChatbotModel(nn.Module): # Defines a basic chatbot model architecture using PyTorch\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, max_len): # Initializes the model with\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * max_len, output_size * max_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.contiguous().view(x.size(0), -1)  # Flatten the output for the linear layer\n",
    "        x = self.fc(x)\n",
    "        return x.view(x.size(0), max_len, -1)  # Reshape to (batch_size, max_len, output_size)\n",
    "\n",
    "# Initialize model\n",
    "vocab_size = len(all_words)\n",
    "embed_size = 10\n",
    "hidden_size = 20\n",
    "output_size = vocab_size  # Output size should match the vocabulary size\n",
    "model = ChatbotModel(vocab_size, embed_size, hidden_size, output_size, max_len)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss() #commonly used loss function for classification problems. It measures the difference between the predicted probability distribution and the actual distribution\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #popular optimization algorithm that combines the advantages of Adagrad and RMSprop. lr parameter (learning rate) determines the step size the optimizer will take when updating parameters.\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Clear the gradients(rate of change of the loss function with respect to the model's parameters.) from the previous step, resets the accumulated gradients for each epoch.\n",
    "    outputs = model(patterns)  # Forward pass: Compute predicted outputs by passing inputs to the model\n",
    "\n",
    "    # Reshape outputs and responses for loss computation to match the expected input format\n",
    "    outputs = outputs.view(-1, output_size)  # Shape: (batch_size * sequence_length, output_size)\n",
    "    responses = responses.view(-1)  # Shape: (batch_size * sequence_length)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Outputs shape: {outputs.shape}, Responses shape: {responses.shape}\")  # Debug print\n",
    "\n",
    "    # Check for size mismatch (optional, but helpful for debugging)\n",
    "    if outputs.shape[0] != responses.shape[0]:\n",
    "        print(f\"WARNING: Mismatch in output and response sizes. Outputs: {outputs.shape}, Responses: {responses.shape}\")\n",
    "\n",
    "    # Proceed with loss calculation only if sizes match\n",
    "    else:\n",
    "        loss = criterion(outputs, responses)  # Compute the loss and difference between predicted and actual outputs.\n",
    "        loss.backward()  # Backward pass: Compute gradient of the loss with respect to model parameters\n",
    "        optimizer.step()  # Update model parameters based on computed gradients\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e4ff69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the bot (type 'quit' to stop)!\n",
      "You: hi\n",
      "Bot: hi good a are a\n",
      "You: how are you\n",
      "Bot: hi good how a a\n",
      "You: what is your name\n",
      "Bot: hi good chatbot a a\n",
      "You: name\n",
      "Bot: hi a a a a\n",
      "You: name\n",
      "Bot: hi a a a a\n",
      "You: name what\n",
      "Bot: hi a a a a\n",
      "You: hi\n",
      "Bot: hi good a are a\n",
      "You: bye\n",
      "Bot: hi a a a a\n",
      "You: how are you doing bot\n",
      "Bot: hi good how a a\n",
      "You: what is your name\n",
      "Bot: hi good chatbot a a\n",
      "You: dont say hi\n",
      "Bot: hi good a are a\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "def predict_response(input_text):\n",
    "    input_text = preprocess(input_text)\n",
    "    input_pattern = encode(input_text)\n",
    "    input_pattern = pad_sequence(input_pattern, max_len)\n",
    "    input_pattern = torch.tensor(input_pattern, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients for inference\n",
    "        output = model(input_pattern)  # Predict output\n",
    "\n",
    "    output = output.view(-1, output_size)  # Flatten output to (batch_size * sequence_length, output_size)\n",
    "    _, predicted = torch.max(output, dim=1)  # Find the index of the most probable response\n",
    "\n",
    "    predicted = predicted.numpy()  # Convert to NumPy array\n",
    "\n",
    "    response_words = [all_words[idx] for idx in predicted if idx < len(all_words)]  # Decode predicted indices\n",
    "    response_text = ' '.join(response_words)  # Form response text\n",
    "    return response_text\n",
    "\n",
    "# Chat with the bot\n",
    "print(\"Start chatting with the bot (type 'quit' to stop)!\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    response = predict_response(user_input)\n",
    "    print(f\"Bot: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
