{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cb8aa81-2b8a-4937-be5c-20d756fd8efa",
   "metadata": {},
   "source": [
    "### Types of Chatbots:\n",
    "\n",
    "There are broadly two variants of chatbots: Rule-Based and Self-learning.\n",
    "\n",
    "    1. In a Rule-based approach, a bot answers questions based on some rules, which it is trained on. The rules defined can be very simple to very complex. The bots can handle simple queries but fail to manage complex ones.\n",
    "    \n",
    "    2. Self-learning bots are the ones that use some Machine Learning-based approaches and are more efficient than rule-based bots. These bots can be of further two types: Retrieval Based or Generative. \n",
    "\n",
    "\n",
    "    2.a) In retrieval-based models, a chatbot uses some heuristic to select a response from a library of predefined responses. The chatbot uses the message and context of the conversation for choosing the best response from a predefined list of bot messages. The context can include a current position in the dialogue tree, all previous messages in the conversation, previously saved variables (e.g., username). Heuristics for selecting a response can be engineered in many different ways, from rule-based if-else conditional logic to machine learning classifiers.\n",
    "\n",
    "    2. b) Generative bots can generate the answers and not always replies with one of the answers from a set of answers. This makes them more intelligent as they take word by word from the query and generates the answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89c6752-cda5-46c1-9b04-60a8db4a1745",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "Retrieval-Based Chatbot Example:\n",
    "\n",
    "\n",
    "Predefined Responses:\n",
    "\n",
    "    \"The library is open from 9 AM to 5 PM on weekdays.\"\n",
    "    \"You can borrow up to 5 books at a time.\"\n",
    "    \"To renew a book, please visit the library's website or contact the help desk.\"\n",
    "\n",
    "Conversation:\n",
    "User: \"What are the library's opening hours?\"\n",
    "Bot: \"The library is open from 9 AM to 5 PM on weekdays.\"\n",
    "\n",
    "How it works:\n",
    "\n",
    "    Message: \"What are the library's opening hours?\"\n",
    "    Heuristic: The bot matches the user's message to the closest predefined response using keywords or patterns (e.g., \"opening hours\").\n",
    "    Selected Response: \"The library is open from 9 AM to 5 PM on weekdays.\"\n",
    "\n",
    "Generative Chatbot Example:\n",
    "\n",
    "Imagine a more advanced chatbot that can generate responses on the fly.\n",
    "\n",
    "Conversation:\n",
    "User: \"What are the library's opening hours?\"\n",
    "Bot: \"The library is open from 9 AM to 5 PM on weekdays, but it is closed on weekends.\"\n",
    "\n",
    "How it works:\n",
    "\n",
    "    Message: \"What are the library's opening hours?\"\n",
    "    Generative Model: The bot processes the input using a neural network that has been trained on a large dataset of conversational text. It generates a response word by word.\n",
    "    Generated Response: \"The library is open from 9 AM to 5 PM on weekdays, but it is closed on weekends.\"\n",
    "\n",
    "Key Differences:\n",
    "\n",
    "    Retrieval-Based Bot:\n",
    "        Response Source: Predefined responses.\n",
    "        Selection Method: Heuristics like keyword matching or pattern recognition.\n",
    "        Flexibility: Limited to the responses it has been given.\n",
    "\n",
    "    Generative Bot:\n",
    "        Response Source: Generates responses dynamically.\n",
    "        Selection Method: Uses machine learning models to create a response based on the input.\n",
    "        Flexibility: More adaptable and can handle a wider range of queries with nuanced answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d3d84-a13c-485c-a46b-0361ce48ab72",
   "metadata": {},
   "source": [
    "### Text Pre- Processing with NLTK\n",
    "\n",
    "The main issue with text data is that it is all in text format (strings). However, Machine learning algorithms need some sort of numerical feature vector to perform the task. So before we start with any NLP project, we need to pre-process it to make it ideal for work. Basic text pre-processing includes:\n",
    "\n",
    "    Converting the entire text into uppercase or lowercase so that the algorithm does not treat the same words in different cases as different\n",
    "    \n",
    "### Tokenization\n",
    "    Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens, i.e., words that we want. A sentence tokenizer can be used to find the list of sentences, and a Word tokenizer can be used to find the list of words in strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669eb128-9cd6-4838-88fe-e49a7400bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercase\n",
    "import nltk\n",
    "nltk.download('punkt')  # Downloading the punkt tokenizer models\n",
    "\n",
    "text = \"Natural Language Processing with NLTK is Fun!\"\n",
    "text_lowercase = text.lower()\n",
    "print(text_lowercase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0108a-5503-4c1d-bb76-e342a296759d",
   "metadata": {},
   "source": [
    "### Tokenization:\n",
    "\n",
    "Purpose: Breaking down the text into smaller pieces like sentences or words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5c37951-1b24-4514-8b46-1d78625c246c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentencewise Hello World.', 'Natural Language Processing with NLTK is Fun!']\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Sentencewise Hello World. Natural Language Processing with NLTK is Fun!\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a480f7-764e-4627-961a-40d1baac2b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wordwise', ':', 'Natural', 'Language', 'Processing', 'with', 'NLTK', 'is', 'Fun', '!']\n"
     ]
    }
   ],
   "source": [
    "#word tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Wordwise: Natural Language Processing with NLTK is Fun!\"\n",
    "words = word_tokenize(text)\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14df75c-2cf0-4266-b5ea-0cc8bb8184f3",
   "metadata": {},
   "source": [
    "## Term Frequency(TF) and Inverse Document Frequency (IDF):\n",
    "\n",
    "Term Frequency (TF) and Inverse Document Frequency (IDF) are fundamental concepts in Natural Language Processing (NLP) used to measure the importance of a word in a document relative to a collection of documents (corpus).\n",
    "\n",
    "a) Term Frequency (TF):\n",
    "TF measures how frequently a term appears in a document. It is the ratio of the number of times a term appears in a document to the total number of terms in the document.\n",
    "\n",
    "Formula:\n",
    "TF(t,d)=Number of times term t appears in document d / Total number of terms in document d\n",
    "\n",
    "Example: TF(t,d)= 3/100 =0.03\n",
    "\n",
    "b) Inverse Document Frequency (IDF):\n",
    "IDF measures how important a term is. While computing TF, all terms are considered equally important. However, certain terms like \"is\", \"of\", and \"that\" may appear frequently but have little importance. IDF weighs down the frequent terms while scaling up the rare ones.\n",
    "\n",
    "Formula:\n",
    "IDF(t,D)=log⁡(Total number of documents (N) / Number of documents with term t)\n",
    "\n",
    "Example: IDF(t,D)=log(1000/10)=log(100)≈2\n",
    "\n",
    "c) TF-IDF:\n",
    "TF-IDF is a numerical statistic that is intended to reflect how important a word is to a document in a corpus.\n",
    "\n",
    "Formula:\n",
    "TF-IDF(t,d,D)= TF(t,d) × IDF(t,D)\n",
    "\n",
    "Example: TF-IDF(t,d,D)=0.03×2=0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04ad2ace-3838-4775-98bc-3d6e0beb3178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document 1 TF-IDF scores:\n",
      "natural: 0.0405\n",
      ",: 0.2197\n",
      "language: 0.0405\n",
      "processing: 0.0000\n",
      "with: 0.0405\n",
      "nltk: 0.0405\n",
      "is: 0.0405\n",
      "fun: 0.1099\n",
      ".: 0.0000\n",
      "\n",
      "Document 2 TF-IDF scores:\n",
      "natural: 0.0405\n",
      "language: 0.0405\n",
      "processing: 0.0000\n",
      "and: 0.0405\n",
      "machine: 0.1099\n",
      "learning: 0.1099\n",
      "are: 0.1099\n",
      "closely: 0.1099\n",
      "related: 0.1099\n",
      ".: 0.0000\n",
      "\n",
      "Document 3 TF-IDF scores:\n",
      "text: 0.1221\n",
      "processing: 0.0000\n",
      "with: 0.0451\n",
      "nltk: 0.0451\n",
      "and: 0.0451\n",
      "python: 0.1221\n",
      "is: 0.0451\n",
      "powerful: 0.1221\n",
      ".: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\A S P I R E\n",
      "[nltk_data]     7\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"Natural, Language, Processing with NLTK is fun.\",\n",
    "    \"Natural Language Processing and machine learning are closely related.\",\n",
    "    \"Text processing with NLTK and Python is powerful.\"\n",
    "]\n",
    "\n",
    "# Step 1: Convert to lowercase and tokenize the text\n",
    "tokenized_documents = [nltk.word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "# Step 2: Calculate Term Frequency (TF)\n",
    "def compute_tf(word_dict, doc):\n",
    "    tf_dict = {}\n",
    "    doc_count = len(doc)\n",
    "    for word, count in word_dict.items():\n",
    "        tf_dict[word] = count / float(doc_count)\n",
    "    return tf_dict\n",
    "\n",
    "# Compute TF for each document\n",
    "tf_documents = []\n",
    "for doc in tokenized_documents:\n",
    "    word_counts = Counter(doc)\n",
    "    tf_documents.append(compute_tf(word_counts, doc))\n",
    "\n",
    "# Step 3: Calculate Inverse Document Frequency (IDF)\n",
    "def compute_idf(documents):\n",
    "    N = len(documents)\n",
    "    unique_words = set(word for doc in documents for word in doc)\n",
    "    idf_dict = dict.fromkeys(unique_words, 0)\n",
    "    # idf_dict = dict.fromkeys(documents[0], 0)\n",
    "    for doc in documents:\n",
    "        for word in set(doc):\n",
    "            idf_dict[word] += 1\n",
    "    for word, val in idf_dict.items():\n",
    "        idf_dict[word] = math.log(N / float(val))\n",
    "    return idf_dict\n",
    "\n",
    "# Compute IDF\n",
    "idf_dict = compute_idf(tokenized_documents)\n",
    "\n",
    "# Step 4: Calculate TF-IDF\n",
    "def compute_tfidf(tf_doc, idf_dict):\n",
    "    tfidf_dict = {}\n",
    "    for word, tf_val in tf_doc.items():\n",
    "        tfidf_dict[word] = tf_val * idf_dict[word]\n",
    "    return tfidf_dict\n",
    "\n",
    "# Compute TF-IDF for each document\n",
    "tfidf_documents = [compute_tfidf(tf_doc, idf_dict) for tf_doc in tf_documents]\n",
    "\n",
    "# Print results\n",
    "for i, doc in enumerate(tfidf_documents):\n",
    "    print(f\"\\nDocument {i+1} TF-IDF scores:\")\n",
    "    for word, score in doc.items():\n",
    "        print(f\"{word}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c98e18fb-8dc5-4aeb-8c93-6f49227d8738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 2, 2: 2, 32: 2})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "a = [1,2,32,1,2,32]\n",
    "Counter(a) #gives number of digits in dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e654947-45c1-439d-977f-51ef37b95b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
