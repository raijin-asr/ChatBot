{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dedb960-ef39-42ac-b9a0-1e9f3e4c1ee2",
   "metadata": {
    "id": "7dedb960-ef39-42ac-b9a0-1e9f3e4c1ee2"
   },
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ab7d9-a73b-4f65-9897-994beebd7043",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a7ab7d9-a73b-4f65-9897-994beebd7043",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "3ae493ab-5634-40fe-fc8b-445a605156d1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using Device: {device}\")\n",
    "\n",
    "    device_no = torch.cuda.current_device()\n",
    "    print(f\"Current device number is: {device_no}\")\n",
    "\n",
    "    device_name = torch.cuda.get_device_name(device_no)\n",
    "    print(f\"GPU name is: {device_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aef3830-fd37-47a9-b452-e42a03d45fab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aef3830-fd37-47a9-b452-e42a03d45fab",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "16562992-c2da-40bd-da52-35cfe7e2992f"
   },
   "outputs": [],
   "source": [
    "import torch #an open source ML library used for creating deep neural networks\n",
    "import torch.nn as nn # A module in PyTorch that provides classes and functions to build neural networks\n",
    "import torch.optim as optim # A module in PyTorch that provides various optimization algorithms for training neural networks\n",
    "import random # A module that implements pseudo-random number generators for various distributions\n",
    "import re # A module for working with regular expressions to match and manipulate strings\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = [\n",
    "    (\"hello\", \"hi\"),\n",
    "    (\"how are you\", \"I'm good, how about you?\"),\n",
    "    (\"what is your name\", \"I'm a chatbot\"),\n",
    "    (\"bye\", \"goodbye\"),\n",
    "]\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # removes all characters from the input text that are not word characters or whitespace\n",
    "    return text\n",
    "\n",
    "# Vocabulary\n",
    "all_words = []\n",
    "for (pattern, response) in data:\n",
    "    pattern = preprocess(pattern)\n",
    "    response = preprocess(response)\n",
    "    words = pattern.split() + response.split() #splits it into a list of words, using whitespace as the delimiter: \"hello Ameer\" to [\"hello\",\"Aeer\"]\n",
    "    all_words.extend(words) #appends the elements of the list words to the end of the list all_words\n",
    "all_words = sorted(set(all_words))\n",
    "\n",
    "\n",
    "# Word to index mapping\n",
    "word_to_idx = {word: idx for idx, word in enumerate(all_words)} #enumerate iterates on pairs of (index, value) tuples.\n",
    "\n",
    "# Encode patterns and responses\n",
    "def encode(text):\n",
    "    text = preprocess(text)\n",
    "    return [word_to_idx[word] for word in text.split() if word in word_to_idx]\n",
    "\n",
    "encoded_data = [(encode(pattern), encode(response)) for (pattern, response) in data]\n",
    "\n",
    "# Pad sequences: ad_sequence function is used to ensure that all sequences in a dataset have the same length\n",
    "def pad_sequence(seq, max_len, padding_value=0): #seq is input sequence that needs to be padded, max_len is desired length for all sequences, padding_val is alue used to fill the sequence to reach the maximum length\n",
    "    return seq + [padding_value] * (max_len - len(seq)) #If the input sequence is shorter than the max_len, it appends padding_value to the end of the sequence until it reaches the desired length.\n",
    "\n",
    "# Determine the maximum length of patterns and responses\n",
    "max_pattern_len = max(len(pattern) for pattern, response in encoded_data)\n",
    "max_response_len = max(len(response) for pattern, response in encoded_data)\n",
    "max_len = max(max_pattern_len, max_response_len)\n",
    "\n",
    "# Pad patterns and responses\n",
    "padded_patterns = [pad_sequence(pattern, max_len) for pattern, response in encoded_data]\n",
    "padded_responses = [pad_sequence(response, max_len) for pattern, response in encoded_data]\n",
    "\n",
    "print(f\"Max Pattern Length: {max_pattern_len}\")\n",
    "print(f\"Max Response Length: {max_response_len}\")\n",
    "# print(f\"Max Length: {max_len}\")\n",
    "print(f\"Padded Pattern: {padded_patterns}\")\n",
    "print(f\"Padded Response: {padded_responses}\")\n",
    "\n",
    "# Additional debugging:\n",
    "print(\"Example padded pattern:\", padded_patterns[0])\n",
    "print(\"Example padded response:\", padded_responses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7042a-c4c5-46f2-a3bb-08590ad2af11",
   "metadata": {
    "id": "18c7042a-c4c5-46f2-a3bb-08590ad2af11",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Convert to tensors: preparing it for training for efficient computations, leverages GPU acceleration, and integrates seamlessly with the PyTorch ecosystem.\n",
    "patterns = torch.tensor(padded_patterns, dtype=torch.long)\n",
    "responses = torch.tensor(padded_responses, dtype=torch.long)\n",
    "\n",
    "# patterns = torch.tensor([pattern for pattern in padded_patterns], dtype=torch.long)\n",
    "# responses = torch.tensor([response for response in padded_responses], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69b53d-12fe-433d-82c1-0212eb73fc0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec69b53d-12fe-433d-82c1-0212eb73fc0c",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "34eee53e-a5da-42ea-95a2-696816b77481"
   },
   "outputs": [],
   "source": [
    "patterns # gives 2D tensor intergers with 4 rows and 4 columns, making it a 4x4 matrix.\n",
    "# responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6c4b3-c4da-41bd-90dd-5ffa322960d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5f6c4b3-c4da-41bd-90dd-5ffa322960d7",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "c3661ba8-05f3-4bf0-8b0e-8555ac486055"
   },
   "outputs": [],
   "source": [
    "class ChatbotModel(nn.Module): #defines a basic chatbot model architecture using PyTorch\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size,max_len): #Initializes the model with hyperparameters\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size) #An embedding layer to convert word indices to dense vectors. word indices are numerical representations of words in a vocabulary. vocabulary:[\"AMeer\", \"Rai\"], and word indices: \"Ameer\":0, \"rai\":1\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * max_len, output_size * max_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.contiguous().view(x.size(0), -1)  # Flatten the output for the linear layer\n",
    "        x = self.fc(x)\n",
    "        return x.view(x.size(0), max_len, -1)  # Reshape to (batch_size, max_len, output_size)\n",
    "\n",
    "# Initialize model\n",
    "vocab_size = len(all_words)\n",
    "embed_size = 10 #Dimensionality of word embeddings.\n",
    "hidden_size = 20 #number of neurons in the hidden laye\n",
    "# output_size = max_response_len #Size of the output layer (likely the maximum length of a response\n",
    "output_size = vocab_size  # Output size should match the vocabulary size\n",
    "\n",
    "\n",
    "max_len = max(max_pattern_len, max_response_len)\n",
    "model = ChatbotModel(vocab_size, embed_size, hidden_size, output_size,max_len)  #simple feed-forward neural network for generating chatbot responses\n",
    "print(model)\n",
    "\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "print(f\"Output Size: {output_size}\")\n",
    "print(f\"Word to Index Mapping: {word_to_idx}\")\n",
    "print(f\"Patterns Tensor: {patterns}\")\n",
    "print(f\"Max Length: {max_len}\")\n",
    "\n",
    "for pattern in patterns:\n",
    "    for idx in pattern:\n",
    "        if idx >= vocab_size:\n",
    "            print(f\"Invalid Index: {idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6228d7-94c0-47cc-9bc8-ba9921a1a096",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ad6228d7-94c0-47cc-9bc8-ba9921a1a096",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "84e37289-4865-4f6d-84e2-99899dc1381a"
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer :\n",
    "criterion = nn.CrossEntropyLoss() #commonly used loss function for classification problems. It measures the difference between the predicted probability distribution and the actual distribution\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #popular optimization algorithm that combines the advantages of Adagrad and RMSprop. lr parameter (learning rate) determines the step size the optimizer will take when updating parameters.\n",
    "\n",
    "# Additional debugging:\n",
    "print(\"Example padded pattern:\", padded_patterns[0])\n",
    "print(\"Example padded response:\", padded_responses[0])\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Clear the gradients(rate of change of the loss function with respect to the model's parameters.) from the previous step, resets the accumulated gradients for each epoch.\n",
    "    outputs = model(patterns)  # Forward pass: Compute predicted outputs by passing inputs to the model\n",
    "\n",
    "    # Reshape outputs and responses for loss computation to match the expected input format\n",
    "    outputs = outputs.view(-1, output_size)  # Shape: (batch_size, sequence_length, output_size) to (batch_size * sequence_length, output_size)\n",
    "    responses = responses.view(-1)  # Shape: (batch_size, sequence_length) to (batch_size * sequence_length)  # Convert to long for CrossEntropyLoss\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Outputs shape: {outputs.shape}, Responses shape: {responses.shape}\")  # Debug print\n",
    "\n",
    "    # Additional debugging:\n",
    "    print(\"Outuput shape:\", outputs.shape[0])\n",
    "    print(\"Response shape:\", responses.shape[0])\n",
    "\n",
    "    #  Check for size mismatch (optional, but helpful for debugging)\n",
    "    if outputs.shape[0] != responses.shape[0]:\n",
    "        print(f\"WARNING: Mismatch in output and response sizes. Outputs: {outputs.shape}, Responses: {responses.shape}\")\n",
    "\n",
    "    # Proceed with loss calculation only if sizes match\n",
    "    else:\n",
    "      loss = criterion(outputs, responses)  # Compute the loss and difference between predicted and actual outputs.\n",
    "      loss.backward()  # Backward pass: Compute gradient of the loss with respect to model parameters\n",
    "      optimizer.step()  # Update model parameters based on computed gradients\n",
    "      # Print the loss every 100 epochs if epochs =1000\n",
    "      # if (epoch + 1) % 100 == 0:\n",
    "      print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NOb7supH_IeF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOb7supH_IeF",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "a24d4e8d-586a-4f90-e5b2-56e568b8eb29"
   },
   "outputs": [],
   "source": [
    "#Debugging\n",
    "\n",
    "# After forward pass\n",
    "outputs = model(patterns)  # Assuming `outputs` is the model's output\n",
    "print(\"Epoch {}: Outputs shape: {}\".format(epoch, outputs.shape))\n",
    "\n",
    "# If needed, reshaping outputs for loss calculation\n",
    "outputs_reshaped = outputs.view(-1, outputs.size(-1))\n",
    "print(\"Epoch {}: Reshaped Outputs shape: {}\".format(epoch, outputs_reshaped.shape))\n",
    "\n",
    "# Print the target shape\n",
    "print(\"Epoch {}: Responses shape: {}\".format(epoch, responses.shape))\n",
    "\n",
    "# Ensure outputs and targets have matching dimensions\n",
    "print(\"Epoch {}: Output size: {}\".format(epoch, outputs.size()))\n",
    "print(\"Epoch {}: Target size: {}\".format(epoch, responses.size()))\n",
    "\n",
    "# Example padded pattern and response\n",
    "print(\"Example padded pattern: {}\".format(patterns[0].tolist()))\n",
    "print(\"Example padded response: {}\".format(responses[0].tolist()))\n",
    "\n",
    "# Assuming vocab_size is defined somewhere in your code\n",
    "print(\"Vocabulary Size: {}\".format(vocab_size))\n",
    "\n",
    "# Example with CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Debug reshaping logic\n",
    "print(\"Before reshaping Outputs shape: {}\".format(outputs.shape))\n",
    "print(\"After reshaping Outputs shape: {}\".format(outputs_reshaped.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b2f92-f53d-432f-a379-4cf4e68f96df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b93b2f92-f53d-432f-a379-4cf4e68f96df",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "804ccdcf-d858-477e-b901-6ed727b5a6b7"
   },
   "outputs": [],
   "source": [
    "#prediction\n",
    "def predict_response(input_text): #processing the input text, feeding it to a model, and generating a response.\n",
    "    input_text = preprocess(input_text) #preprocessing: perform some initial cleaning or transformation on the input text like lowercasing, removing punctuation, tokenization, or stemming.\n",
    "    input_pattern = encode(input_text) #encoding: preprocessed text is converted into a numerical representation or pattern\n",
    "    input_pattern = pad_sequence(input_pattern, max_len) #padding: ensures that all input patterns have the same length (max_len) by adding padding\n",
    "    input_pattern = torch.tensor(input_pattern, dtype=torch.long).unsqueeze(0) #tensor conversion: padded input pattern is converted into a PyTorch tensor with data type torch.long\n",
    "\n",
    "\n",
    "    output = model(input_pattern) #model predictin: repared input tensor is fed into the neural network model to generate an output\n",
    "    _, predicted = torch.max(output, dim=2) # finds the index of the most probable response for each time step in the output\n",
    "    predicted = predicted.squeeze(0).numpy() #dimension reduction: The predicted indices are squeezed to remove unnecessary dimensions and converted to a NumPy array for easier manipulation.\n",
    "\n",
    "    response_words = [all_words[idx] for idx in predicted if idx in word_to_idx.values()] #decoding: predicted indices are mapped back to words using the all_words vocabulary.\n",
    "    response_text = ' '.join(response_words) #Response Formation: redicted words are joined together to form the final response text.\n",
    "    return response_text\n",
    "\n",
    "# Chat with the bot\n",
    "print(\"Start chatting with the bot (type 'quit' to stop)!\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    response = predict_response(user_input)\n",
    "    print(f\"Bot: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4z8v0qkI_PF8",
   "metadata": {
    "id": "4z8v0qkI_PF8"
   },
   "source": [
    "# Solved Working Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a707ae-0e0f-4e41-8ffe-449b73fdafff",
   "metadata": {
    "id": "4z8v0qkI_PF8"
   },
   "source": [
    "## 1. Training Chatbot (10 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46b4eb9b-a620-4a99-a8ca-af561cd47b50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46b4eb9b-a620-4a99-a8ca-af561cd47b50",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "5886dfed-d325-47b4-9fff-bbedbbd3089d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 7.8607\n",
      "Epoch [2/10], Loss: 7.7759\n",
      "Epoch [3/10], Loss: 7.6911\n",
      "Epoch [4/10], Loss: 7.6032\n",
      "Epoch [5/10], Loss: 7.5107\n",
      "Epoch [6/10], Loss: 7.4130\n",
      "Epoch [7/10], Loss: 7.3097\n",
      "Epoch [8/10], Loss: 7.2004\n",
      "Epoch [9/10], Loss: 7.0846\n",
      "Epoch [10/10], Loss: 6.9621\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = [line.strip().split('\\t') for line in file.readlines()]\n",
    "    return data\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Vocabulary and encoding\n",
    "def build_vocab(data):\n",
    "    all_words = []\n",
    "    for (pattern, response) in data:\n",
    "        pattern = preprocess(pattern)\n",
    "        response = preprocess(response)\n",
    "        words = pattern.split() + response.split()\n",
    "        all_words.extend(words)\n",
    "    all_words = sorted(set(all_words))\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "    return word_to_idx, all_words\n",
    "\n",
    "def encode(text, word_to_idx):\n",
    "    text = preprocess(text)\n",
    "    return [word_to_idx[word] for word in text.split() if word in word_to_idx]\n",
    "\n",
    "def build_vocab_and_encode(data):\n",
    "    word_to_idx, all_words = build_vocab(data)\n",
    "    encoded_data = [(encode(pattern, word_to_idx), encode(response, word_to_idx)) for (pattern, response) in data]\n",
    "    return encoded_data, word_to_idx, all_words\n",
    "\n",
    "# Pad sequences\n",
    "def pad_sequence(seq, max_len, padding_value=0):\n",
    "    return seq + [padding_value] * (max_len - len(seq))\n",
    "\n",
    "# Load and process dataset\n",
    "file_path = 'dataset/chatbot-dialogs.txt'  # Path to your dataset file\n",
    "data = load_dataset(file_path)\n",
    "encoded_data, word_to_idx, all_words = build_vocab_and_encode(data)\n",
    "\n",
    "# Determine maximum length\n",
    "max_pattern_len = max(len(pattern) for pattern, response in encoded_data)\n",
    "max_response_len = max(len(response) for pattern, response in encoded_data)\n",
    "max_len = max(max_pattern_len, max_response_len)\n",
    "\n",
    "# Pad patterns and responses\n",
    "padded_patterns = [pad_sequence(pattern, max_len) for pattern, response in encoded_data]\n",
    "padded_responses = [pad_sequence(response, max_len) for pattern, response in encoded_data]\n",
    "\n",
    "# Convert to tensors\n",
    "patterns = torch.tensor(padded_patterns, dtype=torch.long)\n",
    "responses = torch.tensor(padded_responses, dtype=torch.long)\n",
    "\n",
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, max_len):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hiddequin_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * max_len, output_size * max_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x.view(x.size(0), max_len, -1)\n",
    "\n",
    "# Initialize model\n",
    "vocab_size = len(all_words)\n",
    "embed_size = 10\n",
    "hidden_size = 20\n",
    "output_size = vocab_size\n",
    "model = ChatbotModel(vocab_size, embed_size, hidden_size, output_size, max_len)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(patterns)\n",
    "\n",
    "    outputs = outputs.view(-1, output_size)\n",
    "    responses = responses.view(-1)\n",
    "\n",
    "    if outputs.shape[0] != responses.shape[0]:\n",
    "        print(f\"WARNING: Mismatch in output and response sizes. Outputs: {outputs.shape}, Responses: {responses.shape}\")\n",
    "    else:\n",
    "        loss = criterion(outputs, responses)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f5e01-8fcd-40ff-bbb2-5d87f331d3be",
   "metadata": {
    "id": "4z8v0qkI_PF8"
   },
   "source": [
    "## Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1955292e-5db1-4cf8-a52b-24184d1b9f50",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the bot (type 'quit' to stop)!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi, how are you doing?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: if how matter how and to is this 011287 to too the the 011287 011287 011287 011287 011287 011287\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: if how lot how him get is this 011287 they too in and 011287 011287 011287 011287 011287 011287\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: if how lot how him get is this while his too in and 011287 011287 011287 011287 011287 011287\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is your name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: if how people how him to is this 011287 to too the and 011287 011287 011287 011287 011287 011287\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    }
   ],
   "source": [
    "def predict_response(input_text):\n",
    "    input_text = preprocess(input_text)\n",
    "    input_pattern = encode(input_text, word_to_idx)\n",
    "    input_pattern = pad_sequence(input_pattern, max_len)\n",
    "    input_pattern = torch.tensor(input_pattern, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_pattern)\n",
    "\n",
    "    output = output.view(-1, output_size)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    predicted = predicted.numpy()\n",
    "\n",
    "    response_words = [all_words[idx] for idx in predicted if idx < len(all_words)]\n",
    "    response_text = ' '.join(response_words)\n",
    "    return response_text\n",
    "\n",
    "# Chat with the bot\n",
    "print(\"Start chatting with the bot (type 'quit' to stop)!\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    response = predict_response(user_input)\n",
    "    print(f\"Bot: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89dd390-c62b-43c5-a962-a7f8c513e510",
   "metadata": {},
   "source": [
    "## 2. Training Chatbot(100 epochs and GPU based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6ff085c-9905-4700-9354-61b7f66ae432",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 7.8544\n",
      "Epoch [2/100], Loss: 7.7927\n",
      "Epoch [3/100], Loss: 7.7309\n",
      "Epoch [4/100], Loss: 7.6668\n",
      "Epoch [5/100], Loss: 7.5995\n",
      "Epoch [6/100], Loss: 7.5284\n",
      "Epoch [7/100], Loss: 7.4530\n",
      "Epoch [8/100], Loss: 7.3731\n",
      "Epoch [9/100], Loss: 7.2882\n",
      "Epoch [10/100], Loss: 7.1981\n",
      "Epoch [11/100], Loss: 7.1024\n",
      "Epoch [12/100], Loss: 7.0008\n",
      "Epoch [13/100], Loss: 6.8930\n",
      "Epoch [14/100], Loss: 6.7785\n",
      "Epoch [15/100], Loss: 6.6570\n",
      "Epoch [16/100], Loss: 6.5281\n",
      "Epoch [17/100], Loss: 6.3915\n",
      "Epoch [18/100], Loss: 6.2468\n",
      "Epoch [19/100], Loss: 6.0936\n",
      "Epoch [20/100], Loss: 5.9317\n",
      "Epoch [21/100], Loss: 5.7607\n",
      "Epoch [22/100], Loss: 5.5804\n",
      "Epoch [23/100], Loss: 5.3907\n",
      "Epoch [24/100], Loss: 5.1914\n",
      "Epoch [25/100], Loss: 4.9824\n",
      "Epoch [26/100], Loss: 4.7639\n",
      "Epoch [27/100], Loss: 4.5362\n",
      "Epoch [28/100], Loss: 4.3001\n",
      "Epoch [29/100], Loss: 4.0568\n",
      "Epoch [30/100], Loss: 3.8088\n",
      "Epoch [31/100], Loss: 3.5600\n",
      "Epoch [32/100], Loss: 3.3163\n",
      "Epoch [33/100], Loss: 3.0854\n",
      "Epoch [34/100], Loss: 2.8760\n",
      "Epoch [35/100], Loss: 2.6951\n",
      "Epoch [36/100], Loss: 2.5464\n",
      "Epoch [37/100], Loss: 2.4299\n",
      "Epoch [38/100], Loss: 2.3425\n",
      "Epoch [39/100], Loss: 2.2799\n",
      "Epoch [40/100], Loss: 2.2371\n",
      "Epoch [41/100], Loss: 2.2094\n",
      "Epoch [42/100], Loss: 2.1924\n",
      "Epoch [43/100], Loss: 2.1825\n",
      "Epoch [44/100], Loss: 2.1764\n",
      "Epoch [45/100], Loss: 2.1717\n",
      "Epoch [46/100], Loss: 2.1668\n",
      "Epoch [47/100], Loss: 2.1609\n",
      "Epoch [48/100], Loss: 2.1537\n",
      "Epoch [49/100], Loss: 2.1453\n",
      "Epoch [50/100], Loss: 2.1361\n",
      "Epoch [51/100], Loss: 2.1263\n",
      "Epoch [52/100], Loss: 2.1163\n",
      "Epoch [53/100], Loss: 2.1064\n",
      "Epoch [54/100], Loss: 2.0968\n",
      "Epoch [55/100], Loss: 2.0876\n",
      "Epoch [56/100], Loss: 2.0788\n",
      "Epoch [57/100], Loss: 2.0706\n",
      "Epoch [58/100], Loss: 2.0628\n",
      "Epoch [59/100], Loss: 2.0555\n",
      "Epoch [60/100], Loss: 2.0487\n",
      "Epoch [61/100], Loss: 2.0424\n",
      "Epoch [62/100], Loss: 2.0365\n",
      "Epoch [63/100], Loss: 2.0309\n",
      "Epoch [64/100], Loss: 2.0257\n",
      "Epoch [65/100], Loss: 2.0208\n",
      "Epoch [66/100], Loss: 2.0161\n",
      "Epoch [67/100], Loss: 2.0117\n",
      "Epoch [68/100], Loss: 2.0074\n",
      "Epoch [69/100], Loss: 2.0033\n",
      "Epoch [70/100], Loss: 1.9992\n",
      "Epoch [71/100], Loss: 1.9953\n",
      "Epoch [72/100], Loss: 1.9915\n",
      "Epoch [73/100], Loss: 1.9879\n",
      "Epoch [74/100], Loss: 1.9843\n",
      "Epoch [75/100], Loss: 1.9808\n",
      "Epoch [76/100], Loss: 1.9775\n",
      "Epoch [77/100], Loss: 1.9742\n",
      "Epoch [78/100], Loss: 1.9709\n",
      "Epoch [79/100], Loss: 1.9677\n",
      "Epoch [80/100], Loss: 1.9645\n",
      "Epoch [81/100], Loss: 1.9613\n",
      "Epoch [82/100], Loss: 1.9582\n",
      "Epoch [83/100], Loss: 1.9551\n",
      "Epoch [84/100], Loss: 1.9520\n",
      "Epoch [85/100], Loss: 1.9489\n",
      "Epoch [86/100], Loss: 1.9460\n",
      "Epoch [87/100], Loss: 1.9430\n",
      "Epoch [88/100], Loss: 1.9401\n",
      "Epoch [89/100], Loss: 1.9371\n",
      "Epoch [90/100], Loss: 1.9342\n",
      "Epoch [91/100], Loss: 1.9314\n",
      "Epoch [92/100], Loss: 1.9285\n",
      "Epoch [93/100], Loss: 1.9256\n",
      "Epoch [94/100], Loss: 1.9228\n",
      "Epoch [95/100], Loss: 1.9199\n",
      "Epoch [96/100], Loss: 1.9170\n",
      "Epoch [97/100], Loss: 1.9142\n",
      "Epoch [98/100], Loss: 1.9113\n",
      "Epoch [99/100], Loss: 1.9085\n",
      "Epoch [100/100], Loss: 1.9056\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = [line.strip().split('\\t') for line in file.readlines()]\n",
    "    return data\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Vocabulary and encoding\n",
    "def build_vocab(data):\n",
    "    all_words = []\n",
    "    for (pattern, response) in data:\n",
    "        pattern = preprocess(pattern)\n",
    "        response = preprocess(response)\n",
    "        words = pattern.split() + response.split()\n",
    "        all_words.extend(words)\n",
    "    all_words = sorted(set(all_words))\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "    return word_to_idx, all_words\n",
    "\n",
    "def encode(text, word_to_idx):\n",
    "    text = preprocess(text)\n",
    "    return [word_to_idx[word] for word in text.split() if word in word_to_idx]\n",
    "\n",
    "def build_vocab_and_encode(data):\n",
    "    word_to_idx, all_words = build_vocab(data)\n",
    "    encoded_data = [(encode(pattern, word_to_idx), encode(response, word_to_idx)) for (pattern, response) in data]\n",
    "    return encoded_data, word_to_idx, all_words\n",
    "\n",
    "# Pad sequences\n",
    "def pad_sequence(seq, max_len, padding_value=0):\n",
    "    return seq + [padding_value] * (max_len - len(seq))\n",
    "\n",
    "# Load and process dataset\n",
    "file_path = 'dataset/chatbot-dialogs.txt'  # Path to your dataset file\n",
    "data = load_dataset(file_path)\n",
    "encoded_data, word_to_idx, all_words = build_vocab_and_encode(data)\n",
    "\n",
    "# Determine maximum length\n",
    "max_pattern_len = max(len(pattern) for pattern, response in encoded_data)\n",
    "max_response_len = max(len(response) for pattern, response in encoded_data)\n",
    "max_len = max(max_pattern_len, max_response_len)\n",
    "\n",
    "# Pad patterns and responses\n",
    "padded_patterns = [pad_sequence(pattern, max_len) for pattern, response in encoded_data]\n",
    "padded_responses = [pad_sequence(response, max_len) for pattern, response in encoded_data]\n",
    "\n",
    "# Convert to tensors\n",
    "patterns = torch.tensor(padded_patterns, dtype=torch.long)\n",
    "responses = torch.tensor(padded_responses, dtype=torch.long)\n",
    "\n",
    "# Check for GPU availability and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, max_len):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * max_len, output_size * max_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x.view(x.size(0), max_len, -1)\n",
    "\n",
    "# Initialize model\n",
    "vocab_size = len(all_words)\n",
    "embed_size = 10\n",
    "hidden_size = 20\n",
    "output_size = vocab_size\n",
    "model = ChatbotModel(vocab_size, embed_size, hidden_size, output_size, max_len).to(device)\n",
    "\n",
    "# Move tensors to device\n",
    "patterns = patterns.to(device)\n",
    "responses = responses.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100  # Increase the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(patterns)\n",
    "\n",
    "    outputs = outputs.view(-1, output_size)\n",
    "    responses = responses.view(-1)\n",
    "\n",
    "    if outputs.shape[0] != responses.shape[0]:\n",
    "        print(f\"WARNING: Mismatch in output and response sizes. Outputs: {outputs.shape}, Responses: {responses.shape}\")\n",
    "    else:\n",
    "        loss = criterion(outputs, responses)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26d4f7-2784-48e0-aeda-5d364b3cc1cc",
   "metadata": {},
   "source": [
    "# Prediction (100 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6870c59-3e4e-4af1-9ac4-7daa10ed0176",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the bot (type 'quit' to stop)!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi, how are you doing?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: i you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: i you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is your name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: i you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    }
   ],
   "source": [
    "def predict_response(input_text):\n",
    "    input_text = preprocess(input_text)\n",
    "    input_pattern = encode(input_text, word_to_idx)\n",
    "    input_pattern = pad_sequence(input_pattern, max_len)\n",
    "    input_pattern = torch.tensor(input_pattern, dtype=torch.long).unsqueeze(0).to(device)  # Move to device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_pattern)\n",
    "\n",
    "    output = output.view(-1, output_size)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    predicted = predicted.cpu().numpy()  # Move to CPU before converting to numpy\n",
    "\n",
    "    response_words = [all_words[idx] for idx in predicted if idx < len(all_words)]\n",
    "    response_text = ' '.join(response_words)\n",
    "    return response_text\n",
    "\n",
    "# Chat with the bot\n",
    "print(\"Start chatting with the bot (type 'quit' to stop)!\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    response = predict_response(user_input)\n",
    "    print(f\"Bot: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec6d01-6c19-401d-b571-f98ce74f9b7f",
   "metadata": {},
   "source": [
    "# 3. Training (400 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d5eb69e-25ae-46e1-84c8-2cffcb97dbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs:\n",
      "GPU 0: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Available GPUs:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb9f1e9b-0555-4bdb-a9c6-3476c8ff1817",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 7.8412\n",
      "Epoch [2/10], Loss: 7.6640\n",
      "Epoch [3/10], Loss: 7.4188\n",
      "Epoch [4/10], Loss: 7.0501\n",
      "Epoch [5/10], Loss: 6.5117\n",
      "Epoch [6/10], Loss: 5.7540\n",
      "Epoch [7/10], Loss: 4.7650\n",
      "Epoch [8/10], Loss: 3.6457\n",
      "Epoch [9/10], Loss: 2.7997\n",
      "Epoch [10/10], Loss: 2.4560\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = [line.strip().split('\\t') for line in file.readlines()]\n",
    "    return data\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Vocabulary and encoding\n",
    "def build_vocab(data):\n",
    "    all_words = []\n",
    "    for (pattern, response) in data:\n",
    "        pattern = preprocess(pattern)\n",
    "        response = preprocess(response)\n",
    "        words = pattern.split() + response.split()\n",
    "        all_words.extend(words)\n",
    "    all_words = sorted(set(all_words))\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "    return word_to_idx, all_words\n",
    "\n",
    "def encode(text, word_to_idx):\n",
    "    text = preprocess(text)\n",
    "    return [word_to_idx[word] for word in text.split() if word in word_to_idx]\n",
    "\n",
    "def build_vocab_and_encode(data):\n",
    "    word_to_idx, all_words = build_vocab(data)\n",
    "    encoded_data = [(encode(pattern, word_to_idx), encode(response, word_to_idx)) for (pattern, response) in data]\n",
    "    return encoded_data, word_to_idx, all_words\n",
    "\n",
    "# Pad sequences\n",
    "def pad_sequence(seq, max_len, padding_value=0):\n",
    "    return seq + [padding_value] * (max_len - len(seq))\n",
    "\n",
    "# Load and process dataset\n",
    "file_path = 'dataset/chatbot-dialogs.txt'  # Path to your dataset file\n",
    "data = load_dataset(file_path)\n",
    "encoded_data, word_to_idx, all_words = build_vocab_and_encode(data)\n",
    "\n",
    "# Determine maximum length\n",
    "max_pattern_len = max(len(pattern) for pattern, response in encoded_data)\n",
    "max_response_len = max(len(response) for pattern, response in encoded_data)\n",
    "max_len = max(max_pattern_len, max_response_len)\n",
    "\n",
    "# Pad patterns and responses\n",
    "padded_patterns = [pad_sequence(pattern, max_len) for pattern, response in encoded_data]\n",
    "padded_responses = [pad_sequence(response, max_len) for pattern, response in encoded_data]\n",
    "\n",
    "# Convert to tensors\n",
    "patterns = torch.tensor(padded_patterns, dtype=torch.long)\n",
    "responses = torch.tensor(padded_responses, dtype=torch.long)\n",
    "\n",
    "# Check for GPU availability and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, output_size, max_len):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_size * max_len, output_size * max_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x.view(x.size(0), max_len, -1)\n",
    "\n",
    "# Initialize model\n",
    "vocab_size = len(all_words)\n",
    "embed_size = 50\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = vocab_size\n",
    "model = ChatbotModel(vocab_size, embed_size, hidden_size, num_layers, output_size, max_len).to(device)\n",
    "\n",
    "# Move tensors to device\n",
    "patterns = patterns.to(device)\n",
    "responses = responses.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # Increase the number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(patterns)\n",
    "\n",
    "    outputs = outputs.view(-1, output_size)\n",
    "    responses = responses.view(-1)\n",
    "\n",
    "    if outputs.shape[0] != responses.shape[0]:\n",
    "        print(f\"WARNING: Mismatch in output and response sizes. Outputs: {outputs.shape}, Responses: {responses.shape}\")\n",
    "    else:\n",
    "        loss = criterion(outputs, responses)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca0ac3-974e-43b5-810f-bec0261277b6",
   "metadata": {},
   "source": [
    "# 3. Prediction (400 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7700afae-3903-457f-8ffc-a0425bd605ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the bot (type 'quit' to stop)!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    }
   ],
   "source": [
    "def predict_response(input_text):\n",
    "    input_text = preprocess(input_text)\n",
    "    input_pattern = encode(input_text, word_to_idx)\n",
    "    input_pattern = pad_sequence(input_pattern, max_len)\n",
    "    input_pattern = torch.tensor(input_pattern, dtype=torch.long).unsqueeze(0).to(device)  # Move to device\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_pattern)\n",
    "\n",
    "    output = output.view(-1, output_size)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    predicted = predicted.cpu().numpy()  # Move to CPU before converting to numpy\n",
    "\n",
    "    response_words = [all_words[idx] for idx in predicted if idx < len(all_words)]\n",
    "    response_text = ' '.join(response_words)\n",
    "    return response_text\n",
    "\n",
    "# Chat with the bot\n",
    "print(\"Start chatting with the bot (type 'quit' to stop)!\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    response = predict_response(user_input)\n",
    "    print(f\"Bot: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65758d7-06ce-4c6c-a09d-572bfda8e3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ab1e9-40e5-4519-9b6a-7993d2ef54c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba2e302f-6ff9-4c24-a6fa-7196ad23eb72",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 1.0598\n"
     ]
    }
   ],
   "source": [
    "# Data Loader used---------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load dataset\n",
    "def load_dataset(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if '\\t' in line:\n",
    "                pattern, response = line.strip().split('\\t')\n",
    "                data.append((pattern, response))\n",
    "    return data\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Vocabulary and encoding\n",
    "def build_vocab(data):\n",
    "    all_words = []\n",
    "    for (pattern, response) in data:\n",
    "        pattern = preprocess(pattern)\n",
    "        response = preprocess(response)\n",
    "        words = pattern.split() + response.split()\n",
    "        all_words.extend(words)\n",
    "    all_words = sorted(set(all_words))\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "    return word_to_idx, all_words\n",
    "\n",
    "def encode(text, word_to_idx):\n",
    "    text = preprocess(text)\n",
    "    return [word_to_idx[word] for word in text.split() if word in word_to_idx]\n",
    "\n",
    "def build_vocab_and_encode(data):\n",
    "    word_to_idx, all_words = build_vocab(data)\n",
    "    encoded_data = [(encode(pattern, word_to_idx), encode(response, word_to_idx)) for (pattern, response) in data]\n",
    "    return encoded_data, word_to_idx, all_words\n",
    "\n",
    "# Pad sequences\n",
    "def pad_sequence(seq, max_len, padding_value=0):\n",
    "    return seq + [padding_value] * (max_len - len(seq))\n",
    "\n",
    "# Load and process dataset\n",
    "file_path = 'dataset/chatbot-dialogs.txt'\n",
    "data = load_dataset(file_path)\n",
    "encoded_data, word_to_idx, all_words = build_vocab_and_encode(data)\n",
    "\n",
    "# Determine maximum length\n",
    "max_pattern_len = max(len(pattern) for pattern, response in encoded_data)\n",
    "max_response_len = max(len(response) for pattern, response in encoded_data)\n",
    "max_len = max(max_pattern_len, max_response_len)\n",
    "\n",
    "# Pad patterns and responses\n",
    "padded_patterns = [pad_sequence(pattern, max_len) for pattern, response in encoded_data]\n",
    "padded_responses = [pad_sequence(response, max_len) for pattern, response in encoded_data]\n",
    "\n",
    "# Convert to tensors\n",
    "patterns = torch.tensor(padded_patterns, dtype=torch.long)\n",
    "responses = torch.tensor(padded_responses, dtype=torch.long)\n",
    "\n",
    "# Check for GPU availability and set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move tensors to device\n",
    "patterns = patterns.to(device)\n",
    "responses = responses.to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(patterns, responses)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, output_size, max_len):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_size * max_len, output_size * max_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x.view(x.size(0), max_len, -1)\n",
    "\n",
    "# Initialize model\n",
    "vocab_size = len(all_words)\n",
    "embed_size = 50\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = vocab_size\n",
    "model = ChatbotModel(vocab_size, embed_size, hidden_size, num_layers, output_size, max_len).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_patterns, batch_responses in dataloader:\n",
    "        batch_patterns = batch_patterns.to(device)\n",
    "        batch_responses = batch_responses.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_patterns)\n",
    "        outputs = outputs.view(-1, output_size)\n",
    "        batch_responses = batch_responses.view(-1)\n",
    "\n",
    "        loss = criterion(outputs, batch_responses)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(dataloader):.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b57a9e4-e6ed-4b64-9a3d-ab0463352aec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the bot (type 'quit' to stop)!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is your name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi, how are you doing?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    }
   ],
   "source": [
    "def predict_response(input_text):\n",
    "    input_text = preprocess(input_text)\n",
    "    input_pattern = encode(input_text, word_to_idx)\n",
    "    input_pattern = pad_sequence(input_pattern, max_len)\n",
    "    input_pattern = torch.tensor(input_pattern, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_pattern)\n",
    "\n",
    "    output = output.view(-1, output_size)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    predicted = predicted.cpu().numpy()\n",
    "\n",
    "    response_words = [all_words[idx] for idx in predicted if idx < len(all_words)]\n",
    "    response_text = ' '.join(response_words)\n",
    "    return response_text\n",
    "\n",
    "# Chat with the bot\n",
    "print(\"Start chatting with the bot (type 'quit' to stop)!\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    response = predict_response(user_input)\n",
    "    print(f\"Bot: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0097f74c-64f8-4b45-b2a3-7808d6c41299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4489e7b2-08e3-410f-a5a4-a91435e4998e",
   "metadata": {},
   "source": [
    "# CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f714b17-f8e3-4ed6-b058-e900dd325f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_csv(txt_file, csv_file):\n",
    "  with open(txt_file, 'r', encoding='utf-8') as txt_file, open(csv_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    fieldnames = ['question', 'response']\n",
    "    csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "    for line in txt_file:\n",
    "      columns = line.strip().split('\\t')\n",
    "      csv_writer.writerow({'question': columns[0], 'response': columns[1]})\n",
    "\n",
    "# Example usage:\n",
    "txt_file_path = 'dataset/chatbot-dialogs.txt'\n",
    "csv_file_path = 'dataset/chatbot-dialogs1.csv'\n",
    "txt_to_csv(txt_file_path, csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3e00ae5-5b02-405c-b337-4bb8ccbc493d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data: [('hi, how are you doing?', \"i'm fine. how about yourself?\"), (\"i'm fine. how about yourself?\", \"i'm pretty good. thanks for asking.\"), (\"i'm pretty good. thanks for asking.\", 'no problem. so how have you been?'), ('no problem. so how have you been?', \"i've been great. what about you?\"), (\"i've been great. what about you?\", \"i've been good. i'm in school right now.\")]\n",
      "Encoded data: [([1014, 1063, 149, 2508, 644], [1089, 809, 1063, 65, 2515]), ([1089, 809, 1063, 65, 2515], [1089, 1671, 928, 2198, 852, 168]), ([1089, 1671, 928, 2198, 852, 168], [1443, 1678, 1999, 1063, 985, 2508, 235]), ([1443, 1678, 1999, 1063, 985, 2508, 235], [1139, 235, 945, 2424, 65, 2508]), ([1139, 235, 945, 2424, 65, 2508], [1139, 235, 928, 1089, 1095, 1854, 1788, 1462])]\n",
      "Epoch [1/10], Loss: 7.8319\n",
      "Epoch [2/10], Loss: 7.6702\n",
      "Epoch [3/10], Loss: 7.4357\n",
      "Epoch [4/10], Loss: 7.0745\n",
      "Epoch [5/10], Loss: 6.5361\n",
      "Epoch [6/10], Loss: 5.7611\n",
      "Epoch [7/10], Loss: 4.7232\n",
      "Epoch [8/10], Loss: 3.5437\n",
      "Epoch [9/10], Loss: 2.7280\n",
      "Epoch [10/10], Loss: 2.4484\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load dataset from CSV\n",
    "def load_dataset(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        pattern = str(row['question'])\n",
    "        response = str(row['response'])\n",
    "        data.append((pattern, response))\n",
    "    return data\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Vocabulary and encoding\n",
    "def build_vocab(data):\n",
    "    all_words = []\n",
    "    for (pattern, response) in data:\n",
    "        pattern = preprocess(pattern)\n",
    "        response = preprocess(response)\n",
    "        words = pattern.split() + response.split()\n",
    "        all_words.extend(words)\n",
    "    all_words = sorted(set(all_words))\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "    return word_to_idx, all_words\n",
    "\n",
    "def encode(text, word_to_idx):\n",
    "    text = preprocess(text)\n",
    "    return [word_to_idx[word] for word in text.split() if word in word_to_idx]\n",
    "\n",
    "def build_vocab_and_encode(data):\n",
    "    word_to_idx, all_words = build_vocab(data)\n",
    "    encoded_data = [(encode(pattern, word_to_idx), encode(response, word_to_idx)) for (pattern, response) in data]\n",
    "    return encoded_data, word_to_idx, all_words\n",
    "\n",
    "# Pad sequences\n",
    "def pad_sequence(seq, max_len, padding_value=0):\n",
    "    return seq + [padding_value] * (max_len - len(seq))\n",
    "\n",
    "# Load and process dataset\n",
    "file_path = 'dataset/chatbot-dialogs1.csv'  # Update to your dataset file path\n",
    "data = load_dataset(file_path)\n",
    "\n",
    "# Debugging: Print loaded data\n",
    "print(f\"Loaded data: {data[:5]}\")  # Print first 5 entries for inspection\n",
    "\n",
    "encoded_data, word_to_idx, all_words = build_vocab_and_encode(data)\n",
    "\n",
    "# Debugging: Print encoded data\n",
    "print(f\"Encoded data: {encoded_data[:5]}\")  # Print first 5 entries for inspection\n",
    "\n",
    "\n",
    "# Determine maximum length\n",
    "max_pattern_len = max(len(pattern) for pattern, response in encoded_data)\n",
    "max_response_len = max(len(response) for pattern, response in encoded_data)\n",
    "max_len = max(max_pattern_len, max_response_len)\n",
    "\n",
    "# Pad patterns and responses\n",
    "padded_patterns = [pad_sequence(pattern, max_len) for pattern, response in encoded_data]\n",
    "padded_responses = [pad_sequence(response, max_len) for pattern, response in encoded_data]\n",
    "\n",
    "# Convert to tensors\n",
    "patterns = torch.tensor(padded_patterns, dtype=torch.long)\n",
    "responses = torch.tensor(padded_responses, dtype=torch.long)\n",
    "\n",
    "# Check for GPU availability and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU 1 (NVIDIA RTX)\n",
    "\n",
    "# Move tensors to device\n",
    "patterns = patterns.to(device)\n",
    "responses = responses.to(device)\n",
    "\n",
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, output_size, max_len):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_size * max_len, output_size * max_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x.view(x.size(0), max_len, -1)\n",
    "\n",
    "# Initialize model\n",
    "vocab_size = len(all_words)\n",
    "embed_size = 50\n",
    "hidden_size = 128\n",
    "num_layers = 2quit\n",
    "output_size = vocab_size\n",
    "model = ChatbotModel(vocab_size, embed_size, hidden_size, num_layers, output_size, max_len).to(device)  # Move model to GPU\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(patterns)\n",
    "\n",
    "    outputs = outputs.view(-1, output_size)\n",
    "    responses = responses.view(-1)\n",
    "\n",
    "    if outputs.shape[0] != responses.shape[0]:\n",
    "        print(f\"WARNING: Mismatch in output and response sizes. Outputs: {outputs.shape}, Responses: {responses.shape}\")\n",
    "    else:\n",
    "        loss = criterion(outputs, responses)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f33d462b-bd32-4f27-889e-076c78f82c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the bot (type 'quit' to stop)!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi, how are you doing?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: i i you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is your name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: i i you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  who are you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: i i you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what you doing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: i i you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  where are you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: i i you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: i you you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  quit\n"
     ]
    }
   ],
   "source": [
    "def predict_response(input_text):\n",
    "    input_text = preprocess(input_text)\n",
    "    input_pattern = encode(input_text, word_to_idx)\n",
    "    input_pattern = pad_sequence(input_pattern, max_len)\n",
    "    input_pattern = torch.tensor(input_pattern, dtype=torch.long).unsqueeze(0).to(device)  # Move to GPU\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_pattern)\n",
    "\n",
    "    output = output.view(-1, output_size)\n",
    "    _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "    predicted = predicted.cpu().numpy()  # Move to CPU before converting to numpy\n",
    "\n",
    "    response_words = [all_words[idx] for idx in predicted if idx < len(all_words)]\n",
    "    response_text = ' '.join(response_words)\n",
    "    return response_text\n",
    "\n",
    "# Chat with the bot\n",
    "print(\"Start chatting with the bot (type 'quit' to stop)!\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    response = predict_response(user_input)\n",
    "    print(f\"Bot: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4439407-3210-45b1-8dc4-038e448238d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
