{"cells":[{"cell_type":"code","execution_count":null,"id":"7dedb960-ef39-42ac-b9a0-1e9f3e4c1ee2","metadata":{"id":"7dedb960-ef39-42ac-b9a0-1e9f3e4c1ee2"},"outputs":[],"source":["# pip install torch"]},{"cell_type":"code","execution_count":1,"id":"1a7ab7d9-a73b-4f65-9897-994beebd7043","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1a7ab7d9-a73b-4f65-9897-994beebd7043","jupyter":{"source_hidden":true},"outputId":"957022f8-22fa-4f4f-8add-c917ba1cbfd5","executionInfo":{"status":"ok","timestamp":1723189855321,"user_tz":-345,"elapsed":4564,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Using Device: cuda\n","Current device number is: 0\n","GPU name is: Tesla T4\n"]}],"source":["import torch\n","\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(f\"Using Device: {device}\")\n","\n","    device_no = torch.cuda.current_device()\n","    print(f\"Current device number is: {device_no}\")\n","\n","    device_name = torch.cuda.get_device_name(device_no)\n","    print(f\"GPU name is: {device_name}\")\n","else:\n","    print(\"CUDA is not available\")"]},{"cell_type":"code","execution_count":2,"id":"8aef3830-fd37-47a9-b452-e42a03d45fab","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8aef3830-fd37-47a9-b452-e42a03d45fab","jupyter":{"source_hidden":true},"outputId":"4b6aba96-ff30-4eb0-d5a6-ab26464e0bd7","executionInfo":{"status":"ok","timestamp":1723189876860,"user_tz":-345,"elapsed":506,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Max Pattern Length: 4\n","Max Response Length: 5\n","Padded Pattern: [[7, 0, 0, 0, 0], [9, 2, 14, 0, 0], [13, 11, 15, 12, 0], [3, 0, 0, 0, 0]]\n","Padded Response: [[8, 0, 0, 0, 0], [10, 5, 9, 1, 14], [10, 0, 4, 0, 0], [6, 0, 0, 0, 0]]\n","Example padded pattern: [7, 0, 0, 0, 0]\n","Example padded response: [8, 0, 0, 0, 0]\n"]}],"source":["import torch #an open source ML library used for creating deep neural networks\n","import torch.nn as nn # A module in PyTorch that provides classes and functions to build neural networks\n","import torch.optim as optim # A module in PyTorch that provides various optimization algorithms for training neural networks\n","import random # A module that implements pseudo-random number generators for various distributions\n","import re # A module for working with regular expressions to match and manipulate strings\n","import numpy as np\n","\n","# Sample dataset\n","data = [\n","    (\"hello\", \"hi\"),\n","    (\"how are you\", \"I'm good, how about you?\"),\n","    (\"what is your name\", \"I'm a chatbot\"),\n","    (\"bye\", \"goodbye\"),\n","]\n","\n","# Preprocessing\n","def preprocess(text):\n","    text = text.lower()\n","    text = re.sub(r'[^\\w\\s]', '', text) # removes all characters from the input text that are not word characters or whitespace\n","    return text\n","\n","# Vocabulary\n","all_words = []\n","for (pattern, response) in data:\n","    pattern = preprocess(pattern)\n","    response = preprocess(response)\n","    words = pattern.split() + response.split() #splits it into a list of words, using whitespace as the delimiter: \"hello Ameer\" to [\"hello\",\"Aeer\"]\n","    all_words.extend(words) #appends the elements of the list words to the end of the list all_words\n","all_words = sorted(set(all_words))\n","\n","\n","# Word to index mapping\n","word_to_idx = {word: idx for idx, word in enumerate(all_words)} #enumerate iterates on pairs of (index, value) tuples.\n","\n","# Encode patterns and responses\n","def encode(text):\n","    text = preprocess(text)\n","    return [word_to_idx[word] for word in text.split() if word in word_to_idx]\n","\n","encoded_data = [(encode(pattern), encode(response)) for (pattern, response) in data]\n","\n","# Pad sequences: ad_sequence function is used to ensure that all sequences in a dataset have the same length\n","def pad_sequence(seq, max_len, padding_value=0): #seq is input sequence that needs to be padded, max_len is desired length for all sequences, padding_val is alue used to fill the sequence to reach the maximum length\n","    return seq + [padding_value] * (max_len - len(seq)) #If the input sequence is shorter than the max_len, it appends padding_value to the end of the sequence until it reaches the desired length.\n","\n","# Determine the maximum length of patterns and responses\n","max_pattern_len = max(len(pattern) for pattern, response in encoded_data)\n","max_response_len = max(len(response) for pattern, response in encoded_data)\n","max_len = max(max_pattern_len, max_response_len)\n","\n","# Pad patterns and responses\n","padded_patterns = [pad_sequence(pattern, max_len) for pattern, response in encoded_data]\n","padded_responses = [pad_sequence(response, max_len) for pattern, response in encoded_data]\n","\n","print(f\"Max Pattern Length: {max_pattern_len}\")\n","print(f\"Max Response Length: {max_response_len}\")\n","# print(f\"Max Length: {max_len}\")\n","print(f\"Padded Pattern: {padded_patterns}\")\n","print(f\"Padded Response: {padded_responses}\")\n","\n","# Additional debugging:\n","print(\"Example padded pattern:\", padded_patterns[0])\n","print(\"Example padded response:\", padded_responses[0])"]},{"cell_type":"code","execution_count":3,"id":"18c7042a-c4c5-46f2-a3bb-08590ad2af11","metadata":{"id":"18c7042a-c4c5-46f2-a3bb-08590ad2af11","jupyter":{"source_hidden":true},"executionInfo":{"status":"ok","timestamp":1723189881672,"user_tz":-345,"elapsed":492,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"}}},"outputs":[],"source":["# Convert to tensors: preparing it for training for efficient computations, leverages GPU acceleration, and integrates seamlessly with the PyTorch ecosystem.\n","patterns = torch.tensor(padded_patterns, dtype=torch.long)\n","responses = torch.tensor(padded_responses, dtype=torch.long)\n","\n","# patterns = torch.tensor([pattern for pattern in padded_patterns], dtype=torch.long)\n","# responses = torch.tensor([response for response in padded_responses], dtype=torch.long)\n"]},{"cell_type":"code","execution_count":null,"id":"ec69b53d-12fe-433d-82c1-0212eb73fc0c","metadata":{"id":"ec69b53d-12fe-433d-82c1-0212eb73fc0c","jupyter":{"source_hidden":true}},"outputs":[],"source":["patterns # gives 2D tensor intergers with 4 rows and 4 columns, making it a 4x4 matrix.\n","# responses"]},{"cell_type":"code","execution_count":4,"id":"d5f6c4b3-c4da-41bd-90dd-5ffa322960d7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d5f6c4b3-c4da-41bd-90dd-5ffa322960d7","jupyter":{"source_hidden":true},"outputId":"91b7d789-6ac6-45c3-c7ca-f8fd575c36c4","executionInfo":{"status":"ok","timestamp":1723189884259,"user_tz":-345,"elapsed":494,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ChatbotModel(\n","  (embedding): Embedding(16, 10)\n","  (lstm): LSTM(10, 20, batch_first=True)\n","  (fc): Linear(in_features=100, out_features=80, bias=True)\n",")\n","Vocabulary Size: 16\n","Output Size: 16\n","Word to Index Mapping: {'a': 0, 'about': 1, 'are': 2, 'bye': 3, 'chatbot': 4, 'good': 5, 'goodbye': 6, 'hello': 7, 'hi': 8, 'how': 9, 'im': 10, 'is': 11, 'name': 12, 'what': 13, 'you': 14, 'your': 15}\n","Patterns Tensor: tensor([[ 7,  0,  0,  0,  0],\n","        [ 9,  2, 14,  0,  0],\n","        [13, 11, 15, 12,  0],\n","        [ 3,  0,  0,  0,  0]])\n","Max Length: 5\n"]}],"source":["class ChatbotModel(nn.Module): #defines a basic chatbot model architecture using PyTorch\n","    def __init__(self, vocab_size, embed_size, hidden_size, output_size,max_len): #Initializes the model with hyperparameters\n","        super(ChatbotModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size) #An embedding layer to convert word indices to dense vectors. word indices are numerical representations of words in a vocabulary. vocabulary:[\"AMeer\", \"Rai\"], and word indices: \"Ameer\":0, \"rai\":1\n","        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n","        self.fc = nn.Linear(hidden_size * max_len, output_size * max_len)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x, _ = self.lstm(x)\n","        x = x.contiguous().view(x.size(0), -1)  # Flatten the output for the linear layer\n","        x = self.fc(x)\n","        return x.view(x.size(0), max_len, -1)  # Reshape to (batch_size, max_len, output_size)\n","\n","# Initialize model\n","vocab_size = len(all_words)\n","embed_size = 10 #Dimensionality of word embeddings.\n","hidden_size = 20 #number of neurons in the hidden laye\n","# output_size = max_response_len #Size of the output layer (likely the maximum length of a response\n","output_size = vocab_size  # Output size should match the vocabulary size\n","\n","\n","max_len = max(max_pattern_len, max_response_len)\n","model = ChatbotModel(vocab_size, embed_size, hidden_size, output_size,max_len)  #simple feed-forward neural network for generating chatbot responses\n","print(model)\n","\n","print(f\"Vocabulary Size: {vocab_size}\")\n","print(f\"Output Size: {output_size}\")\n","print(f\"Word to Index Mapping: {word_to_idx}\")\n","print(f\"Patterns Tensor: {patterns}\")\n","print(f\"Max Length: {max_len}\")\n","\n","for pattern in patterns:\n","    for idx in pattern:\n","        if idx >= vocab_size:\n","            print(f\"Invalid Index: {idx}\")\n"]},{"cell_type":"code","execution_count":5,"id":"ad6228d7-94c0-47cc-9bc8-ba9921a1a096","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ad6228d7-94c0-47cc-9bc8-ba9921a1a096","jupyter":{"source_hidden":true},"outputId":"00fe4830-705a-4dc1-f98a-6cba716d8ece","executionInfo":{"status":"ok","timestamp":1723189897890,"user_tz":-345,"elapsed":2783,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Example padded pattern: [7, 0, 0, 0, 0]\n","Example padded response: [8, 0, 0, 0, 0]\n","Epoch 1: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [1/200], Loss: 2.7766\n","Epoch 2: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [2/200], Loss: 2.7588\n","Epoch 3: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [3/200], Loss: 2.7413\n","Epoch 4: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [4/200], Loss: 2.7238\n","Epoch 5: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [5/200], Loss: 2.7060\n","Epoch 6: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [6/200], Loss: 2.6878\n","Epoch 7: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [7/200], Loss: 2.6691\n","Epoch 8: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [8/200], Loss: 2.6497\n","Epoch 9: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [9/200], Loss: 2.6294\n","Epoch 10: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [10/200], Loss: 2.6082\n","Epoch 11: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [11/200], Loss: 2.5859\n","Epoch 12: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [12/200], Loss: 2.5625\n","Epoch 13: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [13/200], Loss: 2.5377\n","Epoch 14: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [14/200], Loss: 2.5116\n","Epoch 15: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [15/200], Loss: 2.4841\n","Epoch 16: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [16/200], Loss: 2.4549\n","Epoch 17: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [17/200], Loss: 2.4242\n","Epoch 18: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [18/200], Loss: 2.3918\n","Epoch 19: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [19/200], Loss: 2.3576\n","Epoch 20: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [20/200], Loss: 2.3217\n","Epoch 21: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [21/200], Loss: 2.2838\n","Epoch 22: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [22/200], Loss: 2.2441\n","Epoch 23: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [23/200], Loss: 2.2026\n","Epoch 24: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [24/200], Loss: 2.1591\n","Epoch 25: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [25/200], Loss: 2.1137\n","Epoch 26: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [26/200], Loss: 2.0665\n","Epoch 27: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [27/200], Loss: 2.0176\n","Epoch 28: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [28/200], Loss: 1.9670\n","Epoch 29: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [29/200], Loss: 1.9149\n","Epoch 30: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [30/200], Loss: 1.8614\n","Epoch 31: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [31/200], Loss: 1.8068\n","Epoch 32: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [32/200], Loss: 1.7513\n","Epoch 33: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [33/200], Loss: 1.6950\n","Epoch 34: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [34/200], Loss: 1.6384\n","Epoch 35: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [35/200], Loss: 1.5816\n","Epoch 36: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [36/200], Loss: 1.5250\n","Epoch 37: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [37/200], Loss: 1.4688\n","Epoch 38: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [38/200], Loss: 1.4134\n","Epoch 39: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [39/200], Loss: 1.3590\n","Epoch 40: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [40/200], Loss: 1.3059\n","Epoch 41: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [41/200], Loss: 1.2543\n","Epoch 42: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [42/200], Loss: 1.2044\n","Epoch 43: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [43/200], Loss: 1.1564\n","Epoch 44: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [44/200], Loss: 1.1103\n","Epoch 45: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [45/200], Loss: 1.0663\n","Epoch 46: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [46/200], Loss: 1.0243\n","Epoch 47: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [47/200], Loss: 0.9845\n","Epoch 48: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [48/200], Loss: 0.9467\n","Epoch 49: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [49/200], Loss: 0.9110\n","Epoch 50: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [50/200], Loss: 0.8772\n","Epoch 51: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [51/200], Loss: 0.8454\n","Epoch 52: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [52/200], Loss: 0.8154\n","Epoch 53: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [53/200], Loss: 0.7872\n","Epoch 54: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [54/200], Loss: 0.7606\n","Epoch 55: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [55/200], Loss: 0.7355\n","Epoch 56: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [56/200], Loss: 0.7119\n","Epoch 57: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [57/200], Loss: 0.6897\n","Epoch 58: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [58/200], Loss: 0.6687\n","Epoch 59: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [59/200], Loss: 0.6489\n","Epoch 60: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [60/200], Loss: 0.6303\n","Epoch 61: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [61/200], Loss: 0.6126\n","Epoch 62: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [62/200], Loss: 0.5958\n","Epoch 63: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [63/200], Loss: 0.5799\n","Epoch 64: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [64/200], Loss: 0.5648\n","Epoch 65: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [65/200], Loss: 0.5504\n","Epoch 66: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [66/200], Loss: 0.5366\n","Epoch 67: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [67/200], Loss: 0.5235\n","Epoch 68: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [68/200], Loss: 0.5109\n","Epoch 69: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [69/200], Loss: 0.4988\n","Epoch 70: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [70/200], Loss: 0.4872\n","Epoch 71: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [71/200], Loss: 0.4761\n","Epoch 72: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [72/200], Loss: 0.4653\n","Epoch 73: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [73/200], Loss: 0.4549\n","Epoch 74: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [74/200], Loss: 0.4449\n","Epoch 75: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [75/200], Loss: 0.4352\n","Epoch 76: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [76/200], Loss: 0.4258\n","Epoch 77: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [77/200], Loss: 0.4166\n","Epoch 78: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [78/200], Loss: 0.4077\n","Epoch 79: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [79/200], Loss: 0.3990\n","Epoch 80: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [80/200], Loss: 0.3906\n","Epoch 81: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [81/200], Loss: 0.3823\n","Epoch 82: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [82/200], Loss: 0.3741\n","Epoch 83: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [83/200], Loss: 0.3662\n","Epoch 84: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [84/200], Loss: 0.3584\n","Epoch 85: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [85/200], Loss: 0.3507\n","Epoch 86: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [86/200], Loss: 0.3432\n","Epoch 87: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [87/200], Loss: 0.3358\n","Epoch 88: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [88/200], Loss: 0.3285\n","Epoch 89: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [89/200], Loss: 0.3213\n","Epoch 90: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [90/200], Loss: 0.3143\n","Epoch 91: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [91/200], Loss: 0.3074\n","Epoch 92: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [92/200], Loss: 0.3005\n","Epoch 93: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [93/200], Loss: 0.2938\n","Epoch 94: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [94/200], Loss: 0.2872\n","Epoch 95: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [95/200], Loss: 0.2807\n","Epoch 96: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [96/200], Loss: 0.2742\n","Epoch 97: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [97/200], Loss: 0.2679\n","Epoch 98: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [98/200], Loss: 0.2616\n","Epoch 99: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [99/200], Loss: 0.2554\n","Epoch 100: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [100/200], Loss: 0.2494\n","Epoch 101: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [101/200], Loss: 0.2434\n","Epoch 102: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [102/200], Loss: 0.2375\n","Epoch 103: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [103/200], Loss: 0.2317\n","Epoch 104: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [104/200], Loss: 0.2260\n","Epoch 105: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [105/200], Loss: 0.2204\n","Epoch 106: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [106/200], Loss: 0.2149\n","Epoch 107: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [107/200], Loss: 0.2096\n","Epoch 108: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [108/200], Loss: 0.2043\n","Epoch 109: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [109/200], Loss: 0.1992\n","Epoch 110: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [110/200], Loss: 0.1942\n","Epoch 111: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [111/200], Loss: 0.1894\n","Epoch 112: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [112/200], Loss: 0.1847\n","Epoch 113: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [113/200], Loss: 0.1801\n","Epoch 114: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [114/200], Loss: 0.1756\n","Epoch 115: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [115/200], Loss: 0.1713\n","Epoch 116: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [116/200], Loss: 0.1671\n","Epoch 117: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [117/200], Loss: 0.1630\n","Epoch 118: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [118/200], Loss: 0.1590\n","Epoch 119: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [119/200], Loss: 0.1552\n","Epoch 120: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [120/200], Loss: 0.1514\n","Epoch 121: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [121/200], Loss: 0.1478\n","Epoch 122: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [122/200], Loss: 0.1443\n","Epoch 123: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [123/200], Loss: 0.1409\n","Epoch 124: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [124/200], Loss: 0.1376\n","Epoch 125: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [125/200], Loss: 0.1344\n","Epoch 126: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [126/200], Loss: 0.1314\n","Epoch 127: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [127/200], Loss: 0.1284\n","Epoch 128: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [128/200], Loss: 0.1255\n","Epoch 129: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [129/200], Loss: 0.1227\n","Epoch 130: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [130/200], Loss: 0.1199\n","Epoch 131: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [131/200], Loss: 0.1173\n","Epoch 132: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [132/200], Loss: 0.1147\n","Epoch 133: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [133/200], Loss: 0.1122\n","Epoch 134: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [134/200], Loss: 0.1098\n","Epoch 135: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [135/200], Loss: 0.1075\n","Epoch 136: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [136/200], Loss: 0.1053\n","Epoch 137: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [137/200], Loss: 0.1031\n","Epoch 138: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [138/200], Loss: 0.1009\n","Epoch 139: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [139/200], Loss: 0.0989\n","Epoch 140: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [140/200], Loss: 0.0969\n","Epoch 141: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [141/200], Loss: 0.0949\n","Epoch 142: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [142/200], Loss: 0.0930\n","Epoch 143: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [143/200], Loss: 0.0912\n","Epoch 144: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [144/200], Loss: 0.0894\n","Epoch 145: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [145/200], Loss: 0.0877\n","Epoch 146: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [146/200], Loss: 0.0860\n","Epoch 147: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [147/200], Loss: 0.0844\n","Epoch 148: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [148/200], Loss: 0.0828\n","Epoch 149: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [149/200], Loss: 0.0812\n","Epoch 150: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [150/200], Loss: 0.0797\n","Epoch 151: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [151/200], Loss: 0.0783\n","Epoch 152: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [152/200], Loss: 0.0768\n","Epoch 153: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [153/200], Loss: 0.0754\n","Epoch 154: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [154/200], Loss: 0.0741\n","Epoch 155: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [155/200], Loss: 0.0728\n","Epoch 156: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [156/200], Loss: 0.0715\n","Epoch 157: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [157/200], Loss: 0.0703\n","Epoch 158: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [158/200], Loss: 0.0690\n","Epoch 159: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [159/200], Loss: 0.0679\n","Epoch 160: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [160/200], Loss: 0.0667\n","Epoch 161: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [161/200], Loss: 0.0656\n","Epoch 162: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [162/200], Loss: 0.0645\n","Epoch 163: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [163/200], Loss: 0.0634\n","Epoch 164: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [164/200], Loss: 0.0623\n","Epoch 165: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [165/200], Loss: 0.0613\n","Epoch 166: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [166/200], Loss: 0.0603\n","Epoch 167: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [167/200], Loss: 0.0594\n","Epoch 168: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [168/200], Loss: 0.0584\n","Epoch 169: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [169/200], Loss: 0.0575\n","Epoch 170: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [170/200], Loss: 0.0566\n","Epoch 171: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [171/200], Loss: 0.0557\n","Epoch 172: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [172/200], Loss: 0.0548\n","Epoch 173: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [173/200], Loss: 0.0540\n","Epoch 174: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [174/200], Loss: 0.0531\n","Epoch 175: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [175/200], Loss: 0.0523\n","Epoch 176: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [176/200], Loss: 0.0515\n","Epoch 177: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [177/200], Loss: 0.0507\n","Epoch 178: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [178/200], Loss: 0.0500\n","Epoch 179: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [179/200], Loss: 0.0492\n","Epoch 180: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [180/200], Loss: 0.0485\n","Epoch 181: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [181/200], Loss: 0.0478\n","Epoch 182: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [182/200], Loss: 0.0471\n","Epoch 183: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [183/200], Loss: 0.0464\n","Epoch 184: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [184/200], Loss: 0.0457\n","Epoch 185: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [185/200], Loss: 0.0451\n","Epoch 186: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [186/200], Loss: 0.0444\n","Epoch 187: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [187/200], Loss: 0.0438\n","Epoch 188: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [188/200], Loss: 0.0432\n","Epoch 189: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [189/200], Loss: 0.0426\n","Epoch 190: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [190/200], Loss: 0.0420\n","Epoch 191: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [191/200], Loss: 0.0414\n","Epoch 192: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [192/200], Loss: 0.0408\n","Epoch 193: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [193/200], Loss: 0.0403\n","Epoch 194: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [194/200], Loss: 0.0397\n","Epoch 195: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [195/200], Loss: 0.0392\n","Epoch 196: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [196/200], Loss: 0.0387\n","Epoch 197: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [197/200], Loss: 0.0382\n","Epoch 198: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [198/200], Loss: 0.0376\n","Epoch 199: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [199/200], Loss: 0.0371\n","Epoch 200: Outputs shape: torch.Size([20, 16]), Responses shape: torch.Size([20])\n","Outuput shape: 20\n","Response shape: 20\n","Epoch [200/200], Loss: 0.0367\n"]}],"source":["# Define loss function and optimizer :\n","criterion = nn.CrossEntropyLoss() #commonly used loss function for classification problems. It measures the difference between the predicted probability distribution and the actual distribution\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #popular optimization algorithm that combines the advantages of Adagrad and RMSprop. lr parameter (learning rate) determines the step size the optimizer will take when updating parameters.\n","\n","# Additional debugging:\n","print(\"Example padded pattern:\", padded_patterns[0])\n","print(\"Example padded response:\", padded_responses[0])\n","\n","# Training loop\n","num_epochs = 200\n","for epoch in range(num_epochs):\n","    optimizer.zero_grad()  # Clear the gradients(rate of change of the loss function with respect to the model's parameters.) from the previous step, resets the accumulated gradients for each epoch.\n","    outputs = model(patterns)  # Forward pass: Compute predicted outputs by passing inputs to the model\n","\n","    # Reshape outputs and responses for loss computation to match the expected input format\n","    outputs = outputs.view(-1, output_size)  # Shape: (batch_size, sequence_length, output_size) to (batch_size * sequence_length, output_size)\n","    responses = responses.view(-1)  # Shape: (batch_size, sequence_length) to (batch_size * sequence_length)  # Convert to long for CrossEntropyLoss\n","\n","    print(f\"Epoch {epoch+1}: Outputs shape: {outputs.shape}, Responses shape: {responses.shape}\")  # Debug print\n","\n","    # Additional debugging:\n","    print(\"Outuput shape:\", outputs.shape[0])\n","    print(\"Response shape:\", responses.shape[0])\n","\n","    #  Check for size mismatch (optional, but helpful for debugging)\n","    if outputs.shape[0] != responses.shape[0]:\n","        print(f\"WARNING: Mismatch in output and response sizes. Outputs: {outputs.shape}, Responses: {responses.shape}\")\n","\n","    # Proceed with loss calculation only if sizes match\n","    else:\n","      loss = criterion(outputs, responses)  # Compute the loss and difference between predicted and actual outputs.\n","      loss.backward()  # Backward pass: Compute gradient of the loss with respect to model parameters\n","      optimizer.step()  # Update model parameters based on computed gradients\n","      # Print the loss every 100 epochs if epochs =1000\n","      # if (epoch + 1) % 100 == 0:\n","      print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n"]},{"cell_type":"code","execution_count":6,"id":"NOb7supH_IeF","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NOb7supH_IeF","jupyter":{"source_hidden":true},"outputId":"603be32a-4aee-4b01-b24c-5aa11c1177d7","executionInfo":{"status":"ok","timestamp":1723189941981,"user_tz":-345,"elapsed":480,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 199: Outputs shape: torch.Size([4, 5, 16])\n","Epoch 199: Reshaped Outputs shape: torch.Size([20, 16])\n","Epoch 199: Responses shape: torch.Size([20])\n","Epoch 199: Output size: torch.Size([4, 5, 16])\n","Epoch 199: Target size: torch.Size([20])\n","Example padded pattern: [7, 0, 0, 0, 0]\n","Example padded response: 8\n","Vocabulary Size: 16\n","Before reshaping Outputs shape: torch.Size([4, 5, 16])\n","After reshaping Outputs shape: torch.Size([20, 16])\n"]}],"source":["#Debugging\n","\n","# After forward pass\n","outputs = model(patterns)  # Assuming `outputs` is the model's output\n","print(\"Epoch {}: Outputs shape: {}\".format(epoch, outputs.shape))\n","\n","# If needed, reshaping outputs for loss calculation\n","outputs_reshaped = outputs.view(-1, outputs.size(-1))\n","print(\"Epoch {}: Reshaped Outputs shape: {}\".format(epoch, outputs_reshaped.shape))\n","\n","# Print the target shape\n","print(\"Epoch {}: Responses shape: {}\".format(epoch, responses.shape))\n","\n","# Ensure outputs and targets have matching dimensions\n","print(\"Epoch {}: Output size: {}\".format(epoch, outputs.size()))\n","print(\"Epoch {}: Target size: {}\".format(epoch, responses.size()))\n","\n","# Example padded pattern and response\n","print(\"Example padded pattern: {}\".format(patterns[0].tolist()))\n","print(\"Example padded response: {}\".format(responses[0].tolist()))\n","\n","# Assuming vocab_size is defined somewhere in your code\n","print(\"Vocabulary Size: {}\".format(vocab_size))\n","\n","# Example with CrossEntropyLoss\n","criterion = nn.CrossEntropyLoss()\n","\n","# Debug reshaping logic\n","print(\"Before reshaping Outputs shape: {}\".format(outputs.shape))\n","print(\"After reshaping Outputs shape: {}\".format(outputs_reshaped.shape))\n","\n"]},{"cell_type":"code","execution_count":7,"id":"b93b2f92-f53d-432f-a379-4cf4e68f96df","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b93b2f92-f53d-432f-a379-4cf4e68f96df","jupyter":{"source_hidden":true},"outputId":"09de2ecd-3859-4d60-fa45-870be19e98ce","executionInfo":{"status":"ok","timestamp":1723190043496,"user_tz":-345,"elapsed":98025,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Start chatting with the bot (type 'quit' to stop)!\n","You: hi\n","Bot: hi a a a a\n","You: what is your name\n","Bot: im a chatbot a a\n","You: how are you\n","Bot: im good how about you\n","You: hello\n","Bot: hi a a a a\n","You: im also fine\n","Bot: goodbye a a a a\n","You: how are you\n","Bot: im good how about you\n","You: bye\n","Bot: goodbye a a a a\n","You: quit\n"]}],"source":["#prediction\n","def predict_response(input_text): #processing the input text, feeding it to a model, and generating a response.\n","    input_text = preprocess(input_text) #preprocessing: perform some initial cleaning or transformation on the input text like lowercasing, removing punctuation, tokenization, or stemming.\n","    input_pattern = encode(input_text) #encoding: preprocessed text is converted into a numerical representation or pattern\n","    input_pattern = pad_sequence(input_pattern, max_len) #padding: ensures that all input patterns have the same length (max_len) by adding padding\n","    input_pattern = torch.tensor(input_pattern, dtype=torch.long).unsqueeze(0) #tensor conversion: padded input pattern is converted into a PyTorch tensor with data type torch.long\n","\n","\n","    output = model(input_pattern) #model predictin: repared input tensor is fed into the neural network model to generate an output\n","    _, predicted = torch.max(output, dim=2) # finds the index of the most probable response for each time step in the output\n","    predicted = predicted.squeeze(0).numpy() #dimension reduction: The predicted indices are squeezed to remove unnecessary dimensions and converted to a NumPy array for easier manipulation.\n","\n","    response_words = [all_words[idx] for idx in predicted if idx in word_to_idx.values()] #decoding: predicted indices are mapped back to words using the all_words vocabulary.\n","    response_text = ' '.join(response_words) #Response Formation: redicted words are joined together to form the final response text.\n","    return response_text\n","\n","# Chat with the bot\n","print(\"Start chatting with the bot (type 'quit' to stop)!\")\n","while True:\n","    user_input = input(\"You: \")\n","    if user_input.lower() == 'quit':\n","        break\n","    response = predict_response(user_input)\n","    print(f\"Bot: {response}\")\n"]},{"cell_type":"markdown","id":"4z8v0qkI_PF8","metadata":{"id":"4z8v0qkI_PF8"},"source":["# Solved Working Chatbot"]},{"cell_type":"markdown","id":"72a707ae-0e0f-4e41-8ffe-449b73fdafff","metadata":{"id":"72a707ae-0e0f-4e41-8ffe-449b73fdafff"},"source":["## 1. Training Chatbot (10 epochs)"]},{"cell_type":"code","execution_count":14,"id":"46b4eb9b-a620-4a99-a8ca-af561cd47b50","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":264},"id":"46b4eb9b-a620-4a99-a8ca-af561cd47b50","jupyter":{"source_hidden":true},"outputId":"6951a533-51cc-4aac-efa8-076fe40e4682","executionInfo":{"status":"ok","timestamp":1723190784864,"user_tz":-345,"elapsed":91355,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-1c13caa1-2625-4e7a-b480-9bcf5a3c560c\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-1c13caa1-2625-4e7a-b480-9bcf5a3c560c\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving chatbot-dialogs.txt to chatbot-dialogs.txt\n","User uploaded file \"chatbot-dialogs.txt\" with length 243904 bytes\n","Epoch [1/10], Loss: 7.8241\n","Epoch [2/10], Loss: 7.7590\n","Epoch [3/10], Loss: 7.6935\n","Epoch [4/10], Loss: 7.6249\n","Epoch [5/10], Loss: 7.5519\n","Epoch [6/10], Loss: 7.4736\n","Epoch [7/10], Loss: 7.3895\n","Epoch [8/10], Loss: 7.2992\n","Epoch [9/10], Loss: 7.2022\n","Epoch [10/10], Loss: 7.0981\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import random\n","import re\n","import numpy as np\n","\n","#loading file from drive directly\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","#loading file through upload button\n","from google.colab import files\n","\n","uploaded = files.upload()\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))\n","\n","# Load dataset\n","def load_dataset(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        data = [line.strip().split('\\t') for line in file.readlines()]\n","    return data\n","\n","# Preprocessing\n","def preprocess(text):\n","    text = text.lower()\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    return text\n","\n","# Vocabulary and encoding\n","def build_vocab(data):\n","    all_words = []\n","    for (pattern, response) in data:\n","        pattern = preprocess(pattern)\n","        response = preprocess(response)\n","        words = pattern.split() + response.split()\n","        all_words.extend(words)\n","    all_words = sorted(set(all_words))\n","    word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n","    return word_to_idx, all_words\n","\n","def encode(text, word_to_idx):\n","    text = preprocess(text)\n","    return [word_to_idx[word] for word in text.split() if word in word_to_idx]\n","\n","def build_vocab_and_encode(data):\n","    word_to_idx, all_words = build_vocab(data)\n","    encoded_data = [(encode(pattern, word_to_idx), encode(response, word_to_idx)) for (pattern, response) in data]\n","    return encoded_data, word_to_idx, all_words\n","\n","# Pad sequences\n","def pad_sequence(seq, max_len, padding_value=0):\n","    return seq + [padding_value] * (max_len - len(seq))\n","\n","# Load and process dataset\n","# file_path = '/content/drive/My Drive/Colab Notebooks/chatbot-dialogs.txt'  # Path to your dataset file\n","file_path = 'chatbot-dialogs.txt'  # Path to your dataset file\n","data = load_dataset(file_path)\n","encoded_data, word_to_idx, all_words = build_vocab_and_encode(data)\n","\n","# Determine maximum length\n","max_pattern_len = max(len(pattern) for pattern, response in encoded_data)\n","max_response_len = max(len(response) for pattern, response in encoded_data)\n","max_len = max(max_pattern_len, max_response_len)\n","\n","# Pad patterns and responses\n","padded_patterns = [pad_sequence(pattern, max_len) for pattern, response in encoded_data]\n","padded_responses = [pad_sequence(response, max_len) for pattern, response in encoded_data]\n","\n","# Convert to tensors\n","patterns = torch.tensor(padded_patterns, dtype=torch.long)\n","responses = torch.tensor(padded_responses, dtype=torch.long)\n","\n","class ChatbotModel(nn.Module):\n","    def __init__(self, vocab_size, embed_size, hidden_size, output_size, max_len):\n","        super(ChatbotModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n","        self.fc = nn.Linear(hidden_size * max_len, output_size * max_len)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x, _ = self.lstm(x)\n","        x = x.contiguous().view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x.view(x.size(0), max_len, -1)\n","\n","# Initialize model\n","vocab_size = len(all_words)\n","embed_size = 10\n","hidden_size = 20\n","output_size = vocab_size\n","model = ChatbotModel(vocab_size, embed_size, hidden_size, output_size, max_len)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    optimizer.zero_grad()\n","    outputs = model(patterns)\n","\n","    outputs = outputs.view(-1, output_size)\n","    responses = responses.view(-1)\n","\n","    if outputs.shape[0] != responses.shape[0]:\n","        print(f\"WARNING: Mismatch in output and response sizes. Outputs: {outputs.shape}, Responses: {responses.shape}\")\n","    else:\n","        loss = criterion(outputs, responses)\n","        loss.backward()\n","        optimizer.step()\n","        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n"]},{"cell_type":"markdown","id":"ff6f5e01-8fcd-40ff-bbb2-5d87f331d3be","metadata":{"id":"ff6f5e01-8fcd-40ff-bbb2-5d87f331d3be"},"source":["## Prediction"]},{"cell_type":"code","execution_count":15,"id":"1955292e-5db1-4cf8-a52b-24184d1b9f50","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"1955292e-5db1-4cf8-a52b-24184d1b9f50","executionInfo":{"status":"ok","timestamp":1723190874314,"user_tz":-345,"elapsed":81557,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"}},"outputId":"33825f98-a7be-41bb-cfaf-98792fe8089a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start chatting with the bot (type 'quit' to stop)!\n","You: hi\n","Bot: a i watch sure not the all 011287 was you me it 011287 the 011287 011287 011287 011287 011287\n","You: how are you\n","Bot: a i watch about here the all 011287 was you 011287 it 011287 011287 011287 011287 011287 011287 011287\n","You: 've actually been pretty good. you?\n","Bot: are i wont about give the all to was 011287 011287 the 011287 011287 011287 011287 011287 011287 011287\n","You: which school do you attend?\n","Bot: really i wont about give about up to was you 011287 to it 011287 011287 011287 011287 011287 011287\n","You: quit\n"]}],"source":["def predict_response(input_text):\n","    input_text = preprocess(input_text)\n","    input_pattern = encode(input_text, word_to_idx)\n","    input_pattern = pad_sequence(input_pattern, max_len)\n","    input_pattern = torch.tensor(input_pattern, dtype=torch.long).unsqueeze(0)\n","\n","    with torch.no_grad():\n","        output = model(input_pattern)\n","\n","    output = output.view(-1, output_size)\n","    _, predicted = torch.max(output, dim=1)\n","\n","    predicted = predicted.numpy()\n","\n","    response_words = [all_words[idx] for idx in predicted if idx < len(all_words)]\n","    response_text = ' '.join(response_words)\n","    return response_text\n","\n","# Chat with the bot\n","print(\"Start chatting with the bot (type 'quit' to stop)!\")\n","while True:\n","    user_input = input(\"You: \")\n","    if user_input.lower() == 'quit':\n","        break\n","    response = predict_response(user_input)\n","    print(f\"Bot: {response}\")"]},{"cell_type":"markdown","id":"a89dd390-c62b-43c5-a962-a7f8c513e510","metadata":{"id":"a89dd390-c62b-43c5-a962-a7f8c513e510"},"source":["## 2. Training Chatbot(100 epochs and GPU based)"]},{"cell_type":"code","execution_count":17,"id":"d6ff085c-9905-4700-9354-61b7f66ae432","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"d6ff085c-9905-4700-9354-61b7f66ae432","executionInfo":{"status":"ok","timestamp":1723191002377,"user_tz":-345,"elapsed":25876,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"}},"outputId":"70c8f708-a45e-417f-b2fb-d4452627d809"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-a59364d1-66a3-4dc9-9ab0-5eed6214f8d7\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-a59364d1-66a3-4dc9-9ab0-5eed6214f8d7\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving chatbot-dialogs.txt to chatbot-dialogs (2).txt\n","User uploaded file \"chatbot-dialogs (2).txt\" with length 243904 bytes\n","Epoch [1/100], Loss: 7.8505\n","Epoch [2/100], Loss: 7.7394\n","Epoch [3/100], Loss: 7.6283\n","Epoch [4/100], Loss: 7.5127\n","Epoch [5/100], Loss: 7.3913\n","Epoch [6/100], Loss: 7.2638\n","Epoch [7/100], Loss: 7.1297\n","Epoch [8/100], Loss: 6.9888\n","Epoch [9/100], Loss: 6.8408\n","Epoch [10/100], Loss: 6.6857\n","Epoch [11/100], Loss: 6.5232\n","Epoch [12/100], Loss: 6.3532\n","Epoch [13/100], Loss: 6.1758\n","Epoch [14/100], Loss: 5.9908\n","Epoch [15/100], Loss: 5.7982\n","Epoch [16/100], Loss: 5.5982\n","Epoch [17/100], Loss: 5.3908\n","Epoch [18/100], Loss: 5.1762\n","Epoch [19/100], Loss: 4.9546\n","Epoch [20/100], Loss: 4.7263\n","Epoch [21/100], Loss: 4.4921\n","Epoch [22/100], Loss: 4.2528\n","Epoch [23/100], Loss: 4.0101\n","Epoch [24/100], Loss: 3.7666\n","Epoch [25/100], Loss: 3.5263\n","Epoch [26/100], Loss: 3.2946\n","Epoch [27/100], Loss: 3.0779\n","Epoch [28/100], Loss: 2.8828\n","Epoch [29/100], Loss: 2.7140\n","Epoch [30/100], Loss: 2.5741\n","Epoch [31/100], Loss: 2.4625\n","Epoch [32/100], Loss: 2.3768\n","Epoch [33/100], Loss: 2.3133\n","Epoch [34/100], Loss: 2.2678\n","Epoch [35/100], Loss: 2.2361\n","Epoch [36/100], Loss: 2.2144\n","Epoch [37/100], Loss: 2.1992\n","Epoch [38/100], Loss: 2.1880\n","Epoch [39/100], Loss: 2.1788\n","Epoch [40/100], Loss: 2.1702\n","Epoch [41/100], Loss: 2.1615\n","Epoch [42/100], Loss: 2.1523\n","Epoch [43/100], Loss: 2.1426\n","Epoch [44/100], Loss: 2.1325\n","Epoch [45/100], Loss: 2.1222\n","Epoch [46/100], Loss: 2.1118\n","Epoch [47/100], Loss: 2.1016\n","Epoch [48/100], Loss: 2.0918\n","Epoch [49/100], Loss: 2.0824\n","Epoch [50/100], Loss: 2.0735\n","Epoch [51/100], Loss: 2.0653\n","Epoch [52/100], Loss: 2.0577\n","Epoch [53/100], Loss: 2.0506\n","Epoch [54/100], Loss: 2.0441\n","Epoch [55/100], Loss: 2.0381\n","Epoch [56/100], Loss: 2.0324\n","Epoch [57/100], Loss: 2.0270\n","Epoch [58/100], Loss: 2.0219\n","Epoch [59/100], Loss: 2.0170\n","Epoch [60/100], Loss: 2.0122\n","Epoch [61/100], Loss: 2.0076\n","Epoch [62/100], Loss: 2.0031\n","Epoch [63/100], Loss: 1.9987\n","Epoch [64/100], Loss: 1.9944\n","Epoch [65/100], Loss: 1.9903\n","Epoch [66/100], Loss: 1.9864\n","Epoch [67/100], Loss: 1.9826\n","Epoch [68/100], Loss: 1.9790\n","Epoch [69/100], Loss: 1.9754\n","Epoch [70/100], Loss: 1.9720\n","Epoch [71/100], Loss: 1.9686\n","Epoch [72/100], Loss: 1.9652\n","Epoch [73/100], Loss: 1.9619\n","Epoch [74/100], Loss: 1.9585\n","Epoch [75/100], Loss: 1.9552\n","Epoch [76/100], Loss: 1.9519\n","Epoch [77/100], Loss: 1.9486\n","Epoch [78/100], Loss: 1.9454\n","Epoch [79/100], Loss: 1.9422\n","Epoch [80/100], Loss: 1.9391\n","Epoch [81/100], Loss: 1.9361\n","Epoch [82/100], Loss: 1.9330\n","Epoch [83/100], Loss: 1.9300\n","Epoch [84/100], Loss: 1.9270\n","Epoch [85/100], Loss: 1.9240\n","Epoch [86/100], Loss: 1.9210\n","Epoch [87/100], Loss: 1.9181\n","Epoch [88/100], Loss: 1.9151\n","Epoch [89/100], Loss: 1.9121\n","Epoch [90/100], Loss: 1.9092\n","Epoch [91/100], Loss: 1.9062\n","Epoch [92/100], Loss: 1.9033\n","Epoch [93/100], Loss: 1.9003\n","Epoch [94/100], Loss: 1.8974\n","Epoch [95/100], Loss: 1.8945\n","Epoch [96/100], Loss: 1.8915\n","Epoch [97/100], Loss: 1.8886\n","Epoch [98/100], Loss: 1.8857\n","Epoch [99/100], Loss: 1.8828\n","Epoch [100/100], Loss: 1.8799\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import random\n","import re\n","import numpy as np\n","\n","#loading file through upload button\n","from google.colab import files\n","\n","uploaded = files.upload()\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))\n","\n","# Load dataset\n","def load_dataset(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        data = [line.strip().split('\\t') for line in file.readlines()]\n","    return data\n","\n","# Preprocessing\n","def preprocess(text):\n","    text = text.lower()\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    return text\n","\n","# Vocabulary and encoding\n","def build_vocab(data):\n","    all_words = []\n","    for (pattern, response) in data:\n","        pattern = preprocess(pattern)\n","        response = preprocess(response)\n","        words = pattern.split() + response.split()\n","        all_words.extend(words)\n","    all_words = sorted(set(all_words))\n","    word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n","    return word_to_idx, all_words\n","\n","def encode(text, word_to_idx):\n","    text = preprocess(text)\n","    return [word_to_idx[word] for word in text.split() if word in word_to_idx]\n","\n","def build_vocab_and_encode(data):\n","    word_to_idx, all_words = build_vocab(data)\n","    encoded_data = [(encode(pattern, word_to_idx), encode(response, word_to_idx)) for (pattern, response) in data]\n","    return encoded_data, word_to_idx, all_words\n","\n","# Pad sequences\n","def pad_sequence(seq, max_len, padding_value=0):\n","    return seq + [padding_value] * (max_len - len(seq))\n","\n","# Load and process dataset\n","file_path = 'chatbot-dialogs.txt'  # Path to your dataset file\n","data = load_dataset(file_path)\n","encoded_data, word_to_idx, all_words = build_vocab_and_encode(data)\n","\n","# Determine maximum length\n","max_pattern_len = max(len(pattern) for pattern, response in encoded_data)\n","max_response_len = max(len(response) for pattern, response in encoded_data)\n","max_len = max(max_pattern_len, max_response_len)\n","\n","# Pad patterns and responses\n","padded_patterns = [pad_sequence(pattern, max_len) for pattern, response in encoded_data]\n","padded_responses = [pad_sequence(response, max_len) for pattern, response in encoded_data]\n","\n","# Convert to tensors\n","patterns = torch.tensor(padded_patterns, dtype=torch.long)\n","responses = torch.tensor(padded_responses, dtype=torch.long)\n","\n","# Check for GPU availability and set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class ChatbotModel(nn.Module):\n","    def __init__(self, vocab_size, embed_size, hidden_size, output_size, max_len):\n","        super(ChatbotModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n","        self.fc = nn.Linear(hidden_size * max_len, output_size * max_len)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x, _ = self.lstm(x)\n","        x = x.contiguous().view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x.view(x.size(0), max_len, -1)\n","\n","# Initialize model\n","vocab_size = len(all_words)\n","embed_size = 10\n","hidden_size = 20\n","output_size = vocab_size\n","model = ChatbotModel(vocab_size, embed_size, hidden_size, output_size, max_len).to(device)\n","\n","# Move tensors to device\n","patterns = patterns.to(device)\n","responses = responses.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 100  # Increase the number of epochs\n","for epoch in range(num_epochs):\n","    optimizer.zero_grad()\n","    outputs = model(patterns)\n","\n","    outputs = outputs.view(-1, output_size)\n","    responses = responses.view(-1)\n","\n","    if outputs.shape[0] != responses.shape[0]:\n","        print(f\"WARNING: Mismatch in output and response sizes. Outputs: {outputs.shape}, Responses: {responses.shape}\")\n","    else:\n","        loss = criterion(outputs, responses)\n","        loss.backward()\n","        optimizer.step()\n","        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n"]},{"cell_type":"markdown","id":"7e26d4f7-2784-48e0-aeda-5d364b3cc1cc","metadata":{"id":"7e26d4f7-2784-48e0-aeda-5d364b3cc1cc"},"source":["# Prediction (100 epochs)"]},{"cell_type":"code","execution_count":18,"id":"d6870c59-3e4e-4af1-9ac4-7daa10ed0176","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"colab":{"base_uri":"https://localhost:8080/"},"id":"d6870c59-3e4e-4af1-9ac4-7daa10ed0176","executionInfo":{"status":"ok","timestamp":1723191108256,"user_tz":-345,"elapsed":100537,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"}},"outputId":"2b14cedc-5ba3-4604-c087-1ade64a5a5f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start chatting with the bot (type 'quit' to stop)!\n","You: hi\n","Bot: i you you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n","You: hi, how are you doing?\n","Bot: i you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n","You: what school do you go to?\n","Bot: i you you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n","You: how are you\n","Bot: i you you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n","You: bye\n","Bot: i you you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n","You: quit\n"]}],"source":["def predict_response(input_text):\n","    input_text = preprocess(input_text)\n","    input_pattern = encode(input_text, word_to_idx)\n","    input_pattern = pad_sequence(input_pattern, max_len)\n","    input_pattern = torch.tensor(input_pattern, dtype=torch.long).unsqueeze(0).to(device)  # Move to device\n","\n","    with torch.no_grad():\n","        output = model(input_pattern)\n","\n","    output = output.view(-1, output_size)\n","    _, predicted = torch.max(output, dim=1)\n","\n","    predicted = predicted.cpu().numpy()  # Move to CPU before converting to numpy\n","\n","    response_words = [all_words[idx] for idx in predicted if idx < len(all_words)]\n","    response_text = ' '.join(response_words)\n","    return response_text\n","\n","# Chat with the bot\n","print(\"Start chatting with the bot (type 'quit' to stop)!\")\n","while True:\n","    user_input = input(\"You: \")\n","    if user_input.lower() == 'quit':\n","        break\n","    response = predict_response(user_input)\n","    print(f\"Bot: {response}\")\n"]},{"cell_type":"markdown","id":"b1ec6d01-6c19-401d-b571-f98ce74f9b7f","metadata":{"id":"b1ec6d01-6c19-401d-b571-f98ce74f9b7f"},"source":["# 3. Training (400 epochs)"]},{"cell_type":"code","execution_count":null,"id":"2d5eb69e-25ae-46e1-84c8-2cffcb97dbf5","metadata":{"id":"2d5eb69e-25ae-46e1-84c8-2cffcb97dbf5","outputId":"c9632871-7281-4622-b6d0-373f0d1465e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Available GPUs:\n","GPU 0: NVIDIA GeForce RTX 3050 Laptop GPU\n"]}],"source":["import torch\n","\n","print(\"Available GPUs:\")\n","for i in range(torch.cuda.device_count()):\n","    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n"]},{"cell_type":"code","execution_count":19,"id":"eb9f1e9b-0555-4bdb-a9c6-3476c8ff1817","metadata":{"jupyter":{"source_hidden":true},"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"eb9f1e9b-0555-4bdb-a9c6-3476c8ff1817","executionInfo":{"status":"ok","timestamp":1723191520225,"user_tz":-345,"elapsed":364942,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"}},"outputId":"5324e05d-2644-473a-fcfb-4300e3d042c2"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-2f367b55-9b61-400e-ab13-23aca071f660\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-2f367b55-9b61-400e-ab13-23aca071f660\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving chatbot-dialogs.txt to chatbot-dialogs (3).txt\n","User uploaded file \"chatbot-dialogs (3).txt\" with length 243904 bytes\n","Epoch [1/400], Loss: 7.8297\n","Epoch [2/400], Loss: 7.6606\n","Epoch [3/400], Loss: 7.4114\n","Epoch [4/400], Loss: 7.0219\n","Epoch [5/400], Loss: 6.4362\n","Epoch [6/400], Loss: 5.6056\n","Epoch [7/400], Loss: 4.5346\n","Epoch [8/400], Loss: 3.3894\n","Epoch [9/400], Loss: 2.6556\n","Epoch [10/400], Loss: 2.4141\n","Epoch [11/400], Loss: 2.4277\n","Epoch [12/400], Loss: 2.5111\n","Epoch [13/400], Loss: 2.5162\n","Epoch [14/400], Loss: 2.4490\n","Epoch [15/400], Loss: 2.3816\n","Epoch [16/400], Loss: 2.3438\n","Epoch [17/400], Loss: 2.3216\n","Epoch [18/400], Loss: 2.3036\n","Epoch [19/400], Loss: 2.2841\n","Epoch [20/400], Loss: 2.2594\n","Epoch [21/400], Loss: 2.2280\n","Epoch [22/400], Loss: 2.1923\n","Epoch [23/400], Loss: 2.1578\n","Epoch [24/400], Loss: 2.1316\n","Epoch [25/400], Loss: 2.1159\n","Epoch [26/400], Loss: 2.1104\n","Epoch [27/400], Loss: 2.1110\n","Epoch [28/400], Loss: 2.1100\n","Epoch [29/400], Loss: 2.1026\n","Epoch [30/400], Loss: 2.0897\n","Epoch [31/400], Loss: 2.0744\n","Epoch [32/400], Loss: 2.0630\n","Epoch [33/400], Loss: 2.0543\n","Epoch [34/400], Loss: 2.0486\n","Epoch [35/400], Loss: 2.0429\n","Epoch [36/400], Loss: 2.0364\n","Epoch [37/400], Loss: 2.0292\n","Epoch [38/400], Loss: 2.0215\n","Epoch [39/400], Loss: 2.0149\n","Epoch [40/400], Loss: 2.0100\n","Epoch [41/400], Loss: 2.0056\n","Epoch [42/400], Loss: 2.0022\n","Epoch [43/400], Loss: 1.9982\n","Epoch [44/400], Loss: 1.9937\n","Epoch [45/400], Loss: 1.9884\n","Epoch [46/400], Loss: 1.9825\n","Epoch [47/400], Loss: 1.9768\n","Epoch [48/400], Loss: 1.9721\n","Epoch [49/400], Loss: 1.9679\n","Epoch [50/400], Loss: 1.9637\n","Epoch [51/400], Loss: 1.9596\n","Epoch [52/400], Loss: 1.9548\n","Epoch [53/400], Loss: 1.9503\n","Epoch [54/400], Loss: 1.9457\n","Epoch [55/400], Loss: 1.9415\n","Epoch [56/400], Loss: 1.9366\n","Epoch [57/400], Loss: 1.9321\n","Epoch [58/400], Loss: 1.9274\n","Epoch [59/400], Loss: 1.9220\n","Epoch [60/400], Loss: 1.9174\n","Epoch [61/400], Loss: 1.9125\n","Epoch [62/400], Loss: 1.9076\n","Epoch [63/400], Loss: 1.9028\n","Epoch [64/400], Loss: 1.8974\n","Epoch [65/400], Loss: 1.8922\n","Epoch [66/400], Loss: 1.8874\n","Epoch [67/400], Loss: 1.8816\n","Epoch [68/400], Loss: 1.8763\n","Epoch [69/400], Loss: 1.8707\n","Epoch [70/400], Loss: 1.8652\n","Epoch [71/400], Loss: 1.8598\n","Epoch [72/400], Loss: 1.8538\n","Epoch [73/400], Loss: 1.8473\n","Epoch [74/400], Loss: 1.8414\n","Epoch [75/400], Loss: 1.8346\n","Epoch [76/400], Loss: 1.8283\n","Epoch [77/400], Loss: 1.8217\n","Epoch [78/400], Loss: 1.8149\n","Epoch [79/400], Loss: 1.8077\n","Epoch [80/400], Loss: 1.7997\n","Epoch [81/400], Loss: 1.7925\n","Epoch [82/400], Loss: 1.7846\n","Epoch [83/400], Loss: 1.7766\n","Epoch [84/400], Loss: 1.7685\n","Epoch [85/400], Loss: 1.7590\n","Epoch [86/400], Loss: 1.7505\n","Epoch [87/400], Loss: 1.7411\n","Epoch [88/400], Loss: 1.7315\n","Epoch [89/400], Loss: 1.7218\n","Epoch [90/400], Loss: 1.7119\n","Epoch [91/400], Loss: 1.7008\n","Epoch [92/400], Loss: 1.6893\n","Epoch [93/400], Loss: 1.6785\n","Epoch [94/400], Loss: 1.6665\n","Epoch [95/400], Loss: 1.6538\n","Epoch [96/400], Loss: 1.6408\n","Epoch [97/400], Loss: 1.6281\n","Epoch [98/400], Loss: 1.6144\n","Epoch [99/400], Loss: 1.6005\n","Epoch [100/400], Loss: 1.5866\n","Epoch [101/400], Loss: 1.5723\n","Epoch [102/400], Loss: 1.5567\n","Epoch [103/400], Loss: 1.5426\n","Epoch [104/400], Loss: 1.5261\n","Epoch [105/400], Loss: 1.5099\n","Epoch [106/400], Loss: 1.4947\n","Epoch [107/400], Loss: 1.4770\n","Epoch [108/400], Loss: 1.4602\n","Epoch [109/400], Loss: 1.4434\n","Epoch [110/400], Loss: 1.4260\n","Epoch [111/400], Loss: 1.4079\n","Epoch [112/400], Loss: 1.3903\n","Epoch [113/400], Loss: 1.3734\n","Epoch [114/400], Loss: 1.3547\n","Epoch [115/400], Loss: 1.3371\n","Epoch [116/400], Loss: 1.3193\n","Epoch [117/400], Loss: 1.3024\n","Epoch [118/400], Loss: 1.2843\n","Epoch [119/400], Loss: 1.2666\n","Epoch [120/400], Loss: 1.2486\n","Epoch [121/400], Loss: 1.2316\n","Epoch [122/400], Loss: 1.2137\n","Epoch [123/400], Loss: 1.1976\n","Epoch [124/400], Loss: 1.1831\n","Epoch [125/400], Loss: 1.1664\n","Epoch [126/400], Loss: 1.1497\n","Epoch [127/400], Loss: 1.1325\n","Epoch [128/400], Loss: 1.1162\n","Epoch [129/400], Loss: 1.1027\n","Epoch [130/400], Loss: 1.0864\n","Epoch [131/400], Loss: 1.0734\n","Epoch [132/400], Loss: 1.0575\n","Epoch [133/400], Loss: 1.0441\n","Epoch [134/400], Loss: 1.0304\n","Epoch [135/400], Loss: 1.0158\n","Epoch [136/400], Loss: 1.0029\n","Epoch [137/400], Loss: 0.9909\n","Epoch [138/400], Loss: 0.9760\n","Epoch [139/400], Loss: 0.9647\n","Epoch [140/400], Loss: 0.9529\n","Epoch [141/400], Loss: 0.9395\n","Epoch [142/400], Loss: 0.9265\n","Epoch [143/400], Loss: 0.9172\n","Epoch [144/400], Loss: 0.9053\n","Epoch [145/400], Loss: 0.8929\n","Epoch [146/400], Loss: 0.8846\n","Epoch [147/400], Loss: 0.8716\n","Epoch [148/400], Loss: 0.8602\n","Epoch [149/400], Loss: 0.8517\n","Epoch [150/400], Loss: 0.8397\n","Epoch [151/400], Loss: 0.8311\n","Epoch [152/400], Loss: 0.8202\n","Epoch [153/400], Loss: 0.8115\n","Epoch [154/400], Loss: 0.8028\n","Epoch [155/400], Loss: 0.7929\n","Epoch [156/400], Loss: 0.7842\n","Epoch [157/400], Loss: 0.7755\n","Epoch [158/400], Loss: 0.7661\n","Epoch [159/400], Loss: 0.7569\n","Epoch [160/400], Loss: 0.7488\n","Epoch [161/400], Loss: 0.7387\n","Epoch [162/400], Loss: 0.7327\n","Epoch [163/400], Loss: 0.7248\n","Epoch [164/400], Loss: 0.7153\n","Epoch [165/400], Loss: 0.7068\n","Epoch [166/400], Loss: 0.6997\n","Epoch [167/400], Loss: 0.6928\n","Epoch [168/400], Loss: 0.6851\n","Epoch [169/400], Loss: 0.6776\n","Epoch [170/400], Loss: 0.6698\n","Epoch [171/400], Loss: 0.6640\n","Epoch [172/400], Loss: 0.6557\n","Epoch [173/400], Loss: 0.6494\n","Epoch [174/400], Loss: 0.6433\n","Epoch [175/400], Loss: 0.6370\n","Epoch [176/400], Loss: 0.6310\n","Epoch [177/400], Loss: 0.6245\n","Epoch [178/400], Loss: 0.6191\n","Epoch [179/400], Loss: 0.6112\n","Epoch [180/400], Loss: 0.6050\n","Epoch [181/400], Loss: 0.6005\n","Epoch [182/400], Loss: 0.5942\n","Epoch [183/400], Loss: 0.5886\n","Epoch [184/400], Loss: 0.5821\n","Epoch [185/400], Loss: 0.5768\n","Epoch [186/400], Loss: 0.5704\n","Epoch [187/400], Loss: 0.5658\n","Epoch [188/400], Loss: 0.5605\n","Epoch [189/400], Loss: 0.5543\n","Epoch [190/400], Loss: 0.5498\n","Epoch [191/400], Loss: 0.5447\n","Epoch [192/400], Loss: 0.5393\n","Epoch [193/400], Loss: 0.5353\n","Epoch [194/400], Loss: 0.5301\n","Epoch [195/400], Loss: 0.5247\n","Epoch [196/400], Loss: 0.5216\n","Epoch [197/400], Loss: 0.5158\n","Epoch [198/400], Loss: 0.5123\n","Epoch [199/400], Loss: 0.5071\n","Epoch [200/400], Loss: 0.5032\n","Epoch [201/400], Loss: 0.4979\n","Epoch [202/400], Loss: 0.4948\n","Epoch [203/400], Loss: 0.4897\n","Epoch [204/400], Loss: 0.4862\n","Epoch [205/400], Loss: 0.4827\n","Epoch [206/400], Loss: 0.4779\n","Epoch [207/400], Loss: 0.4728\n","Epoch [208/400], Loss: 0.4700\n","Epoch [209/400], Loss: 0.4660\n","Epoch [210/400], Loss: 0.4625\n","Epoch [211/400], Loss: 0.4575\n","Epoch [212/400], Loss: 0.4537\n","Epoch [213/400], Loss: 0.4493\n","Epoch [214/400], Loss: 0.4480\n","Epoch [215/400], Loss: 0.4432\n","Epoch [216/400], Loss: 0.4404\n","Epoch [217/400], Loss: 0.4358\n","Epoch [218/400], Loss: 0.4322\n","Epoch [219/400], Loss: 0.4297\n","Epoch [220/400], Loss: 0.4253\n","Epoch [221/400], Loss: 0.4242\n","Epoch [222/400], Loss: 0.4206\n","Epoch [223/400], Loss: 0.4174\n","Epoch [224/400], Loss: 0.4130\n","Epoch [225/400], Loss: 0.4098\n","Epoch [226/400], Loss: 0.4070\n","Epoch [227/400], Loss: 0.4036\n","Epoch [228/400], Loss: 0.4016\n","Epoch [229/400], Loss: 0.3980\n","Epoch [230/400], Loss: 0.3951\n","Epoch [231/400], Loss: 0.3904\n","Epoch [232/400], Loss: 0.3881\n","Epoch [233/400], Loss: 0.3865\n","Epoch [234/400], Loss: 0.3827\n","Epoch [235/400], Loss: 0.3801\n","Epoch [236/400], Loss: 0.3790\n","Epoch [237/400], Loss: 0.3750\n","Epoch [238/400], Loss: 0.3727\n","Epoch [239/400], Loss: 0.3701\n","Epoch [240/400], Loss: 0.3686\n","Epoch [241/400], Loss: 0.3652\n","Epoch [242/400], Loss: 0.3634\n","Epoch [243/400], Loss: 0.3592\n","Epoch [244/400], Loss: 0.3569\n","Epoch [245/400], Loss: 0.3536\n","Epoch [246/400], Loss: 0.3534\n","Epoch [247/400], Loss: 0.3495\n","Epoch [248/400], Loss: 0.3484\n","Epoch [249/400], Loss: 0.3449\n","Epoch [250/400], Loss: 0.3438\n","Epoch [251/400], Loss: 0.3408\n","Epoch [252/400], Loss: 0.3387\n","Epoch [253/400], Loss: 0.3361\n","Epoch [254/400], Loss: 0.3343\n","Epoch [255/400], Loss: 0.3310\n","Epoch [256/400], Loss: 0.3286\n","Epoch [257/400], Loss: 0.3269\n","Epoch [258/400], Loss: 0.3242\n","Epoch [259/400], Loss: 0.3220\n","Epoch [260/400], Loss: 0.3208\n","Epoch [261/400], Loss: 0.3182\n","Epoch [262/400], Loss: 0.3162\n","Epoch [263/400], Loss: 0.3145\n","Epoch [264/400], Loss: 0.3127\n","Epoch [265/400], Loss: 0.3099\n","Epoch [266/400], Loss: 0.3085\n","Epoch [267/400], Loss: 0.3044\n","Epoch [268/400], Loss: 0.3034\n","Epoch [269/400], Loss: 0.3024\n","Epoch [270/400], Loss: 0.2999\n","Epoch [271/400], Loss: 0.2987\n","Epoch [272/400], Loss: 0.2962\n","Epoch [273/400], Loss: 0.2940\n","Epoch [274/400], Loss: 0.2925\n","Epoch [275/400], Loss: 0.2916\n","Epoch [276/400], Loss: 0.2891\n","Epoch [277/400], Loss: 0.2883\n","Epoch [278/400], Loss: 0.2866\n","Epoch [279/400], Loss: 0.2851\n","Epoch [280/400], Loss: 0.2830\n","Epoch [281/400], Loss: 0.2798\n","Epoch [282/400], Loss: 0.2794\n","Epoch [283/400], Loss: 0.2777\n","Epoch [284/400], Loss: 0.2763\n","Epoch [285/400], Loss: 0.2754\n","Epoch [286/400], Loss: 0.2730\n","Epoch [287/400], Loss: 0.2714\n","Epoch [288/400], Loss: 0.2694\n","Epoch [289/400], Loss: 0.2703\n","Epoch [290/400], Loss: 0.2677\n","Epoch [291/400], Loss: 0.2661\n","Epoch [292/400], Loss: 0.2643\n","Epoch [293/400], Loss: 0.2622\n","Epoch [294/400], Loss: 0.2595\n","Epoch [295/400], Loss: 0.2592\n","Epoch [296/400], Loss: 0.2569\n","Epoch [297/400], Loss: 0.2560\n","Epoch [298/400], Loss: 0.2539\n","Epoch [299/400], Loss: 0.2529\n","Epoch [300/400], Loss: 0.2510\n","Epoch [301/400], Loss: 0.2496\n","Epoch [302/400], Loss: 0.2487\n","Epoch [303/400], Loss: 0.2465\n","Epoch [304/400], Loss: 0.2467\n","Epoch [305/400], Loss: 0.2445\n","Epoch [306/400], Loss: 0.2434\n","Epoch [307/400], Loss: 0.2420\n","Epoch [308/400], Loss: 0.2419\n","Epoch [309/400], Loss: 0.2392\n","Epoch [310/400], Loss: 0.2386\n","Epoch [311/400], Loss: 0.2366\n","Epoch [312/400], Loss: 0.2358\n","Epoch [313/400], Loss: 0.2340\n","Epoch [314/400], Loss: 0.2337\n","Epoch [315/400], Loss: 0.2314\n","Epoch [316/400], Loss: 0.2319\n","Epoch [317/400], Loss: 0.2288\n","Epoch [318/400], Loss: 0.2271\n","Epoch [319/400], Loss: 0.2271\n","Epoch [320/400], Loss: 0.2268\n","Epoch [321/400], Loss: 0.2249\n","Epoch [322/400], Loss: 0.2230\n","Epoch [323/400], Loss: 0.2229\n","Epoch [324/400], Loss: 0.2217\n","Epoch [325/400], Loss: 0.2200\n","Epoch [326/400], Loss: 0.2185\n","Epoch [327/400], Loss: 0.2183\n","Epoch [328/400], Loss: 0.2176\n","Epoch [329/400], Loss: 0.2161\n","Epoch [330/400], Loss: 0.2140\n","Epoch [331/400], Loss: 0.2131\n","Epoch [332/400], Loss: 0.2128\n","Epoch [333/400], Loss: 0.2113\n","Epoch [334/400], Loss: 0.2105\n","Epoch [335/400], Loss: 0.2097\n","Epoch [336/400], Loss: 0.2095\n","Epoch [337/400], Loss: 0.2090\n","Epoch [338/400], Loss: 0.2064\n","Epoch [339/400], Loss: 0.2054\n","Epoch [340/400], Loss: 0.2044\n","Epoch [341/400], Loss: 0.2035\n","Epoch [342/400], Loss: 0.2035\n","Epoch [343/400], Loss: 0.2003\n","Epoch [344/400], Loss: 0.2010\n","Epoch [345/400], Loss: 0.1998\n","Epoch [346/400], Loss: 0.1984\n","Epoch [347/400], Loss: 0.1967\n","Epoch [348/400], Loss: 0.1963\n","Epoch [349/400], Loss: 0.1962\n","Epoch [350/400], Loss: 0.1945\n","Epoch [351/400], Loss: 0.1944\n","Epoch [352/400], Loss: 0.1923\n","Epoch [353/400], Loss: 0.1922\n","Epoch [354/400], Loss: 0.1920\n","Epoch [355/400], Loss: 0.1920\n","Epoch [356/400], Loss: 0.1884\n","Epoch [357/400], Loss: 0.1873\n","Epoch [358/400], Loss: 0.1868\n","Epoch [359/400], Loss: 0.1872\n","Epoch [360/400], Loss: 0.1863\n","Epoch [361/400], Loss: 0.1855\n","Epoch [362/400], Loss: 0.1854\n","Epoch [363/400], Loss: 0.1824\n","Epoch [364/400], Loss: 0.1823\n","Epoch [365/400], Loss: 0.1821\n","Epoch [366/400], Loss: 0.1809\n","Epoch [367/400], Loss: 0.1814\n","Epoch [368/400], Loss: 0.1806\n","Epoch [369/400], Loss: 0.1788\n","Epoch [370/400], Loss: 0.1772\n","Epoch [371/400], Loss: 0.1773\n","Epoch [372/400], Loss: 0.1759\n","Epoch [373/400], Loss: 0.1738\n","Epoch [374/400], Loss: 0.1746\n","Epoch [375/400], Loss: 0.1743\n","Epoch [376/400], Loss: 0.1744\n","Epoch [377/400], Loss: 0.1731\n","Epoch [378/400], Loss: 0.1717\n","Epoch [379/400], Loss: 0.1702\n","Epoch [380/400], Loss: 0.1703\n","Epoch [381/400], Loss: 0.1688\n","Epoch [382/400], Loss: 0.1689\n","Epoch [383/400], Loss: 0.1673\n","Epoch [384/400], Loss: 0.1675\n","Epoch [385/400], Loss: 0.1676\n","Epoch [386/400], Loss: 0.1658\n","Epoch [387/400], Loss: 0.1643\n","Epoch [388/400], Loss: 0.1640\n","Epoch [389/400], Loss: 0.1649\n","Epoch [390/400], Loss: 0.1632\n","Epoch [391/400], Loss: 0.1634\n","Epoch [392/400], Loss: 0.1623\n","Epoch [393/400], Loss: 0.1609\n","Epoch [394/400], Loss: 0.1612\n","Epoch [395/400], Loss: 0.1601\n","Epoch [396/400], Loss: 0.1606\n","Epoch [397/400], Loss: 0.1590\n","Epoch [398/400], Loss: 0.1582\n","Epoch [399/400], Loss: 0.1576\n","Epoch [400/400], Loss: 0.1560\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import random\n","import re\n","import numpy as np\n","\n","#loading file through upload button\n","from google.colab import files\n","\n","uploaded = files.upload()\n","for fn in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","      name=fn, length=len(uploaded[fn])))\n","\n","\n","# Load dataset\n","def load_dataset(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        data = [line.strip().split('\\t') for line in file.readlines()]\n","    return data\n","\n","# Preprocessing\n","def preprocess(text):\n","    text = text.lower()\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    return text\n","\n","# Vocabulary and encoding\n","def build_vocab(data):\n","    all_words = []\n","    for (pattern, response) in data:\n","        pattern = preprocess(pattern)\n","        response = preprocess(response)\n","        words = pattern.split() + response.split()\n","        all_words.extend(words)\n","    all_words = sorted(set(all_words))\n","    word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n","    return word_to_idx, all_words\n","\n","def encode(text, word_to_idx):\n","    text = preprocess(text)\n","    return [word_to_idx[word] for word in text.split() if word in word_to_idx]\n","\n","def build_vocab_and_encode(data):\n","    word_to_idx, all_words = build_vocab(data)\n","    encoded_data = [(encode(pattern, word_to_idx), encode(response, word_to_idx)) for (pattern, response) in data]\n","    return encoded_data, word_to_idx, all_words\n","\n","# Pad sequences\n","def pad_sequence(seq, max_len, padding_value=0):\n","    return seq + [padding_value] * (max_len - len(seq))\n","\n","# Load and process dataset\n","file_path = 'chatbot-dialogs.txt'  # Path to your dataset file\n","data = load_dataset(file_path)\n","encoded_data, word_to_idx, all_words = build_vocab_and_encode(data)\n","\n","# Determine maximum length\n","max_pattern_len = max(len(pattern) for pattern, response in encoded_data)\n","max_response_len = max(len(response) for pattern, response in encoded_data)\n","max_len = max(max_pattern_len, max_response_len)\n","\n","# Pad patterns and responses\n","padded_patterns = [pad_sequence(pattern, max_len) for pattern, response in encoded_data]\n","padded_responses = [pad_sequence(response, max_len) for pattern, response in encoded_data]\n","\n","# Convert to tensors\n","patterns = torch.tensor(padded_patterns, dtype=torch.long)\n","responses = torch.tensor(padded_responses, dtype=torch.long)\n","\n","# Check for GPU availability and set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class ChatbotModel(nn.Module):\n","    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, output_size, max_len):\n","        super(ChatbotModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n","        self.fc = nn.Linear(hidden_size * max_len, output_size * max_len)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x, _ = self.lstm(x)\n","        x = x.contiguous().view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x.view(x.size(0), max_len, -1)\n","\n","# Initialize model\n","vocab_size = len(all_words)\n","embed_size = 50\n","hidden_size = 128\n","num_layers = 2\n","output_size = vocab_size\n","model = ChatbotModel(vocab_size, embed_size, hidden_size, num_layers, output_size, max_len).to(device)\n","\n","# Move tensors to device\n","patterns = patterns.to(device)\n","responses = responses.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 400  # Increase the number of epochs\n","for epoch in range(num_epochs):\n","    optimizer.zero_grad()\n","    outputs = model(patterns)\n","\n","    outputs = outputs.view(-1, output_size)\n","    responses = responses.view(-1)\n","\n","    if outputs.shape[0] != responses.shape[0]:\n","        print(f\"WARNING: Mismatch in output and response sizes. Outputs: {outputs.shape}, Responses: {responses.shape}\")\n","    else:\n","        loss = criterion(outputs, responses)\n","        loss.backward()\n","        optimizer.step()\n","        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n"]},{"cell_type":"markdown","id":"b4ca0ac3-974e-43b5-810f-bec0261277b6","metadata":{"id":"b4ca0ac3-974e-43b5-810f-bec0261277b6"},"source":["# 3. Prediction (400 epochs)"]},{"cell_type":"code","execution_count":21,"id":"7700afae-3903-457f-8ffc-a0425bd605ab","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7700afae-3903-457f-8ffc-a0425bd605ab","executionInfo":{"status":"ok","timestamp":1723193215145,"user_tz":-345,"elapsed":99689,"user":{"displayName":"Krish Ken","userId":"09620971272108714705"}},"outputId":"57837441-53e9-4ac7-a8aa-0f62c631da17"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start chatting with the bot (type 'quit' to stop)!\n","You: hi\n","Bot: what you you you\n","You: how are\n","Bot: two dont got\n","You: what school do you go to?\n","Bot: i go a pcc\n","You: how's it going?\n","Bot: im doing well how about\n","You: hi, how are you doing?\n","Bot: im fine how yourself\n","You: i'm fine. how about yourself?\n","Bot: im pretty good thanks for asking\n","You: which school do you attend?\n","Bot: im attending pcc right now\n","You: are you enjoying it there?\n","Bot: its course bad there are lot of people there\n","You: quit\n"]}],"source":["def predict_response(input_text):\n","    input_text = preprocess(input_text)\n","    input_pattern = encode(input_text, word_to_idx)\n","    input_pattern = pad_sequence(input_pattern, max_len)\n","    input_pattern = torch.tensor(input_pattern, dtype=torch.long).unsqueeze(0).to(device)  # Move to device\n","\n","\n","    with torch.no_grad():\n","        output = model(input_pattern)\n","\n","    output = output.view(-1, output_size)\n","    _, predicted = torch.max(output, dim=1)\n","\n","    predicted = predicted.cpu().numpy()  # Move to CPU before converting to numpy\n","\n","    # response_words = [all_words[idx] for idx in predicted if idx < len(all_words)]\n","    response_words = [all_words[idx] for idx in predicted if idx > 0 and idx < len(all_words)]\n","    response_text = ' '.join(response_words)\n","    return response_text\n","\n","# Chat with the bot\n","print(\"Start chatting with the bot (type 'quit' to stop)!\")\n","while True:\n","    user_input = input(\"You: \")\n","    if user_input.lower() == 'quit':\n","        break\n","    response = predict_response(user_input)\n","    print(f\"Bot: {response}\")\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"a65758d7-06ce-4c6c-a09d-572bfda8e3a8","metadata":{"id":"a65758d7-06ce-4c6c-a09d-572bfda8e3a8"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"066ab1e9-40e5-4519-9b6a-7993d2ef54c2","metadata":{"id":"066ab1e9-40e5-4519-9b6a-7993d2ef54c2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ba2e302f-6ff9-4c24-a6fa-7196ad23eb72","metadata":{"jupyter":{"source_hidden":true},"id":"ba2e302f-6ff9-4c24-a6fa-7196ad23eb72","outputId":"c93252a2-0ae6-4c93-b305-3d293176aa52"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch [1/1], Loss: 1.0598\n"]}],"source":["# Data Loader used---------------\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import re\n","import numpy as np\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# Load dataset\n","def load_dataset(file_path):\n","    data = []\n","    with open(file_path, 'r') as f:\n","        for line in f:\n","            if '\\t' in line:\n","                pattern, response = line.strip().split('\\t')\n","                data.append((pattern, response))\n","    return data\n","\n","# Preprocessing\n","def preprocess(text):\n","    text = text.lower()\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    return text\n","\n","# Vocabulary and encoding\n","def build_vocab(data):\n","    all_words = []\n","    for (pattern, response) in data:\n","        pattern = preprocess(pattern)\n","        response = preprocess(response)\n","        words = pattern.split() + response.split()\n","        all_words.extend(words)\n","    all_words = sorted(set(all_words))\n","    word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n","    return word_to_idx, all_words\n","\n","def encode(text, word_to_idx):\n","    text = preprocess(text)\n","    return [word_to_idx[word] for word in text.split() if word in word_to_idx]\n","\n","def build_vocab_and_encode(data):\n","    word_to_idx, all_words = build_vocab(data)\n","    encoded_data = [(encode(pattern, word_to_idx), encode(response, word_to_idx)) for (pattern, response) in data]\n","    return encoded_data, word_to_idx, all_words\n","\n","# Pad sequences\n","def pad_sequence(seq, max_len, padding_value=0):\n","    return seq + [padding_value] * (max_len - len(seq))\n","\n","# Load and process dataset\n","file_path = 'dataset/chatbot-dialogs.txt'\n","data = load_dataset(file_path)\n","encoded_data, word_to_idx, all_words = build_vocab_and_encode(data)\n","\n","# Determine maximum length\n","max_pattern_len = max(len(pattern) for pattern, response in encoded_data)\n","max_response_len = max(len(response) for pattern, response in encoded_data)\n","max_len = max(max_pattern_len, max_response_len)\n","\n","# Pad patterns and responses\n","padded_patterns = [pad_sequence(pattern, max_len) for pattern, response in encoded_data]\n","padded_responses = [pad_sequence(response, max_len) for pattern, response in encoded_data]\n","\n","# Convert to tensors\n","patterns = torch.tensor(padded_patterns, dtype=torch.long)\n","responses = torch.tensor(padded_responses, dtype=torch.long)\n","\n","# Check for GPU availability and set device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Move tensors to device\n","patterns = patterns.to(device)\n","responses = responses.to(device)\n","\n","# Create DataLoader\n","batch_size = 64\n","dataset = TensorDataset(patterns, responses)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n","\n","class ChatbotModel(nn.Module):\n","    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, output_size, max_len):\n","        super(ChatbotModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n","        self.fc = nn.Linear(hidden_size * max_len, output_size * max_len)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x, _ = self.lstm(x)\n","        x = x.contiguous().view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x.view(x.size(0), max_len, -1)\n","\n","# Initialize model\n","vocab_size = len(all_words)\n","embed_size = 50\n","hidden_size = 128\n","num_layers = 2\n","output_size = vocab_size\n","model = ChatbotModel(vocab_size, embed_size, hidden_size, num_layers, output_size, max_len).to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 1\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0\n","    for batch_patterns, batch_responses in dataloader:\n","        batch_patterns = batch_patterns.to(device)\n","        batch_responses = batch_responses.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(batch_patterns)\n","        outputs = outputs.view(-1, output_size)\n","        batch_responses = batch_responses.view(-1)\n","\n","        loss = criterion(outputs, batch_responses)\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(dataloader):.4f}')\n","\n"]},{"cell_type":"code","execution_count":null,"id":"8b57a9e4-e6ed-4b64-9a3d-ab0463352aec","metadata":{"jupyter":{"source_hidden":true},"id":"8b57a9e4-e6ed-4b64-9a3d-ab0463352aec","outputId":"cbd62c19-c4ee-4a70-81b3-377d59766716"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start chatting with the bot (type 'quit' to stop)!\n"]},{"name":"stdin","output_type":"stream","text":["You:  what is your name\n"]},{"name":"stdout","output_type":"stream","text":["Bot: 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"]},{"name":"stdin","output_type":"stream","text":["You:  hi, how are you doing?\n"]},{"name":"stdout","output_type":"stream","text":["Bot: 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"]},{"name":"stdin","output_type":"stream","text":["You:  hi\n"]},{"name":"stdout","output_type":"stream","text":["Bot: 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"]},{"name":"stdin","output_type":"stream","text":["You:  quit\n"]}],"source":["def predict_response(input_text):\n","    input_text = preprocess(input_text)\n","    input_pattern = encode(input_text, word_to_idx)\n","    input_pattern = pad_sequence(input_pattern, max_len)\n","    input_pattern = torch.tensor(input_pattern, dtype=torch.long).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = model(input_pattern)\n","\n","    output = output.view(-1, output_size)\n","    _, predicted = torch.max(output, dim=1)\n","\n","    predicted = predicted.cpu().numpy()\n","\n","    response_words = [all_words[idx] for idx in predicted if idx < len(all_words)]\n","    response_text = ' '.join(response_words)\n","    return response_text\n","\n","# Chat with the bot\n","print(\"Start chatting with the bot (type 'quit' to stop)!\")\n","while True:\n","    user_input = input(\"You: \")\n","    if user_input.lower() == 'quit':\n","        break\n","    response = predict_response(user_input)\n","    print(f\"Bot: {response}\")"]},{"cell_type":"code","execution_count":null,"id":"0097f74c-64f8-4b45-b2a3-7808d6c41299","metadata":{"id":"0097f74c-64f8-4b45-b2a3-7808d6c41299"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"4489e7b2-08e3-410f-a5a4-a91435e4998e","metadata":{"id":"4489e7b2-08e3-410f-a5a4-a91435e4998e"},"source":["# CSV"]},{"cell_type":"code","execution_count":null,"id":"1f714b17-f8e3-4ed6-b058-e900dd325f69","metadata":{"id":"1f714b17-f8e3-4ed6-b058-e900dd325f69"},"outputs":[],"source":["def txt_to_csv(txt_file, csv_file):\n","  with open(txt_file, 'r', encoding='utf-8') as txt_file, open(csv_file, 'w', newline='', encoding='utf-8') as csv_file:\n","    fieldnames = ['question', 'response']\n","    csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n","    csv_writer.writeheader()\n","    for line in txt_file:\n","      columns = line.strip().split('\\t')\n","      csv_writer.writerow({'question': columns[0], 'response': columns[1]})\n","\n","# Example usage:\n","txt_file_path = 'dataset/chatbot-dialogs.txt'\n","csv_file_path = 'dataset/chatbot-dialogs1.csv'\n","txt_to_csv(txt_file_path, csv_file_path)"]},{"cell_type":"code","execution_count":null,"id":"d3e00ae5-5b02-405c-b337-4bb8ccbc493d","metadata":{"id":"d3e00ae5-5b02-405c-b337-4bb8ccbc493d","outputId":"436ea79e-0642-4da9-d635-56ac43746c71"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded data: [('hi, how are you doing?', \"i'm fine. how about yourself?\"), (\"i'm fine. how about yourself?\", \"i'm pretty good. thanks for asking.\"), (\"i'm pretty good. thanks for asking.\", 'no problem. so how have you been?'), ('no problem. so how have you been?', \"i've been great. what about you?\"), (\"i've been great. what about you?\", \"i've been good. i'm in school right now.\")]\n","Encoded data: [([1014, 1063, 149, 2508, 644], [1089, 809, 1063, 65, 2515]), ([1089, 809, 1063, 65, 2515], [1089, 1671, 928, 2198, 852, 168]), ([1089, 1671, 928, 2198, 852, 168], [1443, 1678, 1999, 1063, 985, 2508, 235]), ([1443, 1678, 1999, 1063, 985, 2508, 235], [1139, 235, 945, 2424, 65, 2508]), ([1139, 235, 945, 2424, 65, 2508], [1139, 235, 928, 1089, 1095, 1854, 1788, 1462])]\n","Epoch [1/10], Loss: 7.8319\n","Epoch [2/10], Loss: 7.6702\n","Epoch [3/10], Loss: 7.4357\n","Epoch [4/10], Loss: 7.0745\n","Epoch [5/10], Loss: 6.5361\n","Epoch [6/10], Loss: 5.7611\n","Epoch [7/10], Loss: 4.7232\n","Epoch [8/10], Loss: 3.5437\n","Epoch [9/10], Loss: 2.7280\n","Epoch [10/10], Loss: 2.4484\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import re\n","import numpy as np\n","import pandas as pd\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# Load dataset from CSV\n","def load_dataset(file_path):\n","    df = pd.read_csv(file_path)\n","    data = []\n","    for _, row in df.iterrows():\n","        pattern = str(row['question'])\n","        response = str(row['response'])\n","        data.append((pattern, response))\n","    return data\n","\n","# Preprocessing\n","def preprocess(text):\n","    text = text.lower()\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    return text\n","\n","# Vocabulary and encoding\n","def build_vocab(data):\n","    all_words = []\n","    for (pattern, response) in data:\n","        pattern = preprocess(pattern)\n","        response = preprocess(response)\n","        words = pattern.split() + response.split()\n","        all_words.extend(words)\n","    all_words = sorted(set(all_words))\n","    word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n","    return word_to_idx, all_words\n","\n","def encode(text, word_to_idx):\n","    text = preprocess(text)\n","    return [word_to_idx[word] for word in text.split() if word in word_to_idx]\n","\n","def build_vocab_and_encode(data):\n","    word_to_idx, all_words = build_vocab(data)\n","    encoded_data = [(encode(pattern, word_to_idx), encode(response, word_to_idx)) for (pattern, response) in data]\n","    return encoded_data, word_to_idx, all_words\n","\n","# Pad sequences\n","def pad_sequence(seq, max_len, padding_value=0):\n","    return seq + [padding_value] * (max_len - len(seq))\n","\n","# Load and process dataset\n","file_path = 'dataset/chatbot-dialogs1.csv'  # Update to your dataset file path\n","data = load_dataset(file_path)\n","\n","# Debugging: Print loaded data\n","print(f\"Loaded data: {data[:5]}\")  # Print first 5 entries for inspection\n","\n","encoded_data, word_to_idx, all_words = build_vocab_and_encode(data)\n","\n","# Debugging: Print encoded data\n","print(f\"Encoded data: {encoded_data[:5]}\")  # Print first 5 entries for inspection\n","\n","\n","# Determine maximum length\n","max_pattern_len = max(len(pattern) for pattern, response in encoded_data)\n","max_response_len = max(len(response) for pattern, response in encoded_data)\n","max_len = max(max_pattern_len, max_response_len)\n","\n","# Pad patterns and responses\n","padded_patterns = [pad_sequence(pattern, max_len) for pattern, response in encoded_data]\n","padded_responses = [pad_sequence(response, max_len) for pattern, response in encoded_data]\n","\n","# Convert to tensors\n","patterns = torch.tensor(padded_patterns, dtype=torch.long)\n","responses = torch.tensor(padded_responses, dtype=torch.long)\n","\n","# Check for GPU availability and set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU 1 (NVIDIA RTX)\n","\n","# Move tensors to device\n","patterns = patterns.to(device)\n","responses = responses.to(device)\n","\n","class ChatbotModel(nn.Module):\n","    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, output_size, max_len):\n","        super(ChatbotModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n","        self.fc = nn.Linear(hidden_size * max_len, output_size * max_len)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x, _ = self.lstm(x)\n","        x = x.contiguous().view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x.view(x.size(0), max_len, -1)\n","\n","# Initialize model\n","vocab_size = len(all_words)\n","embed_size = 50\n","hidden_size = 128\n","num_layers = 2quit\n","output_size = vocab_size\n","model = ChatbotModel(vocab_size, embed_size, hidden_size, num_layers, output_size, max_len).to(device)  # Move model to GPU\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 10\n","for epoch in range(num_epochs):\n","    optimizer.zero_grad()\n","    outputs = model(patterns)\n","\n","    outputs = outputs.view(-1, output_size)\n","    responses = responses.view(-1)\n","\n","    if outputs.shape[0] != responses.shape[0]:\n","        print(f\"WARNING: Mismatch in output and response sizes. Outputs: {outputs.shape}, Responses: {responses.shape}\")\n","    else:\n","        loss = criterion(outputs, responses)\n","        loss.backward()\n","        optimizer.step()\n","        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n"]},{"cell_type":"code","execution_count":null,"id":"f33d462b-bd32-4f27-889e-076c78f82c35","metadata":{"id":"f33d462b-bd32-4f27-889e-076c78f82c35","outputId":"179ed9de-9618-41ac-fee9-97fa08e9b81c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Start chatting with the bot (type 'quit' to stop)!\n"]},{"name":"stdin","output_type":"stream","text":["You:  hi, how are you doing?\n"]},{"name":"stdout","output_type":"stream","text":["Bot: i i you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"]},{"name":"stdin","output_type":"stream","text":["You:  what is your name\n"]},{"name":"stdout","output_type":"stream","text":["Bot: i i you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"]},{"name":"stdin","output_type":"stream","text":["You:  who are you\n"]},{"name":"stdout","output_type":"stream","text":["Bot: i i you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"]},{"name":"stdin","output_type":"stream","text":["You:  what you doing\n"]},{"name":"stdout","output_type":"stream","text":["Bot: i i you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"]},{"name":"stdin","output_type":"stream","text":["You:  where are you\n"]},{"name":"stdout","output_type":"stream","text":["Bot: i i you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"]},{"name":"stdin","output_type":"stream","text":["You:  hi\n"]},{"name":"stdout","output_type":"stream","text":["Bot: i you you 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287 011287\n"]},{"name":"stdin","output_type":"stream","text":["You:  quit\n"]}],"source":["def predict_response(input_text):\n","    input_text = preprocess(input_text)\n","    input_pattern = encode(input_text, word_to_idx)\n","    input_pattern = pad_sequence(input_pattern, max_len)\n","    input_pattern = torch.tensor(input_pattern, dtype=torch.long).unsqueeze(0).to(device)  # Move to GPU\n","\n","    with torch.no_grad():\n","        output = model(input_pattern)\n","\n","    output = output.view(-1, output_size)\n","    _, predicted = torch.max(output, dim=1)\n","\n","    predicted = predicted.cpu().numpy()  # Move to CPU before converting to numpy\n","\n","    response_words = [all_words[idx] for idx in predicted if idx < len(all_words)]\n","    response_text = ' '.join(response_words)\n","    return response_text\n","\n","# Chat with the bot\n","print(\"Start chatting with the bot (type 'quit' to stop)!\")\n","while True:\n","    user_input = input(\"You: \")\n","    if user_input.lower() == 'quit':\n","        break\n","    response = predict_response(user_input)\n","    print(f\"Bot: {response}\")\n"]},{"cell_type":"code","execution_count":null,"id":"f4439407-3210-45b1-8dc4-038e448238d2","metadata":{"id":"f4439407-3210-45b1-8dc4-038e448238d2"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":5}